{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 改进模型拟合\r\n",
    "\r\n",
    "有一个矛盾,为了达到完美的拟合,模型必须向过拟合,才能知道哪里是边界.\r\n",
    "\r\n",
    "面对一个机器学习的问题,最初的目标是找到一个表现出一些泛化能力的模型,然后可以过拟合.之后的的事情是对抗过拟合改善模型的泛化能力.使其能达到第一个里程碑: 模型的表现要超过简单方法的基线.\r\n",
    "\r\n",
    "在初始训练模型时常见的 3 个问题\r\n",
    "\r\n",
    "- 训练无效,训练损失不会随着时间而减小.\r\n",
    "- 训练效果很好,但是模型的效果无法打败基线.\r\n",
    "- 训练和验证效果都很好,模型效果也超过了基线,但是似乎一直没有过拟合出现.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 调整梯度下降的参数\r\n",
    "\r\n",
    "有时候会碰到,无论如何训练,损失总是过早的停滞了.这个时候还记得前文对随机数据的实验吗?即使是随机的数据也能训练出一个模型,当然这个模型谈不上什么泛化能力.因此这个问题一定有办法解决.\r\n",
    "\r\n",
    "通常这样的情况发生总是梯度下降的配置问题.\r\n",
    "\r\n",
    "- 模型初始权重分布/优化器选择/学习率/批次大小等等都互相依赖.\r\n",
    "- 其他调整学习率和批次大小就足够了,特别是学习率.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from keras.datasets import mnist\r\n",
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers\r\n",
    "\r\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n",
    "train_images = train_images.reshape((60000, 28 * 28))\r\n",
    "train_images = train_images.astype('float32') / 255\r\n",
    "\r\n",
    "model = keras.Sequential([\r\n",
    "    layers.Dense(512, activation='relu'),\r\n",
    "    layers.Dense(10, activation='softmax')\r\n",
    "])\r\n",
    "\r\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.),\r\n",
    "              loss='sparse_categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "model.fit(train_images,\r\n",
    "          train_labels,\r\n",
    "          epochs=10,\r\n",
    "          batch_size=128,\r\n",
    "          validation_split=0.2)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 784.9496 - accuracy: 0.4603 - val_loss: 5.4565 - val_accuracy: 0.3444\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 6.6639 - accuracy: 0.3249 - val_loss: 2.3721 - val_accuracy: 0.2243\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 3.6877 - accuracy: 0.2662 - val_loss: 2.1725 - val_accuracy: 0.1973\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.7740 - accuracy: 0.2407 - val_loss: 2.2461 - val_accuracy: 0.2333\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 3.0985 - accuracy: 0.2590 - val_loss: 2.2055 - val_accuracy: 0.2407\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.6578 - accuracy: 0.2789 - val_loss: 2.0714 - val_accuracy: 0.2631\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.5720 - accuracy: 0.2725 - val_loss: 2.1041 - val_accuracy: 0.2575\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.5495 - accuracy: 0.2364 - val_loss: 2.2423 - val_accuracy: 0.2323\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.3645 - accuracy: 0.2644 - val_loss: 2.3577 - val_accuracy: 0.3001\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 2.3815 - accuracy: 0.2522 - val_loss: 2.1903 - val_accuracy: 0.2705\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29042768f40>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "学习率是 1 ,无论如何训练,模型的准确度一直都在 0.2 ~ 0.3 之间.接下来我们把学习率改成 1e-2.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from keras.datasets import mnist\r\n",
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers\r\n",
    "\r\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n",
    "train_images = train_images.reshape((60000, 28 * 28))\r\n",
    "train_images = train_images.astype('float32') / 255\r\n",
    "\r\n",
    "model = keras.Sequential([\r\n",
    "    layers.Dense(512, activation='relu'),\r\n",
    "    layers.Dense(10, activation='softmax')\r\n",
    "])\r\n",
    "\r\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\r\n",
    "              loss='sparse_categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "model.fit(train_images,\r\n",
    "          train_labels,\r\n",
    "          epochs=10,\r\n",
    "          batch_size=128,\r\n",
    "          validation_split=0.2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.3525 - accuracy: 0.9108 - val_loss: 0.1891 - val_accuracy: 0.9508\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1420 - accuracy: 0.9642 - val_loss: 0.1329 - val_accuracy: 0.9705\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1143 - accuracy: 0.9728 - val_loss: 0.1561 - val_accuracy: 0.9709\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.1024 - accuracy: 0.9782 - val_loss: 0.1918 - val_accuracy: 0.9694\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0939 - accuracy: 0.9814 - val_loss: 0.2687 - val_accuracy: 0.9628\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0781 - accuracy: 0.9848 - val_loss: 0.2403 - val_accuracy: 0.9709\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0667 - accuracy: 0.9870 - val_loss: 0.2023 - val_accuracy: 0.9743\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0652 - accuracy: 0.9876 - val_loss: 0.2872 - val_accuracy: 0.9698\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0649 - accuracy: 0.9889 - val_loss: 0.3001 - val_accuracy: 0.9721\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 2s 5ms/step - loss: 0.0542 - accuracy: 0.9902 - val_loss: 0.3669 - val_accuracy: 0.9699\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29042875c40>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "调整过学习率,模型训练就正常多了..\r\n",
    "\r\n",
    "如果出现损失一直无法降低等情况\r\n",
    "\r\n",
    "- 降低或提高学习率.过高的学习率会使更新大大超过适当的拟合范围,模型就一直在这个范围左右反复横跳.过低的学习率反而使得学习的速度非常慢,简单的几轮训练验证几乎完全不可能够到合适的拟合范围.\r\n",
    "- 增加批次大小,批次的样本更多,会使得信息量加大,噪音相对更小.\r\n",
    "\r\n",
    "以上,终于有一个能让训练开始的配置了.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf': venv)"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}