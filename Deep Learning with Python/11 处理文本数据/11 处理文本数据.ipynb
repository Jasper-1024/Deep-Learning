{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 11 处理文本数据\r\n",
    "\r\n",
    "本章包括\r\n",
    "\r\n",
    "- 预处理文本数据\r\n",
    "- 用于文本数据处理的词袋和序列建模\r\n",
    "- Transformer 架构(变形金刚?🤣)\r\n",
    "- 序列到序列的学习\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11.6 章节概述\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "最晕的一个章节,建议先不要看这一章的翻译了,止不住那天基础牢靠了,会再回来重写.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "两种 nlp 模型: 词袋模型 和 序列模型\r\n",
    "\r\n",
    "- 词袋模型: 文本处理成 单词集合 或 n-gram,无视词的顺序.使用的神经网络模型多为密集层.\r\n",
    "- 序列模型: 需要处理词的顺序,使用的神经网络模型 rnn 一维卷积 Transformer 等.\r\n",
    "\r\n",
    "一个来自经验的原则: 样本数量/样本平均长度,< 1500 词袋模型 > 1500 序列模型.\r\n",
    "\r\n",
    "- 比例越高,样本平均长度越小,越不能丢弃信息,必须利用词序信息 -> 序列模型\r\n",
    "\r\n",
    "词嵌入是一种向量空间,词向量与词向量直接的距离关系代表了词与词的语义关系.\r\n",
    "\r\n",
    "序列到序列是一个通用的学习框架,可以应用在很多 nlp 问题上.序列到序列模型通常是编码器+解码器.编码器处理源序列,解码器在编码器处理结果的基础上,查看过去的标记,试图预测目标序列中未来的标记.\r\n",
    "\r\n",
    "Neural attention 是一种创建上下文感知的单词表征方式,是 Transformer 架构的基础.\r\n",
    "\r\n",
    "Transformer 由 Transformer 编码器 Transformer 解码器构成,在序列到序列的任务上表现出色,其编码器部分也可以用于分类任务.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}