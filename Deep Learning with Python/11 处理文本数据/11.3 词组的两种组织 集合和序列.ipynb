{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 11.3 词组的两种组织 集合和序列\r\n",
    "\r\n",
    "机器学习模型如何表示单个单词是个相对没有争议的问题,上一节我们提到了如何进行单词的编码.但是另一个棘手的问题: 如何将这些单词组织成句子?\r\n",
    "\r\n",
    "自然语言中单词的顺序是个非常有趣的问题: 与时间序列不同,句子中的单词没有一个自然典型的前后顺序.不同语言下哪怕都是这些单词,其组合顺序相差很大.即使给定一门语言,通常也可以重新排列单词但是表达的意思完全一致.甚至没有语序,只是一堆乱序的单词,我们也能主动的猜测这句话原本的意思,尽管可能出现很多歧义.自然语言句子的语序和意义相关,但又不那么直接.\r\n",
    "\r\n",
    "如何表达词序在不同的 nlp 架构中千差万别,最直接的方法是抛弃词序,将样本转化为无序的单词集合处理.这种方法可以被称为词袋模型.也可以严格按照单词的顺序进行处理,就像处理时序数据一样(这里可以考虑使用 rnn).混合的方式也是可行的,Transformer 架构整体上是不考虑词序的,但是它将词的位置信息注入了它处理的表征中,这使得 Transformer 能同时查看句子的不同部分(与 rnn 不同).因此 Transformer 和 rnn 也被称作序列模型.\r\n",
    "\r\n",
    "历史上看,机器学习早期在 nlp 上的应用只涉及到词袋模型.随着循环神经网络的再次复兴,人们对序列模型的兴趣在 2015 年才逐渐提高，今天这两种模型依然息息相关。\r\n",
    "\r\n",
    "我们将在 imdb 数据集上演示每种方法,在 4 5 章里我们是使用的是预处理完毕矢量化的词袋数据,接下来我们会直接处理 imdb 的原始文本数据.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 准备 imdb 原始数据\r\n",
    "\r\n",
    "原始数据集在这里 > <https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz>\r\n",
    "\r\n",
    "解压后得到了 aclImdb 文件夹\r\n",
    "\r\n",
    "```text\r\n",
    "aclImdb/\r\n",
    "...train/\r\n",
    "......pos/\r\n",
    "......neg/\r\n",
    "...test/\r\n",
    "......pos/\r\n",
    "......neg/\r\n",
    "```\r\n",
    "\r\n",
    "- train 是训练数据,test 是测试数据\r\n",
    "- pos 是正向数据,neg 是负面数据.\r\n",
    "- train/test 各自有 12500,共 25000 条数据.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, pathlib, shutil, random\r\n",
    "\r\n",
    "base_dir = pathlib.Path(\"aclImdb\")\r\n",
    "val_dir = base_dir / \"val\"\r\n",
    "train_dir = base_dir / \"train\"\r\n",
    "for category in (\"neg\", \"pos\"):\r\n",
    "    os.makedirs(val_dir / category)\r\n",
    "    files = os.listdir(train_dir / category)\r\n",
    "    random.Random(1337).shuffle(files)  #按照种子文件洗牌重排列\r\n",
    "    num_val_samples = int(0.2 * len(files))  #取0.2作为验证集\r\n",
    "    val_files = files[-num_val_samples:]\r\n",
    "    for fname in val_files:  #将验证集移动到单独文件夹\r\n",
    "        shutil.move(train_dir / category / fname,\r\n",
    "                    val_dir / category / fname)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "将原来的顺序打乱,选取验证集,将验证集移入单独文件夹.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from tensorflow import keras\r\n",
    "\r\n",
    "batch_size = 32  #批次大小\r\n",
    "\r\n",
    "train_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
    "    \"aclImdb/train\", batch_size=batch_size)  #训练集\r\n",
    "val_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
    "    \"aclImdb/val\", batch_size=batch_size)  #验证集\r\n",
    "test_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
    "    \"aclImdb/test\", batch_size=batch_size)  #测试集\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用 `text_dataset_from_directory` 构建数据集\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for inputs, targets in train_ds:\r\n",
    "    print(\"inputs.shape:\", inputs.shape)\r\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\r\n",
    "    print(\"targets.shape:\", targets.shape)\r\n",
    "    print(\"targets.dtype:\", targets.dtype)\r\n",
    "    print(\"inputs[0]:\", inputs[0])\r\n",
    "    print(\"targets[0]:\", targets[0])\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'What happens when someone has so much social anxiety that they cease to function? How alone can one man get? When the mundane crap we have to do in order to be part of society gets to be too much, what happens? Frownland explores these questions. Definitely a startling original debut from Bronstein. The tone is strange and claustrophobic as we get inside the mind of a guy named Keith that is so messed up he can hardly form a proper sentence. We follow him around as he tries to make contact with people and function day to day. Most of us have known people like this- people that say \"sorry\" too much or \"i appreciate it\" when there\\'s nothing to appreciate. So we know there are people out there like this but why would someone want to make a movie about them? Well, because its interesting and Bronstein and the lead actor, Dore Mann, do an excellent job. This film is about as un-commercial as a film can get. A few friends filmed it over the course of a few years as they saved money. It was shot on 16mm and the scratched film look is beautifully low budget. With no distributer, this may be a tough one to find, I think it\\'s been screening randomly for the past year or so. Hopefully it\\'ll be on DVD at some point. I saw it at the Silent Movie Theater here in LA. There were 10 people in the audience, among them Crispin Glover, if that tells you anything about how weird this movie is. Highly recommended.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "至此数据准备完成.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建单词集合 词袋方法\r\n",
    "\r\n",
    "最直接处理文本信息的方法,直接忽略顺序,将标记处理成无序的集合.\r\n",
    "\r\n",
    "例子 \"the cat sat on the mat\" 处理完毕就成了\r\n",
    "\r\n",
    "```text\r\n",
    "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\r\n",
    "```\r\n",
    "\r\n",
    "这样做的优点是可以方便的将一句话表示为单一的张量,每一个维度都是一个标记存在不存在.如果使用二进制编码,将一句话表示为一个张量,那么这个张量的维度和集合标记的总数相同,最终的张量中大部分维度都是 0,少数才是 1.(这就是 4 5 章我们使用 imdb 的方式)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "text_vectorization = TextVectorization(\r\n",
    "    max_tokens=20000,  #限制最大总标记数量 20000,防止编码空间过大.\r\n",
    "    output_mode=\"binary\",  #输出标记为 2进制\r\n",
    ")\r\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)  #只有原始文本信息的数据集\r\n",
    "text_vectorization.adapt(text_only_train_ds)  # adapt 转换为单词索引表\r\n",
    "#张量化数据集\r\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y:\r\n",
    "                                     (text_vectorization(x), y))  #训练集\r\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))  #验证集\r\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y:\r\n",
    "                                   (text_vectorization(x), y))  #测试集\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用上一节提到的 TextVectorization 层张量化文本数据.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\r\n",
    "    print(\"inputs.shape:\", inputs.shape)\r\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\r\n",
    "    print(\"targets.shape:\", targets.shape)\r\n",
    "    print(\"targets.dtype:\", targets.dtype)\r\n",
    "    print(\"inputs[0]:\", inputs[0])\r\n",
    "    print(\"targets[0]:\", targets[0])\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "一个张量有 20000 个维度\r\n",
    "\r\n",
    "每个张量的值完全是 0-1 组成.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "\r\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\r\n",
    "    inputs = keras.Input(shape=(max_tokens, ))\r\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)  #全连接层\r\n",
    "    x = layers.Dropout(0.5)(x)  #dropout层\r\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  #输出层\r\n",
    "    model = keras.Model(inputs, outputs)\r\n",
    "    model.compile(optimizer=\"rmsprop\",\r\n",
    "                  loss=\"binary_crossentropy\",\r\n",
    "                  metrics=[\"accuracy\"])\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取 model 的函数,接下来会一直用到."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model = get_model()\r\n",
    "model.summary()\r\n",
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\r\n",
    "                                    save_best_only=True)  #只保存最好模型\r\n",
    "]\r\n",
    "model.fit(\r\n",
    "    binary_1gram_train_ds.cache(),\r\n",
    "    validation_data=binary_1gram_val_ds.cache(),  #验证集\r\n",
    "    epochs=10,  #10个epoch\r\n",
    "    callbacks=callbacks)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 15s 21ms/step - loss: 0.3993 - accuracy: 0.8311 - val_loss: 0.3079 - val_accuracy: 0.8756\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2733 - accuracy: 0.9007 - val_loss: 0.3017 - val_accuracy: 0.8856\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2421 - accuracy: 0.9178 - val_loss: 0.3181 - val_accuracy: 0.8868\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2314 - accuracy: 0.9223 - val_loss: 0.3356 - val_accuracy: 0.8908\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2252 - accuracy: 0.9250 - val_loss: 0.3499 - val_accuracy: 0.8868\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2159 - accuracy: 0.9312 - val_loss: 0.3644 - val_accuracy: 0.8902\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2171 - accuracy: 0.9331 - val_loss: 0.3733 - val_accuracy: 0.8890\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2084 - accuracy: 0.9317 - val_loss: 0.3832 - val_accuracy: 0.8858\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2107 - accuracy: 0.9323 - val_loss: 0.3955 - val_accuracy: 0.8914\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2052 - accuracy: 0.9347 - val_loss: 0.3985 - val_accuracy: 0.8876\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2f4330a47c0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = keras.models.load_model(\"binary_1gram.keras\")\r\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "782/782 [==============================] - 23s 29ms/step - loss: 0.2925 - accuracy: 0.8875\n",
      "Test acc: 0.887\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们这里的准确率是 88.7% 相对于基线 50% 已经很好了.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf': venv)"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}