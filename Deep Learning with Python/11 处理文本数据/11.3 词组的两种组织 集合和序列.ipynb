{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 11.3 词组的两种组织 集合和序列\r\n",
    "\r\n",
    "机器学习模型如何表示单个单词是个相对没有争议的问题,上一节我们提到了如何进行单词的编码.但是另一个棘手的问题: 如何将这些单词组织成句子?\r\n",
    "\r\n",
    "自然语言中单词的顺序是个非常有趣的问题: 与时间序列不同,句子中的单词没有一个自然典型的前后顺序.不同语言下哪怕都是这些单词,其组合顺序相差很大.即使给定一门语言,通常也可以重新排列单词但是表达的意思完全一致.甚至没有语序,只是一堆乱序的单词,我们也能主动的猜测这句话原本的意思,尽管可能出现很多歧义.自然语言句子的语序和意义相关,但又不那么直接.\r\n",
    "\r\n",
    "如何表达词序在不同的 nlp 架构中千差万别,最直接的方法是抛弃词序,将样本转化为无序的单词集合处理.这种方法可以被称为词袋模型.也可以严格按照单词的顺序进行处理,就像处理时序数据一样(这里可以考虑使用 rnn).混合的方式也是可行的,Transformer 架构整体上是不考虑词序的,但是它将词的位置信息注入了它处理的表征中,这使得 Transformer 能同时查看句子的不同部分(与 rnn 不同).因此 Transformer 和 rnn 也被称作序列模型.\r\n",
    "\r\n",
    "历史上看,机器学习早期在 nlp 上的应用只涉及到词袋模型.随着循环神经网络的再次复兴,人们对序列模型的兴趣在 2015 年才逐渐提高，今天这两种模型依然息息相关。\r\n",
    "\r\n",
    "我们将在 imdb 数据集上演示每种方法,在 4 5 章里我们是使用的是预处理完毕矢量化的词袋数据,接下来我们会直接处理 imdb 的原始文本数据.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 准备 imdb 原始数据\r\n",
    "\r\n",
    "原始数据集在这里 > <https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz>\r\n",
    "\r\n",
    "解压后得到了 aclImdb 文件夹\r\n",
    "\r\n",
    "```text\r\n",
    "aclImdb/\r\n",
    "...train/\r\n",
    "......pos/\r\n",
    "......neg/\r\n",
    "...test/\r\n",
    "......pos/\r\n",
    "......neg/\r\n",
    "```\r\n",
    "\r\n",
    "- train 是训练数据,test 是测试数据\r\n",
    "- pos 是正向数据,neg 是负面数据.\r\n",
    "- train/test 各自有 12500,共 25000 条数据.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os, pathlib, shutil, random\r\n",
    "\r\n",
    "base_dir = pathlib.Path(\"aclImdb\")\r\n",
    "val_dir = base_dir / \"val\"\r\n",
    "train_dir = base_dir / \"train\"\r\n",
    "for category in (\"neg\", \"pos\"):\r\n",
    "    os.makedirs(val_dir / category)\r\n",
    "    files = os.listdir(train_dir / category)\r\n",
    "    random.Random(1337).shuffle(files)  #按照种子文件洗牌重排列\r\n",
    "    num_val_samples = int(0.2 * len(files))  #取0.2作为验证集\r\n",
    "    val_files = files[-num_val_samples:]\r\n",
    "    for fname in val_files:  #将验证集移动到单独文件夹\r\n",
    "        shutil.move(train_dir / category / fname,\r\n",
    "                    val_dir / category / fname)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "将原来的顺序打乱,选取验证集,将验证集移入单独文件夹.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from tensorflow import keras\r\n",
    "\r\n",
    "batch_size = 32  #批次大小\r\n",
    "\r\n",
    "train_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
    "    \"aclImdb/train\", batch_size=batch_size)  #训练集\r\n",
    "val_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
    "    \"aclImdb/val\", batch_size=batch_size)  #验证集\r\n",
    "test_ds = keras.preprocessing.text_dataset_from_directory(\r\n",
    "    \"aclImdb/test\", batch_size=batch_size)  #测试集\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用 `text_dataset_from_directory` 构建数据集\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for inputs, targets in train_ds:\r\n",
    "    print(\"inputs.shape:\", inputs.shape)\r\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\r\n",
    "    print(\"targets.shape:\", targets.shape)\r\n",
    "    print(\"targets.dtype:\", targets.dtype)\r\n",
    "    print(\"inputs[0]:\", inputs[0])\r\n",
    "    print(\"targets[0]:\", targets[0])\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'Unhinged was part of the Video Nasty censorship film selection that the UK built up in the 80\\'s. Keeps the gory stuff out of the hands of children, don\\'t you know! It must have left many wondering what the fuss was all about. By today\\'s standard, Unhinged is a tame little fairy tale.<br /><br />3 girls are off to a jazz concert... and right away, you know the body count is going to be quite low. They get lost in the woods, & wind up getting in a car accident that looks so fake it\\'s laughable. They are picked up by some nearby residents that live in the woods in a creepy house. One of the girls is seriously injured and has to stay upstairs. Then there\\'s talking. Talking about why the girls are here, and how they must be to dinner on time because mother doesn\\'t like it when someone is late. And more talking. Yakkity yak. Some suspense is built as a crazy guy is walking around and harassing the girls, and someone\\'s eyeball is looking through holes in the walls at the pretty girls in something that looks like Hitchcock\\'s Psycho. I digress because there is so much blah blah in this film, that you wonder when the killings are going to start. In fact, one of the girls gets so bored out of her mind that she walks in the woods, alone, looking for the town. Smart move. She probably knew about the lonely virgin walking alone in the woods part, but just didn\\'t care. More talk continues after this as we wait, wait, and wait some more until the next girl may or may not be killed.<br /><br />And then there\\'s the twist ending. The \"expected\" unexpected for some viewers, for others a real gotcha. Quite possibly the ONLY reason why someone would really want to watch this. I don\\'t care how twisted it is, nothing in this movie makes up for the most boring time I had watching it. Even with the minor impact of the ending, the director just didn\\'t have what it takes to really deliver a good story with it. It would have made a much better 30 minute - 1 hour TV episode on say, Tales from the Darkside.<br /><br />If you really must get this for any reason, perhaps just to say you\\'ve watched every slasher movie, do yourself a favor and have the fast-forward button ready. Since the movie has so many unimportant scenes, just zoom through them, and in no time, you\\'ll get to the \"WOW, that\\'s what it was all this time\" ending. Oh and halfway through the movie there\\'s a shower scene with 2 girls showing boo-bees. Horray for boo-bees. Those beautiful buzzing honey-making boo-bees.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "至此数据准备完成.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建单词集合 词袋方法\r\n",
    "\r\n",
    "最直接处理文本信息的方法,直接忽略顺序,将标记处理成无序的集合.\r\n",
    "\r\n",
    "例子 \"the cat sat on the mat\" 处理完毕就成了\r\n",
    "\r\n",
    "```text\r\n",
    "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\r\n",
    "```\r\n",
    "\r\n",
    "这样做的优点是可以方便的将一句话表示为单一的张量,每一个维度都是一个标记存在不存在.如果使用二进制编码,将一句话表示为一个张量,那么这个张量的维度和集合标记的总数相同,最终的张量中大部分维度都是 0,少数才是 1.(这就是 4 5 章我们使用 imdb 的方式)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers\r\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "text_vectorization = TextVectorization(\r\n",
    "    max_tokens=20000,  #限制最大总标记数量 20000,防止编码空间过大.\r\n",
    "    output_mode=\"binary\",  #输出标记为 2进制\r\n",
    ")\r\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)  #只有原始文本信息的数据集\r\n",
    "text_vectorization.adapt(text_only_train_ds)  # adapt 转换为单词索引表\r\n",
    "#张量化数据集\r\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y:\r\n",
    "                                     (text_vectorization(x), y))  #训练集\r\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))  #验证集\r\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y:\r\n",
    "                                   (text_vectorization(x), y))  #测试集\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用上一节提到的 TextVectorization 层张量化文本数据.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\r\n",
    "    print(\"inputs.shape:\", inputs.shape)\r\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\r\n",
    "    print(\"targets.shape:\", targets.shape)\r\n",
    "    print(\"targets.dtype:\", targets.dtype)\r\n",
    "    print(\"inputs[0]:\", inputs[0])\r\n",
    "    print(\"targets[0]:\", targets[0])\r\n",
    "    break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "一个张量有 20000 个维度\r\n",
    "\r\n",
    "每个张量的值完全是 0-1 组成.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "\r\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\r\n",
    "    inputs = keras.Input(shape=(max_tokens, ))\r\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)  #全连接层\r\n",
    "    x = layers.Dropout(0.5)(x)  #dropout层\r\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  #输出层\r\n",
    "    model = keras.Model(inputs, outputs)\r\n",
    "    model.compile(optimizer=\"rmsprop\",\r\n",
    "                  loss=\"binary_crossentropy\",\r\n",
    "                  metrics=[\"accuracy\"])\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取 model 的函数,接下来会一直用到."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model = get_model()\r\n",
    "model.summary()\r\n",
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\r\n",
    "                                    save_best_only=True)  #只保存最好模型\r\n",
    "]\r\n",
    "model.fit(\r\n",
    "    binary_1gram_train_ds.cache(),\r\n",
    "    validation_data=binary_1gram_val_ds.cache(),  #验证集\r\n",
    "    epochs=10,  #10个epoch\r\n",
    "    callbacks=callbacks)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 11s 16ms/step - loss: 0.4152 - accuracy: 0.8206 - val_loss: 0.3066 - val_accuracy: 0.8786\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2776 - accuracy: 0.8971 - val_loss: 0.3044 - val_accuracy: 0.8846\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2477 - accuracy: 0.9125 - val_loss: 0.3219 - val_accuracy: 0.8848\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2352 - accuracy: 0.9212 - val_loss: 0.3353 - val_accuracy: 0.8878\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2265 - accuracy: 0.9263 - val_loss: 0.3546 - val_accuracy: 0.8796\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2197 - accuracy: 0.9275 - val_loss: 0.3690 - val_accuracy: 0.8784\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2186 - accuracy: 0.9291 - val_loss: 0.3831 - val_accuracy: 0.8754\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2259 - accuracy: 0.9295 - val_loss: 0.3898 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2156 - accuracy: 0.9308 - val_loss: 0.3992 - val_accuracy: 0.8806\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2183 - accuracy: 0.9290 - val_loss: 0.4066 - val_accuracy: 0.8782\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2984d062940>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = keras.models.load_model(\"binary_1gram.keras\")\r\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "782/782 [==============================] - 9s 11ms/step - loss: 0.2930 - accuracy: 0.8869\n",
      "Test acc: 0.887\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们这里的准确率是 88.7% 相对于基线 50% 已经很好了.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 二进制编码的双字母组\r\n",
    "\r\n",
    "抛弃词序是非常简化的方法.但是因为一些非常单一的概念是由多个单词表达的,例如 \"United States\" 如果拆成单个词 \"United\" 和 \"States\" 表达的意思将完全不同.因此进行标记拆分文本时,通常会使用 n-gram 而不是直接拆分成单个的词.通常情况下会使用双字母组(2-gram).\r\n",
    "\r\n",
    "还是 \"the cat sat on the mat\" 的例子,转成 2-gram\r\n",
    "\r\n",
    "```text\r\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\r\n",
    " \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "text_vectorization = TextVectorization(\r\n",
    "    ngrams=2,  #配置 n-gram\r\n",
    "    max_tokens=20000,\r\n",
    "    output_mode=\"binary\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TextVectorization 有 ngrams 参数设置 n-gram.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\r\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\r\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\r\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "重新准备数据"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model = get_model()\r\n",
    "model.summary()\r\n",
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\r\n",
    "]\r\n",
    "model.fit(binary_2gram_train_ds.cache(),\r\n",
    "          validation_data=binary_2gram_val_ds.cache(),\r\n",
    "          epochs=10,\r\n",
    "          callbacks=callbacks)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20000)]           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                320016    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 11s 18ms/step - loss: 0.3868 - accuracy: 0.8372 - val_loss: 0.2902 - val_accuracy: 0.8884\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2494 - accuracy: 0.9104 - val_loss: 0.2952 - val_accuracy: 0.8920\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2154 - accuracy: 0.9294 - val_loss: 0.3023 - val_accuracy: 0.8948\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1988 - accuracy: 0.9387 - val_loss: 0.3175 - val_accuracy: 0.8984\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1818 - accuracy: 0.9431 - val_loss: 0.3389 - val_accuracy: 0.8962\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1807 - accuracy: 0.9466 - val_loss: 0.3531 - val_accuracy: 0.8946\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1755 - accuracy: 0.9502 - val_loss: 0.3705 - val_accuracy: 0.8922\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1805 - accuracy: 0.9488 - val_loss: 0.3711 - val_accuracy: 0.8968\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.1662 - accuracy: 0.9516 - val_loss: 0.4122 - val_accuracy: 0.8926\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1749 - accuracy: 0.9539 - val_loss: 0.3861 - val_accuracy: 0.8946\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x299ea90fa90>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "model = keras.models.load_model(\"binary_2gram.keras\")\r\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "782/782 [==============================] - 10s 13ms/step - loss: 0.2823 - accuracy: 0.8958\n",
      "Test acc: 0.896\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "测试集精度提高到了 89.6% 提高了 1% 大概.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF 编码的双字母组\r\n",
    "\r\n",
    "我们还可以统计标记出现的次数,来为标记增加一些信息.\r\n",
    "\r\n",
    "还是上面的例子,增加频率统计后:\r\n",
    "\r\n",
    "```text\r\n",
    "{\"the\": 2, \"the cat\": 1, \"cat\": 1, \"cat sat\": 1, \"sat\": 1,\r\n",
    " \"sat on\": 1, \"on\": 1, \"on the\": 1, \"the mat: 1\", \"mat\": 1}\r\n",
    "```\r\n",
    "\r\n",
    "如果是文本分类任务,那么标记的次数统计就相当重要了.像是在电影评论中,任何足够长的评论可能都会存在 `terrible` 这个词,但是如果一个评论 `terrible` 出现次数相当多,那这条评论可能会偏向负面评价.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "text_vectorization = TextVectorization(\r\n",
    "    ngrams=2,  #双字母组\r\n",
    "    max_tokens=20000,  #最多20000个标记\r\n",
    "    output_mode=\"count\"  #添加词频\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "上面是 TextVectorization 增加词频的方法,设置 `output_mode=\"count\"`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "基于频率的统计有一个天然的缺陷,总有一些词出现的频率远高于其他,但是这些词与特征没什么关系.例如 `a` `is` 之类的,如果蛮干直接上词频统计,这些词会淹没那些与特征密切相关的词.\r\n",
    "\r\n",
    "按照前文的经验,这里可能的方案是数据规范化.将数据减去平均值/方差规范数字.这样的做法在大多数情况下是有意义的,但是文本张量几乎大部分数据都是0,相当稀疏.这样的稀疏性非常有利于计算.但是直接上前面的归一化会破坏这样的稀疏性,增加计算的成本.因此我们能够在文本张量归一化使用的运算只有除法而已.\r\n",
    "\r\n",
    "文本张量归一化只能用除法,那么分母呢?最好的方法是 TF-IDF(term frequency, inverse document frequency) 规范化的方法.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf')"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}