{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5 从序列到序列\r\n",
    "\r\n",
    "到目前为止,我们已经接触过了大多数 nlp 任务所需要的工具,然而我们只在处理过一类 nlp 任务--文本分类,这一节我们将接触另一种序列到序列的模型.\r\n",
    "\r\n",
    "序列到序列模型是将一个序列作为输入(可以是一个句子或一段文字)并将其翻译成不同的序列,这是许多 nlp 的核心任务.\r\n",
    "\r\n",
    "- 机器翻译: 将源样本的段落转换为目标语言的对应段落.\r\n",
    "- 文本汇总: 将长文本进行总结,只保留最重要信息.\r\n",
    "- 问题问答: 输入问题,返回答案\r\n",
    "- 聊天机器人: 输入对话/历史聊天记录等等转换为下一个回复\r\n",
    "- 文本生成: 将输入文本转换为一个完整的段落.\r\n",
    "- 等等...\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![seq2seq](seq2seq.png)\r\n",
    "\r\n",
    "上图显示了序列到序列模型工作的一般流程\r\n",
    "\r\n",
    "训练期\r\n",
    "\r\n",
    "- 一个编码器将源序列转换为一种中间表示.\r\n",
    "- 通过查看之前的标记和源序列,解码器被用来预测下一个标记.\r\n",
    "\r\n",
    "预测,预测时我们无法接触到目标序列,我们不得不一次生成一个标记.\r\n",
    "\r\n",
    "- 编码器将源序列编码转换\r\n",
    "- 解码器查看源序列和初始'种子'标记(图中是 '[start]'),并预测下一个真实标记.\r\n",
    "- 将真实标记送入解码器推动下一次预测,直到生成一个停止标记(图中是 '[end]').\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一个机器翻译例子\r\n",
    "\r\n",
    "我们将在一个机器翻译的问题上演示序列到序列的模型.机器翻译正是 Transformer 开发的目的,从一个递归的序列模型开始,我们最终会用到完整的 Transformer 模型.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\r\n",
    "# !unzip -q spa-eng.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "英语到西班牙语的样本数据.\r\n",
    "\r\n",
    "- 文本的每一行都是一个例子\r\n",
    "- 英语 制表符 西班牙语\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text_file = \"spa-eng/spa.txt\"\r\n",
    "with open(text_file, encoding='UTF-8') as f:\r\n",
    "    lines = f.read().split(\"\\n\")[:-1]\r\n",
    "text_pairs = []\r\n",
    "for line in lines:\r\n",
    "    english, spanish = line.split(\"\\t\")\r\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\r\n",
    "    text_pairs.append((english, spanish))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import random\r\n",
    "print(random.choice(text_pairs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(\"There wasn't much traffic.\", '[start] No había mucho tráfico. [end]')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "一个样本示例\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import random\r\n",
    "\r\n",
    "random.shuffle(text_pairs)  #随机排序\r\n",
    "num_val_samples = int(0.15 * len(text_pairs))  #15%的验证数据\r\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\r\n",
    "train_pairs = text_pairs[:num_train_samples]  #70%的训练数据\r\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples +\r\n",
    "                       num_val_samples]  #15%的验证数据\r\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]  #15%的测试数据\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "将整个样本分割: 70:15:15 训练:验证:测试\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "import tensorflow as tf\r\n",
    "import string\r\n",
    "import re\r\n",
    "\r\n",
    "strip_chars = string.punctuation + \"¿\"  # strip_chars = string.punctuation(所有标点符号) + ¿\r\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")  #不替换 [\r\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")  #不替换 ]\r\n",
    "\r\n",
    "\r\n",
    "def custom_standardization(input_string):  #全部小写,删除除 '[' ']' 以外的全部标点\r\n",
    "    lowercase = tf.strings.lower(input_string)  #小写\r\n",
    "    #                 按照正则表达式替换          格式化字符串 re.escape 自动添加转义\r\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\",\r\n",
    "                                    \"\")\r\n",
    "\r\n",
    "\r\n",
    "vocab_size = 15000  #设置最大词汇量\r\n",
    "sequence_length = 20  #设置最大句子长度\r\n",
    "\r\n",
    "source_vectorization = TextVectorization(  #英语\r\n",
    "    max_tokens=vocab_size,  #最大的词汇量\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length,\r\n",
    ")\r\n",
    "target_vectorization = TextVectorization(  #西班牙语\r\n",
    "    max_tokens=vocab_size,  #最大的词汇量\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length +\r\n",
    "    1,  #生成具有额外标记的西班牙语句子,我们需要在训练时将句子偏转一步(进行预测时 [start] 和 [end] 的缘故)\r\n",
    "    standardize=custom_standardization,\r\n",
    ")\r\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\r\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\r\n",
    "source_vectorization.adapt(train_english_texts)  #学习\r\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为语言差别,西班牙语和英语需要互相独立处理.西班牙语处理要传入自定义的字符串格式化函数.\r\n",
    "\r\n",
    "- 保留 '[' 和 ']',默认情况下 '[' 和 ']' 都会被删除,但是为了区分 `start` 和 `[start]` 我们需要保留 '[' 和 ']'.(这里是仅西班牙语中有)\r\n",
    "- 不同语言的标点符号有差别,如果英语中我们选择去掉标点符号,那么对应的西班牙语中的 `¿` 也需要删除.\r\n",
    "\r\n",
    "**警告**: 对于应用在生产环境的模型,一般应该将标点符号映射为独立的标记,而不是直接删除.如果直接删除,那输出译文不会有正确的标点符号.在我们的例子中仅仅是为了简化代码而删除了标点符号.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "batch_size = 64  #批次大小\r\n",
    "\r\n",
    "\r\n",
    "def format_dataset(eng, spa):\r\n",
    "    eng = source_vectorization(eng)  #英文 转换为向量\r\n",
    "    spa = target_vectorization(spa)  #西班牙文 转换为向量\r\n",
    "    return (\r\n",
    "        {\r\n",
    "            \"english\": eng,\r\n",
    "            \"spanish\": spa[:, :-1],  #输入西班牙语句子 不包括最好一个字符,保证输入与目标相同的长度\r\n",
    "        },\r\n",
    "        spa[:, 1:])  #目标西班牙语句子 提前一步, 输入/输出依然是相同的长度.\r\n",
    "\r\n",
    "\r\n",
    "def make_dataset(pairs):  #构造数据集\r\n",
    "    eng_texts, spa_texts = zip(*pairs)\r\n",
    "    eng_texts = list(eng_texts)\r\n",
    "    spa_texts = list(spa_texts)\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\r\n",
    "    dataset = dataset.batch(batch_size)\r\n",
    "    dataset = dataset.map(format_dataset)\r\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\r\n",
    "\r\n",
    "\r\n",
    "train_ds = make_dataset(train_pairs)\r\n",
    "val_ds = make_dataset(val_pairs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集是一个元组 (inputs, target),inputs 是字典,两个 key `encoder_inputs`(英语) 和 `decoder_inputs`(西班牙语)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "for inputs, targets in train_ds.take(1):\r\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\r\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\r\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "至此数据准备完毕.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RNN 序列到序列模型\r\n",
    "\r\n",
    "RNN 在 2015~2017 年主导了序列到序列模型的开发,然后其地位被 Transformer 取代.当时 RNN 是许多现实世界机器翻译系统的基础,例如 2017 年时 google 翻译就是由 7 个大型 ltsm 层堆叠的网络.但是今天这样的模型依旧值得学习,它提供了一个理解序列到序列模型的简单入口.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, ), dtype=\"int64\")\r\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\r\n",
    "x = layers.LSTM(32, return_sequences=True)(x)  #LSTM 返回序列\r\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  #输出\r\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "最直观的使用 rnn 的序列到序列模型就是每个迭代后,保持 rnn 的输出.但是这样做有几个问题\r\n",
    "\r\n",
    "- 目标序列长度必须和源序列长度一致,这在实际条件下非常少见,但是我们随时可以对序列进行填充到两者长度一致,因此这不是关键问题.\r\n",
    "- 由于 rnn 的特点,这样的模型预测目标序列 `0~N` 只关注源序列 `0~N` 的标记,则在翻译任务中无法适用.一个例子: 'he weather is nice today' 翻译为法语 'Il fait beau aujourd’hui'.模型需要仅通过  `The` 就能翻译出 `Il` ,通过 `The weather`  翻译出 `Il fait`,这几乎是不可能完成的.\r\n",
    "\r\n",
    "如果是人类译者,应该都会在写下译文前通读整个句子,特别是处理的语言在语序上非常不同时(例如: 英文和日文).而这也是标准序列到序列模型需要做的事情.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![rnn_seq2seq](rnn_seq2seq.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "如上图所示:\r\n",
    "\r\n",
    "- 首先要使用编码器(RNN),将输入整个源序列转换成单一张量(或一组张量).\r\n",
    "- 然后将这个张量(或者一组张量)传递给另外一个解码器(RNN),作为解码器的初始状态,它将查看目标序列的 0~N,并尝试预测目标序列到 N+1.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "embed_dim = 256  #词向量维度\r\n",
    "latent_dim = 1024  #隐藏层维度\r\n",
    "\r\n",
    "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\r\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\r\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim),\r\n",
    "                                      merge_mode=\"sum\")(x)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里编码器的实现: 使用 gru 代替了 ltsm 因为 gru 更加简单,只有一个状态向量.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\r\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)  #词嵌入\r\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\r\n",
    "x = decoder_gru(x, initial_state=encoded_source)  #gru\r\n",
    "x = layers.Dropout(0.5)(x)  #dropout 层\r\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(\r\n",
    "    x)  #密集层 每个输出步骤尝试西班牙语词的概率分布\r\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "解码器实现: 一个简单的 gru 层,gru 层将编码后的英文序列作为初始状态,在 gru 之后又接入了一个密集层,为每个步骤产生一个西班牙语词汇的概率分布.\r\n",
    "\r\n",
    "训练过程中,解码器将整个目标序列作为输入,但是由于 rnn 所以解码器是只看 0~N 预测标记 N,这个过程和前文处理时序数据时意义相同,即 rnn 中我们只能使用 '过去' 的数据预测 '未来',绝对不能打破这个过程,否则我们的模型无法完成翻译工作.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "seq2seq_rnn.compile(optimizer=\"rmsprop\",\r\n",
    "                    loss=\"sparse_categorical_crossentropy\",\r\n",
    "                    metrics=[\"accuracy\"])\r\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 131s 94ms/step - loss: 1.6413 - accuracy: 0.4153 - val_loss: 1.3122 - val_accuracy: 0.5051\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 123s 95ms/step - loss: 1.3216 - accuracy: 0.5248 - val_loss: 1.1524 - val_accuracy: 0.5680\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 133s 102ms/step - loss: 1.1800 - accuracy: 0.5739 - val_loss: 1.0718 - val_accuracy: 0.5998\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 132s 101ms/step - loss: 1.0885 - accuracy: 0.6059 - val_loss: 1.0333 - val_accuracy: 0.6182\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 132s 101ms/step - loss: 1.0430 - accuracy: 0.6299 - val_loss: 1.0207 - val_accuracy: 0.6287\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 1.0130 - accuracy: 0.6481 - val_loss: 1.0158 - val_accuracy: 0.6344\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9936 - accuracy: 0.6618 - val_loss: 1.0156 - val_accuracy: 0.6394\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9800 - accuracy: 0.6717 - val_loss: 1.0182 - val_accuracy: 0.6401\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 0.9699 - accuracy: 0.6798 - val_loss: 1.0192 - val_accuracy: 0.6418\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 128s 98ms/step - loss: 0.9619 - accuracy: 0.6857 - val_loss: 1.0241 - val_accuracy: 0.6424\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 131s 100ms/step - loss: 0.9565 - accuracy: 0.6903 - val_loss: 1.0264 - val_accuracy: 0.6429\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 120s 92ms/step - loss: 0.9521 - accuracy: 0.6937 - val_loss: 1.0286 - val_accuracy: 0.6435\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.9501 - accuracy: 0.6955 - val_loss: 1.0291 - val_accuracy: 0.6449\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9485 - accuracy: 0.6975 - val_loss: 1.0333 - val_accuracy: 0.6434\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 122s 94ms/step - loss: 0.9491 - accuracy: 0.6972 - val_loss: 1.0339 - val_accuracy: 0.6442\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f3b937a310>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里看准确率是 64.4%: 64% 的时间内正确预测了西班牙语的下一个单词...\r\n",
    "\r\n",
    "但是评价机器翻译,但是预测下一个词的正确率这个标准并不适合.特别是这个标准的一个假设前提: 预测前 N+1 个标记时,已经知道 0~N 的正确目标标记了.实际上在预测过程中,我们是从头生成目标序列,不能依赖以前生成标记是 100% 准确的.\r\n",
    "\r\n",
    "在现实世界中考察机器翻译,一般会使用 BLEU 分数,这个是考察整个生成序列的指标,似乎与人类对翻译质量的感知正相关.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "spa_vocab = target_vectorization.get_vocabulary()\r\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\r\n",
    "max_decoded_sentence_length = 20\r\n",
    "\r\n",
    "\r\n",
    "def decode_sequence(input_sentence):\r\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\r\n",
    "    decoded_sentence = \"[start]\"\r\n",
    "    for i in range(max_decoded_sentence_length):\r\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\r\n",
    "        next_token_predictions = seq2seq_rnn.predict(\r\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\r\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\r\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\r\n",
    "        decoded_sentence += \" \" + sampled_token\r\n",
    "        if sampled_token == \"[end]\":\r\n",
    "            break\r\n",
    "    return decoded_sentence\r\n",
    "\r\n",
    "\r\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\r\n",
    "for _ in range(20):  #随机取了 20组\r\n",
    "    input_sentence = random.choice(test_eng_texts)\r\n",
    "    print(\"-\")\r\n",
    "    print(input_sentence)\r\n",
    "    print(decode_sequence(input_sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-\n",
      "The enemy dropped bombs on the factory.\n",
      "[start] el profesor se dijo en la guerra [end]\n",
      "-\n",
      "She had white shoes on.\n",
      "[start] ella tenía los zapatos [UNK] [end]\n",
      "-\n",
      "The letter does not say what time she will come up to Tokyo.\n",
      "[start] la carta no se va a decir que ella había ido a tokio [end]\n",
      "-\n",
      "We are having a mild winter.\n",
      "[start] estamos en un año [end]\n",
      "-\n",
      "What are your responsibilities?\n",
      "[start] cuál son tus cosas [end]\n",
      "-\n",
      "Tom fed the goats.\n",
      "[start] tom de [UNK] a las [UNK] [end]\n",
      "-\n",
      "Tom has a pain in his big toe on his right foot.\n",
      "[start] tom tiene un gran en su [UNK] en su gran estoy de la un todo el trabajo [end]\n",
      "-\n",
      "New Zealand is pretty incredible.\n",
      "[start] [UNK] nueva es [UNK] [end]\n",
      "-\n",
      "I found a place to live.\n",
      "[start] encontré un lugar para vivir [end]\n",
      "-\n",
      "They all left.\n",
      "[start] todos se [UNK] [end]\n",
      "-\n",
      "Japan is not as large as Canada.\n",
      "[start] japón no es tan grande como un aquí [end]\n",
      "-\n",
      "Tom isn't sure.\n",
      "[start] tom no está seguro [end]\n",
      "-\n",
      "Tom is a thirteen-year-old boy.\n",
      "[start] tom es un hombre de por qué edad [end]\n",
      "-\n",
      "Tom is more talented than I am.\n",
      "[start] tom es más que yo [end]\n",
      "-\n",
      "He kept on telling lies.\n",
      "[start] Él se [UNK] a hacer nada [end]\n",
      "-\n",
      "I've been preparing.\n",
      "[start] he estado [end]\n",
      "-\n",
      "I'm opposed to any type of war.\n",
      "[start] me [UNK] en una guerra de guerra [end]\n",
      "-\n",
      "This door will not open.\n",
      "[start] esta puerta no [end]\n",
      "-\n",
      "We have plenty of time.\n",
      "[start] tenemos un montón de tiempo [end]\n",
      "-\n",
      "The one and only dessert my son eats is chocolate cake.\n",
      "[start] el único que mi hijo es el que mi hijo y yo es un poco de dinero [end]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里直接取了 20 组结果,看不懂西班牙文,所以无法人肉评价.书上作者的评价是作为一个玩具而言,效果还可以,尽管还有很多错误.\r\n",
    "\r\n",
    "注意: 这里的代码非常简单,但是效率却很低,我们每次采样一个新词就会重新处理整个源序列和整个生成的目标序列,实际应用中会将编码器和解码器拆开成两个独立模型,解码器每次标记采样迭代时只运行一个步骤,重用其中的内部状态.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于这个玩具模型的改进\r\n",
    "\r\n",
    "- 编码器/解码器 堆叠更多的 rnn 层.\r\n",
    "- 使用 ltsm 而不是 gru\r\n",
    "- 等等\r\n",
    "\r\n",
    "然而 rnn 的序列到序列模型有一些天然的劣势\r\n",
    "\r\n",
    "- 源序列必须完全保存在编码器的状态向量中,这对模型能够翻译的句子大小和复杂性有非常大的限制.这有点像人类翻译句子仅凭记忆的知识,而不是多看几遍原句.\r\n",
    "- rnn 在比较难以处理长序列.rnn 常常会遗忘过去.当你的序列长度超过 100 时,第 100 个标记输入模型,然而此时模型几乎没有序列一开始输入的信息了.这意味着 rnn 模型无法长时间保持上下文,这个缺点翻译长篇文章是致命的.\r\n",
    "\r\n",
    "上面的限制是 2017 年以后业内从 rnn 大量转向 Transformer 的原因.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf')"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}