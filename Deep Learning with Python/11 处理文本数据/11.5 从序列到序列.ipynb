{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5 从序列到序列\r\n",
    "\r\n",
    "到目前为止,我们已经接触过了大多数 nlp 任务所需要的工具,然而我们只在处理过一类 nlp 任务--文本分类,这一节我们将接触另一种序列到序列的模型.\r\n",
    "\r\n",
    "序列到序列模型是将一个序列作为输入(可以是一个句子或一段文字)并将其翻译成不同的序列,这是许多 nlp 的核心任务.\r\n",
    "\r\n",
    "- 机器翻译: 将源样本的段落转换为目标语言的对应段落.\r\n",
    "- 文本汇总: 将长文本进行总结,只保留最重要信息.\r\n",
    "- 问题问答: 输入问题,返回答案\r\n",
    "- 聊天机器人: 输入对话/历史聊天记录等等转换为下一个回复\r\n",
    "- 文本生成: 将输入文本转换为一个完整的段落.\r\n",
    "- 等等...\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![seq2seq](seq2seq.png)\r\n",
    "\r\n",
    "上图显示了序列到序列模型工作的一般流程\r\n",
    "\r\n",
    "训练期\r\n",
    "\r\n",
    "- 一个编码器将源序列转换为一种中间表示.\r\n",
    "- 通过查看之前的标记和源序列,解码器被用来预测下一个标记.\r\n",
    "\r\n",
    "预测,预测时我们无法接触到目标序列,我们不得不一次生成一个标记.\r\n",
    "\r\n",
    "- 编码器将源序列编码转换\r\n",
    "- 解码器查看源序列和初始'种子'标记(图中是 '[start]'),并预测下一个真实标记.\r\n",
    "- 将真实标记送入解码器推动下一次预测,直到生成一个停止标记(图中是 '[end]').\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一个机器翻译例子\r\n",
    "\r\n",
    "我们将在一个机器翻译的问题上演示序列到序列的模型.机器翻译正是 Transformer 开发的目的,从一个递归的序列模型开始,我们最终会用到完整的 Transformer 模型.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\r\n",
    "# !unzip -q spa-eng.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "英语到西班牙语的样本数据.\r\n",
    "\r\n",
    "- 文本的每一行都是一个例子\r\n",
    "- 英语 制表符 西班牙语\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "text_file = \"spa-eng/spa.txt\"\r\n",
    "with open(text_file, encoding='UTF-8') as f:\r\n",
    "    lines = f.read().split(\"\\n\")[:-1]\r\n",
    "text_pairs = []\r\n",
    "for line in lines:\r\n",
    "    english, spanish = line.split(\"\\t\")\r\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\r\n",
    "    text_pairs.append((english, spanish))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import random\r\n",
    "print(random.choice(text_pairs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(\"It's wrong to deceive people, but worse to deceive yourself.\", '[start] Está mal engañar a la gente, pero es aún peor engañarte a ti mismo. [end]')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "一个样本示例\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import random\r\n",
    "\r\n",
    "random.shuffle(text_pairs)  #随机排序\r\n",
    "num_val_samples = int(0.15 * len(text_pairs))  #15%的验证数据\r\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\r\n",
    "train_pairs = text_pairs[:num_train_samples]  #70%的训练数据\r\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples +\r\n",
    "                       num_val_samples]  #15%的验证数据\r\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]  #15%的测试数据\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "将整个样本分割: 70:15:15 训练:验证:测试\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "import tensorflow as tf\r\n",
    "import string\r\n",
    "import re\r\n",
    "\r\n",
    "strip_chars = string.punctuation + \"¿\"  # strip_chars = string.punctuation(所有标点符号) + ¿\r\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")  #不替换 [\r\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")  #不替换 ]\r\n",
    "\r\n",
    "\r\n",
    "def custom_standardization(input_string):  #全部小写,删除除 '[' ']' 以外的全部标点\r\n",
    "    lowercase = tf.strings.lower(input_string)  #小写\r\n",
    "    #                 按照正则表达式替换          格式化字符串 re.escape 自动添加转义\r\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\",\r\n",
    "                                    \"\")\r\n",
    "\r\n",
    "\r\n",
    "vocab_size = 15000  #设置最大词汇量\r\n",
    "sequence_length = 20  #设置最大句子长度\r\n",
    "\r\n",
    "source_vectorization = TextVectorization(  #英语\r\n",
    "    max_tokens=vocab_size,  #最大的词汇量\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length,\r\n",
    ")\r\n",
    "target_vectorization = TextVectorization(  #西班牙语\r\n",
    "    max_tokens=vocab_size,  #最大的词汇量\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length +\r\n",
    "    1,  #生成具有额外标记的西班牙语句子,我们需要在训练时将句子偏转一步(进行预测时 [start] 和 [end] 的缘故)\r\n",
    "    standardize=custom_standardization,\r\n",
    ")\r\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\r\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\r\n",
    "source_vectorization.adapt(train_english_texts)  #学习\r\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为语言差别,西班牙语和英语需要互相独立处理.西班牙语处理要传入自定义的字符串格式化函数.\r\n",
    "\r\n",
    "- 保留 '[' 和 ']',默认情况下 '[' 和 ']' 都会被删除,但是为了区分 `start` 和 `[start]` 我们需要保留 '[' 和 ']'.(这里是仅西班牙语中有)\r\n",
    "- 不同语言的标点符号有差别,如果英语中我们选择去掉标点符号,那么对应的西班牙语中的 `¿` 也需要删除.\r\n",
    "\r\n",
    "**警告**: 对于应用在生产环境的模型,一般应该将标点符号映射为独立的标记,而不是直接删除.如果直接删除,那输出译文不会有正确的标点符号.在我们的例子中仅仅是为了简化代码而删除了标点符号.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "batch_size = 64  #批次大小\r\n",
    "\r\n",
    "\r\n",
    "def format_dataset(eng, spa):\r\n",
    "    eng = source_vectorization(eng)  #英文 转换为向量\r\n",
    "    spa = target_vectorization(spa)  #西班牙文 转换为向量\r\n",
    "    return (\r\n",
    "        {\r\n",
    "            \"english\": eng,\r\n",
    "            \"spanish\": spa[:, :-1],  #输入西班牙语句子 不包括最好一个字符,保证输入与目标相同的长度\r\n",
    "        },\r\n",
    "        spa[:, 1:])  #目标西班牙语句子 提前一步, 输入/输出依然是相同的长度.\r\n",
    "\r\n",
    "\r\n",
    "def make_dataset(pairs):  #构造数据集\r\n",
    "    eng_texts, spa_texts = zip(*pairs)\r\n",
    "    eng_texts = list(eng_texts)\r\n",
    "    spa_texts = list(spa_texts)\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\r\n",
    "    dataset = dataset.batch(batch_size)\r\n",
    "    dataset = dataset.map(format_dataset)\r\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\r\n",
    "\r\n",
    "\r\n",
    "train_ds = make_dataset(train_pairs)\r\n",
    "val_ds = make_dataset(val_pairs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "数据集是一个元组 (inputs, target),inputs 是字典,两个 key `encoder_inputs`(英语) 和 `decoder_inputs`(西班牙语)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for inputs, targets in train_ds.take(1):\r\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\r\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\r\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "至此数据准备完毕.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RNN 序列到序列模型\r\n",
    "\r\n",
    "RNN 在 2015~2017 年主导了序列到序列模型的开发,然后其地位被 Transformer 取代.当时 RNN 是许多现实世界机器翻译系统的基础,例如 2017 年时 google 翻译就是由 7 个大型 ltsm 层堆叠的网络.但是今天这样的模型依旧值得学习,它提供了一个理解序列到序列模型的简单入口.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, ), dtype=\"int64\")\r\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\r\n",
    "x = layers.LSTM(32, return_sequences=True)(x)  #LSTM 返回序列\r\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  #输出\r\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "最直观的使用 rnn 的序列到序列模型就是每个迭代后,保持 rnn 的输出.但是这样做有几个问题\r\n",
    "\r\n",
    "- 目标序列长度必须和源序列长度一致,这在实际条件下非常少见,但是我们随时可以对序列进行填充到两者长度一致,因此这不是关键问题.\r\n",
    "- 由于 rnn 的特点,这样的模型预测目标序列 `0~N` 只关注源序列 `0~N` 的标记,则在翻译任务中无法适用.一个例子: 'he weather is nice today' 翻译为法语 'Il fait beau aujourd’hui'.模型需要仅通过  `The` 就能翻译出 `Il` ,通过 `The weather`  翻译出 `Il fait`,这几乎是不可能完成的.\r\n",
    "\r\n",
    "如果是人类译者,应该都会在写下译文前通读整个句子,特别是处理的语言在语序上非常不同时(例如: 英文和日文).而这也是标准序列到序列模型需要做的事情.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![rnn_seq2seq](rnn_seq2seq.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "如上图所示:\r\n",
    "\r\n",
    "- 首先要使用编码器(RNN),将输入整个源序列转换成单一张量(或一组张量).\r\n",
    "- 然后将这个张量(或者一组张量)传递给另外一个解码器(RNN),作为解码器的初始状态,它将查看目标序列的 0~N,并尝试预测目标序列到 N+1.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "embed_dim = 256  #词向量维度\r\n",
    "latent_dim = 1024  #隐藏层维度\r\n",
    "\r\n",
    "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\r\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\r\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim),\r\n",
    "                                      merge_mode=\"sum\")(x)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里编码器的实现: 使用 gru 代替了 ltsm 因为 gru 更加简单,只有一个状态向量.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\r\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)  #词嵌入\r\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\r\n",
    "x = decoder_gru(x, initial_state=encoded_source)  #gru\r\n",
    "x = layers.Dropout(0.5)(x)  #dropout 层\r\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(\r\n",
    "    x)  #密集层 每个输出步骤尝试西班牙语词的概率分布\r\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "解码器实现: 一个简单的 gru 层,gru 层将编码后的英文序列作为初始状态,在 gru 之后又接入了一个密集层,为每个步骤产生一个西班牙语词汇的概率分布.\r\n",
    "\r\n",
    "训练过程中,解码器将整个目标序列作为输入,但是由于 rnn 所以解码器是只看 0~N 预测标记 N,这个过程和前文处理时序数据时意义相同,即 rnn 中我们只能使用 '过去' 的数据预测 '未来',绝对不能打破这个过程,否则我们的模型无法完成翻译工作.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "seq2seq_rnn.compile(optimizer=\"rmsprop\",\r\n",
    "                    loss=\"sparse_categorical_crossentropy\",\r\n",
    "                    metrics=[\"accuracy\"])\r\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 131s 94ms/step - loss: 1.6413 - accuracy: 0.4153 - val_loss: 1.3122 - val_accuracy: 0.5051\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 123s 95ms/step - loss: 1.3216 - accuracy: 0.5248 - val_loss: 1.1524 - val_accuracy: 0.5680\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 133s 102ms/step - loss: 1.1800 - accuracy: 0.5739 - val_loss: 1.0718 - val_accuracy: 0.5998\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 132s 101ms/step - loss: 1.0885 - accuracy: 0.6059 - val_loss: 1.0333 - val_accuracy: 0.6182\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 132s 101ms/step - loss: 1.0430 - accuracy: 0.6299 - val_loss: 1.0207 - val_accuracy: 0.6287\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 1.0130 - accuracy: 0.6481 - val_loss: 1.0158 - val_accuracy: 0.6344\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9936 - accuracy: 0.6618 - val_loss: 1.0156 - val_accuracy: 0.6394\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9800 - accuracy: 0.6717 - val_loss: 1.0182 - val_accuracy: 0.6401\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 0.9699 - accuracy: 0.6798 - val_loss: 1.0192 - val_accuracy: 0.6418\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 128s 98ms/step - loss: 0.9619 - accuracy: 0.6857 - val_loss: 1.0241 - val_accuracy: 0.6424\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 131s 100ms/step - loss: 0.9565 - accuracy: 0.6903 - val_loss: 1.0264 - val_accuracy: 0.6429\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 120s 92ms/step - loss: 0.9521 - accuracy: 0.6937 - val_loss: 1.0286 - val_accuracy: 0.6435\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.9501 - accuracy: 0.6955 - val_loss: 1.0291 - val_accuracy: 0.6449\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9485 - accuracy: 0.6975 - val_loss: 1.0333 - val_accuracy: 0.6434\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 122s 94ms/step - loss: 0.9491 - accuracy: 0.6972 - val_loss: 1.0339 - val_accuracy: 0.6442\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f3b937a310>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里看准确率是 64.4%: 64% 的时间内正确预测了西班牙语的下一个单词...\r\n",
    "\r\n",
    "但是评价机器翻译,但是预测下一个词的正确率这个标准并不适合.特别是这个标准的一个假设前提: 预测前 N+1 个标记时,已经知道 0~N 的正确目标标记了.实际上在预测过程中,我们是从头生成目标序列,不能依赖以前生成标记是 100% 准确的.\r\n",
    "\r\n",
    "在现实世界中考察机器翻译,一般会使用 BLEU 分数,这个是考察整个生成序列的指标,似乎与人类对翻译质量的感知正相关.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "spa_vocab = target_vectorization.get_vocabulary()\r\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\r\n",
    "max_decoded_sentence_length = 20\r\n",
    "\r\n",
    "\r\n",
    "def decode_sequence(input_sentence):\r\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\r\n",
    "    decoded_sentence = \"[start]\"\r\n",
    "    for i in range(max_decoded_sentence_length):\r\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\r\n",
    "        next_token_predictions = seq2seq_rnn.predict(\r\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\r\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\r\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\r\n",
    "        decoded_sentence += \" \" + sampled_token\r\n",
    "        if sampled_token == \"[end]\":\r\n",
    "            break\r\n",
    "    return decoded_sentence\r\n",
    "\r\n",
    "\r\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\r\n",
    "for _ in range(20):  #随机取了 20组\r\n",
    "    input_sentence = random.choice(test_eng_texts)\r\n",
    "    print(\"-\")\r\n",
    "    print(input_sentence)\r\n",
    "    print(decode_sequence(input_sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-\n",
      "The enemy dropped bombs on the factory.\n",
      "[start] el profesor se dijo en la guerra [end]\n",
      "-\n",
      "She had white shoes on.\n",
      "[start] ella tenía los zapatos [UNK] [end]\n",
      "-\n",
      "The letter does not say what time she will come up to Tokyo.\n",
      "[start] la carta no se va a decir que ella había ido a tokio [end]\n",
      "-\n",
      "We are having a mild winter.\n",
      "[start] estamos en un año [end]\n",
      "-\n",
      "What are your responsibilities?\n",
      "[start] cuál son tus cosas [end]\n",
      "-\n",
      "Tom fed the goats.\n",
      "[start] tom de [UNK] a las [UNK] [end]\n",
      "-\n",
      "Tom has a pain in his big toe on his right foot.\n",
      "[start] tom tiene un gran en su [UNK] en su gran estoy de la un todo el trabajo [end]\n",
      "-\n",
      "New Zealand is pretty incredible.\n",
      "[start] [UNK] nueva es [UNK] [end]\n",
      "-\n",
      "I found a place to live.\n",
      "[start] encontré un lugar para vivir [end]\n",
      "-\n",
      "They all left.\n",
      "[start] todos se [UNK] [end]\n",
      "-\n",
      "Japan is not as large as Canada.\n",
      "[start] japón no es tan grande como un aquí [end]\n",
      "-\n",
      "Tom isn't sure.\n",
      "[start] tom no está seguro [end]\n",
      "-\n",
      "Tom is a thirteen-year-old boy.\n",
      "[start] tom es un hombre de por qué edad [end]\n",
      "-\n",
      "Tom is more talented than I am.\n",
      "[start] tom es más que yo [end]\n",
      "-\n",
      "He kept on telling lies.\n",
      "[start] Él se [UNK] a hacer nada [end]\n",
      "-\n",
      "I've been preparing.\n",
      "[start] he estado [end]\n",
      "-\n",
      "I'm opposed to any type of war.\n",
      "[start] me [UNK] en una guerra de guerra [end]\n",
      "-\n",
      "This door will not open.\n",
      "[start] esta puerta no [end]\n",
      "-\n",
      "We have plenty of time.\n",
      "[start] tenemos un montón de tiempo [end]\n",
      "-\n",
      "The one and only dessert my son eats is chocolate cake.\n",
      "[start] el único que mi hijo es el que mi hijo y yo es un poco de dinero [end]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里直接取了 20 组结果,看不懂西班牙文,所以无法人肉评价.书上作者的评价是作为一个玩具而言,效果还可以,尽管还有很多错误.\r\n",
    "\r\n",
    "注意: 这里的代码非常简单,但是效率却很低,我们每次采样一个新词就会重新处理整个源序列和整个生成的目标序列,实际应用中会将编码器和解码器拆开成两个独立模型,解码器每次标记采样迭代时只运行一个步骤,重用其中的内部状态.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "关于这个玩具模型的改进\r\n",
    "\r\n",
    "- 编码器/解码器 堆叠更多的 rnn 层.\r\n",
    "- 使用 ltsm 而不是 gru\r\n",
    "- 等等\r\n",
    "\r\n",
    "然而 rnn 的序列到序列模型有一些天然的劣势\r\n",
    "\r\n",
    "- 源序列必须完全保存在编码器的状态向量中,这对模型能够翻译的句子大小和复杂性有非常大的限制.这有点像人类翻译句子仅凭记忆的知识,而不是多看几遍原句.\r\n",
    "- rnn 在比较难以处理长序列.rnn 常常会遗忘过去.当你的序列长度超过 100 时,第 100 个标记输入模型,然而此时模型几乎没有序列一开始输入的信息了.这意味着 rnn 模型无法长时间保持上下文,这个缺点翻译长篇文章是致命的.\r\n",
    "\r\n",
    "上面的限制是 2017 年以后业内从 rnn 大量转向 Transformer 的原因.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer 序列到序列模型\r\n",
    "\r\n",
    "序列到序列的模型是 Transformer 真正能发挥作用的地方,Neural attention 使得 Transformer 模型能比 rnn 更成功处理更复杂/更长序列.\r\n",
    "\r\n",
    "人肉将英语翻译为西班牙语时,我们不可能一个词一个词的读英文记住,再一个词一个词的翻译成西班牙语,这样翻译方式对一句话5个词,这样的文本还行,对付一大段文字就无能为力了.实际上人肉翻译时(比如现在,打下译文),我们会在译文和原句之间反复横跳,当写下译文时会参考原句/原句上下很多不同的词语构成的语境.\r\n",
    "\r\n",
    "上面那样人肉翻译的模式,正是可以通过 neural attention 和 Transformer 实现的.上一节我们已经熟悉了 Transformer 的编码器,编码器使用 elf-attention 实现对输入序列上下文感知的表示.在 Transformer 实现的序列到序列的模型中,编码器自然还是编码器的角色,读取源序列并输出源序列的编码表示.与 rnn 模型不同的是,Transformer 编码器的输出依旧是序列(一串上下文感知的嵌入向量)\r\n",
    "\r\n",
    "Transformer 序列到序列模型的第二个部分是 Transformer 解码器,如同 rnn 解码器,它会读取目标序列的 0~N 试图预测标记的 N+1.最重要的是在预测时候,解码器会通过 neural attention 识别源序列中哪些标记与目前试图预测标记最密切(或许还和人工翻译不那么一样).回顾一下 查询-键-值模型: Transformer 解码器中,目标序列作为了 查询用来关注源序列的不同部分(源序列承担了 键\r\n",
    " 和 值).\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![transformer](transformer.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "上图完整显示了 Transformer 序列到序列模型,仔细看一下解码器内部,似乎与编码器非常相似,除了插入了额外的 self-attention 层.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer 解码器\r\n",
    "\r\n",
    "与之前一样,我们要实现 Transformer 解码器,\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class TransformerDecoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.dense_dim = dense_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads,\r\n",
    "                                                     key_dim=embed_dim)\r\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads,\r\n",
    "                                                     key_dim=embed_dim)\r\n",
    "        self.dense_proj = keras.Sequential([\r\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\r\n",
    "            layers.Dense(embed_dim),\r\n",
    "        ])\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()\r\n",
    "        self.layernorm_3 = layers.LayerNormalization()\r\n",
    "        self.supports_masking = True  #这个属性确保了该层将输入序列的 mask 传播到输出上.\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "        return config\r\n",
    "\r\n",
    "    def get_causal_attention_mask(self, inputs):\r\n",
    "        input_shape = tf.shape(inputs)\r\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "        j = tf.range(sequence_length)\r\n",
    "        mask = tf.cast(\r\n",
    "            i >= j,\r\n",
    "            dtype=\"int32\")  #产生形状 （sequence_length, sequence_length）的 0-1 矩阵\r\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\r\n",
    "        mult = tf.concat(\r\n",
    "            [\r\n",
    "                tf.expand_dims(batch_size, -1),\r\n",
    "                tf.constant([1, 1], dtype=tf.int32)\r\n",
    "            ],\r\n",
    "            axis=0\r\n",
    "        )  #沿着批次轴复制它，得到一个形状矩阵（batch_size, sequence_length, sequence_length）。\r\n",
    "        return tf.tile(mask, mult)\r\n",
    "\r\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)  #获取 causal_mask\r\n",
    "        if mask is not None:  #准备输入掩码\r\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\r\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)  #合并掩码\r\n",
    "        attention_output_1 = self.attention_1(\r\n",
    "            query=inputs, value=inputs, key=inputs,\r\n",
    "            attention_mask=causal_mask)  # 因果掩码传递给 self-attention\r\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "        attention_output_2 = self.attention_2(\r\n",
    "            query=attention_output_1,\r\n",
    "            value=encoder_outputs,\r\n",
    "            key=encoder_outputs,\r\n",
    "            attention_mask=padding_mask,  #将合并后的掩码传递给第二注意力层，该层将源序列与目标序列联系起来。\r\n",
    "        )\r\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 +\r\n",
    "                                              attention_output_2)\r\n",
    "        proj_output = self.dense_proj(attention_output_2)\r\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "值得说的是 `self.supports_masking = True`,keras 并没有默认启用 mask 支持,你无法将 mask 数组传递会没有实现 `compute_mask` 没有暴露 `supports_masking` 属性的层,这会导致一个报错.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```py\r\n",
    "def get_causal_attention_mask(self, inputs):\r\n",
    "    input_shape = tf.shape(inputs)\r\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "    i = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "    j = tf.range(sequence_length)\r\n",
    "    mask = tf.cast(\r\n",
    "        i >= j,\r\n",
    "        dtype=\"int32\")  #产生形状 （sequence_length, sequence_length）的 0-1 矩阵\r\n",
    "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\r\n",
    "    mult = tf.concat(\r\n",
    "        [tf.expand_dims(batch_size, -1),\r\n",
    "         tf.constant([1, 1], dtype=tf.int32)],\r\n",
    "        axis=0\r\n",
    "    )  #沿着批次轴复制它，得到一个形状矩阵（batch_size, sequence_length, sequence_length）。\r\n",
    "    return tf.tile(mask, mult)\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "与 RNN 不同 Transformer 解码器解码器一次性看到全部输入.RNN 解码器每次只看一个步骤的输入,只能访问当前步骤的 0~N 预测当前步骤输出 N (目标序列的 N+1).Transformer 的解码器是不区分先后的,一次性看到整个目标序列,如果不加限制允许它使用整个输入,解码器最终会直接将输入的 N + 1 复制到输出 N 中,这样会使得模型准确度达到 100%.但是允许预测时,这样的模型是完全无用的,预测推理时,根本没有 N+1 的输入.\r\n",
    "\r\n",
    "解决这个问题一个简单的方法: 屏蔽模型对此次迭代后续数据的访问,将输入顺序看作是时序后,屏蔽任何对未来信息的访问.\r\n",
    "\r\n",
    "这个解决方案对应 `get_causal_attention_mask` 方法.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```py\r\n",
    "def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "    causal_mask = self.get_causal_attention_mask(inputs)  #获取 causal_mask\r\n",
    "    if mask is not None:  #准备输入掩码\r\n",
    "        padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\r\n",
    "        padding_mask = tf.minimum(padding_mask, causal_mask)  #合并掩码\r\n",
    "    attention_output_1 = self.attention_1(\r\n",
    "        query=inputs, value=inputs, key=inputs,\r\n",
    "        attention_mask=causal_mask)  # 因果掩码传递给 self-attention\r\n",
    "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "    attention_output_2 = self.attention_2(\r\n",
    "        query=attention_output_1,\r\n",
    "        value=encoder_outputs,\r\n",
    "        key=encoder_outputs,\r\n",
    "        attention_mask=padding_mask,  #将合并后的掩码传递给第二注意力层，该层将源序列与目标序列联系起来。\r\n",
    "    )\r\n",
    "    attention_output_2 = self.layernorm_2(attention_output_1 +\r\n",
    "                                          attention_output_2)\r\n",
    "    proj_output = self.dense_proj(attention_output_2)\r\n",
    "    return self.layernorm_3(attention_output_2 + proj_output)\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 组合\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "\r\n",
    "class TransformerEncoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim  #输入的维度\r\n",
    "        self.dense_dim = dense_dim  #密集层大小\r\n",
    "        self.num_heads = num_heads  #head 的数量\r\n",
    "        self.attention = layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim)  #MultiHeadAttention 层\r\n",
    "        self.dense_proj = keras.Sequential([\r\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\r\n",
    "            layers.Dense(embed_dim),\r\n",
    "        ])  #密集层\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()  #归一化层\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()  #归一化层\r\n",
    "\r\n",
    "    def call(self, inputs, mask=None):  #实现 call 方法\r\n",
    "        if mask is not None:  # 小细节: 词嵌入层生成的 mask 是二维的,但是 attention 层的期望是 3/4 维,这里要扩大 mask 维度\r\n",
    "            mask = mask[:, tf.newaxis, :]\r\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\r\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\r\n",
    "        proj_output = self.dense_proj(proj_input)\r\n",
    "        return self.layernorm_2(proj_input + proj_output)\r\n",
    "\r\n",
    "    def get_config(self):  #序列化,可以通过这个方法保存模型\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class PositionalEmbedding(layers.Layer):\r\n",
    "    def __init__(self, sequence_length, input_dim, output_dim,\r\n",
    "                 **kwargs):  #位置嵌入缺点是必须事先知道序列长度\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim,\r\n",
    "                                                 output_dim=output_dim)  #嵌入层\r\n",
    "        self.position_embeddings = layers.Embedding(\r\n",
    "            input_dim=sequence_length, output_dim=output_dim)  #标记位置嵌入层\r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        length = tf.shape(inputs)[-1]\r\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\r\n",
    "        embedded_tokens = self.token_embeddings(inputs)\r\n",
    "        embedded_positions = self.position_embeddings(positions)\r\n",
    "        return embedded_tokens + embedded_positions  #两个嵌入层相加\r\n",
    "\r\n",
    "    def compute_mask(\r\n",
    "            self,\r\n",
    "            inputs,\r\n",
    "            mask=None):  #与直接使用 Embedding 一样,这个层应该能够生成 mask,这样就能忽略掉填充的数据,减少运算量.\r\n",
    "        return tf.math.not_equal(inputs, 0)\r\n",
    "\r\n",
    "    def get_config(self):  #保存模型时调用\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"output_dim\": self.output_dim,\r\n",
    "            \"sequence_length\": self.sequence_length,\r\n",
    "            \"input_dim\": self.input_dim,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "embed_dim = 256\r\n",
    "dense_dim = 2048\r\n",
    "num_heads = 8\r\n",
    "\r\n",
    "encoder_inputs = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\r\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)  #编码器\r\n",
    "\r\n",
    "decoder_inputs = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\r\n",
    "x = TransformerDecoder(embed_dim, dense_dim,\r\n",
    "                       num_heads)(x, encoder_outputs)  #对目标序列进行编码,将其与源结合\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  #为输出位置预测\r\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "transformer.compile(optimizer=\"rmsprop\",\r\n",
    "                    loss=\"sparse_categorical_crossentropy\",\r\n",
    "                    metrics=[\"accuracy\"])\r\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 183s 137ms/step - loss: 1.6965 - accuracy: 0.4098 - val_loss: 1.3724 - val_accuracy: 0.4975\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 165s 127ms/step - loss: 1.3630 - accuracy: 0.5233 - val_loss: 1.1919 - val_accuracy: 0.5589\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 145s 112ms/step - loss: 1.2038 - accuracy: 0.5746 - val_loss: 1.1039 - val_accuracy: 0.5928\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 147s 113ms/step - loss: 1.1094 - accuracy: 0.6091 - val_loss: 1.0418 - val_accuracy: 0.6235\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 164s 126ms/step - loss: 1.0541 - accuracy: 0.6323 - val_loss: 1.0291 - val_accuracy: 0.6290\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0239 - accuracy: 0.6489 - val_loss: 1.0184 - val_accuracy: 0.6348\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 162s 124ms/step - loss: 0.9988 - accuracy: 0.6630 - val_loss: 1.0128 - val_accuracy: 0.6411\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 151s 116ms/step - loss: 0.9789 - accuracy: 0.6740 - val_loss: 1.0006 - val_accuracy: 0.6481\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 169s 130ms/step - loss: 0.9604 - accuracy: 0.6837 - val_loss: 1.0008 - val_accuracy: 0.6503\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 162s 125ms/step - loss: 0.9451 - accuracy: 0.6920 - val_loss: 1.0041 - val_accuracy: 0.6511\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 144s 110ms/step - loss: 0.9288 - accuracy: 0.6997 - val_loss: 1.0005 - val_accuracy: 0.6549\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 140s 108ms/step - loss: 0.9167 - accuracy: 0.7060 - val_loss: 1.0083 - val_accuracy: 0.6564\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 141s 108ms/step - loss: 0.9039 - accuracy: 0.7123 - val_loss: 0.9963 - val_accuracy: 0.6585\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 162s 125ms/step - loss: 0.8904 - accuracy: 0.7181 - val_loss: 1.0068 - val_accuracy: 0.6595\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 158s 121ms/step - loss: 0.8781 - accuracy: 0.7226 - val_loss: 1.0184 - val_accuracy: 0.6546\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 155s 119ms/step - loss: 0.8687 - accuracy: 0.7273 - val_loss: 1.0150 - val_accuracy: 0.6596\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.8577 - accuracy: 0.7319 - val_loss: 1.0183 - val_accuracy: 0.6595\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 105s 81ms/step - loss: 0.8472 - accuracy: 0.7362 - val_loss: 1.0214 - val_accuracy: 0.6607\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.8356 - accuracy: 0.7401 - val_loss: 1.0271 - val_accuracy: 0.6602\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 111s 85ms/step - loss: 0.8287 - accuracy: 0.7431 - val_loss: 1.0303 - val_accuracy: 0.6617\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.8195 - accuracy: 0.7466 - val_loss: 1.0303 - val_accuracy: 0.6638\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 111s 86ms/step - loss: 0.8103 - accuracy: 0.7503 - val_loss: 1.0302 - val_accuracy: 0.6600\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 113s 87ms/step - loss: 0.8002 - accuracy: 0.7530 - val_loss: 1.0343 - val_accuracy: 0.6636\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 113s 87ms/step - loss: 0.7924 - accuracy: 0.7566 - val_loss: 1.0476 - val_accuracy: 0.6610\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.7869 - accuracy: 0.7592 - val_loss: 1.0503 - val_accuracy: 0.6614\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.7793 - accuracy: 0.7617 - val_loss: 1.0576 - val_accuracy: 0.6614\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.7718 - accuracy: 0.7639 - val_loss: 1.0732 - val_accuracy: 0.6601\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 115s 88ms/step - loss: 0.7639 - accuracy: 0.7671 - val_loss: 1.0640 - val_accuracy: 0.6613\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 115s 89ms/step - loss: 0.7565 - accuracy: 0.7691 - val_loss: 1.0769 - val_accuracy: 0.6611\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 109s 83ms/step - loss: 0.7530 - accuracy: 0.7714 - val_loss: 1.0852 - val_accuracy: 0.6608\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1822f72c640>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "spa_vocab = target_vectorization.get_vocabulary()\r\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\r\n",
    "max_decoded_sentence_length = 20\r\n",
    "\r\n",
    "\r\n",
    "def decode_sequence(input_sentence):\r\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\r\n",
    "    decoded_sentence = \"[start]\"\r\n",
    "    for i in range(max_decoded_sentence_length):\r\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence\r\n",
    "                                                          ])[:, :-1]\r\n",
    "        predictions = transformer(\r\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\r\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\r\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\r\n",
    "        decoded_sentence += \" \" + sampled_token\r\n",
    "        if sampled_token == \"[end]\":\r\n",
    "            break\r\n",
    "    return decoded_sentence\r\n",
    "\r\n",
    "\r\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\r\n",
    "for _ in range(20):\r\n",
    "    input_sentence = random.choice(test_eng_texts)\r\n",
    "    print(\"-\")\r\n",
    "    print(input_sentence)\r\n",
    "    print(decode_sequence(input_sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-\n",
      "Is that so terrible?\n",
      "[start] es eso tan terrible [end]\n",
      "-\n",
      "Tom and Mary are about the same height.\n",
      "[start] tom y mary están de la mismo [UNK] [end]\n",
      "-\n",
      "Put out the light.\n",
      "[start] [UNK] la luz [end]\n",
      "-\n",
      "I usually take a shower before breakfast.\n",
      "[start] yo yo yo jamás yo no [UNK] [end]\n",
      "-\n",
      "The shop is closed today.\n",
      "[start] la tienda está abierta hoy [end]\n",
      "-\n",
      "I don't like thick soup.\n",
      "[start] no me gusta la sopa [UNK] [end]\n",
      "-\n",
      "How long have you been in Japan?\n",
      "[start] cuánto tiempo estuviste en japón [end]\n",
      "-\n",
      "Tom sang to Mary.\n",
      "[start] tom le compraste a mary [end]\n",
      "-\n",
      "Tom and Mary don't know what's happening.\n",
      "[start] tom y mary no se [UNK] lo que está hablando [end]\n",
      "-\n",
      "She listened to music for hours.\n",
      "[start] ella perdió la música por horas [end]\n",
      "-\n",
      "Tom and Mary drank beer together.\n",
      "[start] tom y mary se [UNK] la cerveza juntos [end]\n",
      "-\n",
      "This bed is too hard to sleep in.\n",
      "[start] esta cama es demasiado difícil para dormir en él [end]\n",
      "-\n",
      "He plays the violin very well.\n",
      "[start] sabe bien la radio demasiado bien [end]\n",
      "-\n",
      "Is there anything you can't do?\n",
      "[start] hay algún que no pueda hacer [end]\n",
      "-\n",
      "Please forgive me.\n",
      "[start] por favor [UNK] [end]\n",
      "-\n",
      "Leave this to me.\n",
      "[start] [UNK] esto por mí [end]\n",
      "-\n",
      "She was brought up by her aunt.\n",
      "[start] ella fue la [UNK] a su tía [end]\n",
      "-\n",
      "Show pity on me.\n",
      "[start] [UNK] [end]\n",
      "-\n",
      "It smells delicious.\n",
      "[start] huele nuevos [end]\n",
      "-\n",
      "Are we allowed to take pictures here?\n",
      "[start] nos tenemos que tomar fotos aquí [end]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里就不评论效果了吧,毕竟我又看不懂西班牙语.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这一节真的是😵,希望 12 章好一点.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf')"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}