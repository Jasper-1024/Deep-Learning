{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 11.5 ä»åºåˆ—åˆ°åºåˆ—\r\n",
    "\r\n",
    "åˆ°ç›®å‰ä¸ºæ­¢,æˆ‘ä»¬å·²ç»æ¥è§¦è¿‡äº†å¤§å¤šæ•° nlp ä»»åŠ¡æ‰€éœ€è¦çš„å·¥å…·,ç„¶è€Œæˆ‘ä»¬åªåœ¨å¤„ç†è¿‡ä¸€ç±» nlp ä»»åŠ¡--æ–‡æœ¬åˆ†ç±»,è¿™ä¸€èŠ‚æˆ‘ä»¬å°†æ¥è§¦å¦ä¸€ç§åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹.\r\n",
    "\r\n",
    "åºåˆ—åˆ°åºåˆ—æ¨¡å‹æ˜¯å°†ä¸€ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥(å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­æˆ–ä¸€æ®µæ–‡å­—)å¹¶å°†å…¶ç¿»è¯‘æˆä¸åŒçš„åºåˆ—,è¿™æ˜¯è®¸å¤š nlp çš„æ ¸å¿ƒä»»åŠ¡.\r\n",
    "\r\n",
    "- æœºå™¨ç¿»è¯‘: å°†æºæ ·æœ¬çš„æ®µè½è½¬æ¢ä¸ºç›®æ ‡è¯­è¨€çš„å¯¹åº”æ®µè½.\r\n",
    "- æ–‡æœ¬æ±‡æ€»: å°†é•¿æ–‡æœ¬è¿›è¡Œæ€»ç»“,åªä¿ç•™æœ€é‡è¦ä¿¡æ¯.\r\n",
    "- é—®é¢˜é—®ç­”: è¾“å…¥é—®é¢˜,è¿”å›ç­”æ¡ˆ\r\n",
    "- èŠå¤©æœºå™¨äºº: è¾“å…¥å¯¹è¯/å†å²èŠå¤©è®°å½•ç­‰ç­‰è½¬æ¢ä¸ºä¸‹ä¸€ä¸ªå›å¤\r\n",
    "- æ–‡æœ¬ç”Ÿæˆ: å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºä¸€ä¸ªå®Œæ•´çš„æ®µè½.\r\n",
    "- ç­‰ç­‰...\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![seq2seq](seq2seq.png)\r\n",
    "\r\n",
    "ä¸Šå›¾æ˜¾ç¤ºäº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹å·¥ä½œçš„ä¸€èˆ¬æµç¨‹\r\n",
    "\r\n",
    "è®­ç»ƒæœŸ\r\n",
    "\r\n",
    "- ä¸€ä¸ªç¼–ç å™¨å°†æºåºåˆ—è½¬æ¢ä¸ºä¸€ç§ä¸­é—´è¡¨ç¤º.\r\n",
    "- é€šè¿‡æŸ¥çœ‹ä¹‹å‰çš„æ ‡è®°å’Œæºåºåˆ—,è§£ç å™¨è¢«ç”¨æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°.\r\n",
    "\r\n",
    "é¢„æµ‹,é¢„æµ‹æ—¶æˆ‘ä»¬æ— æ³•æ¥è§¦åˆ°ç›®æ ‡åºåˆ—,æˆ‘ä»¬ä¸å¾—ä¸ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªæ ‡è®°.\r\n",
    "\r\n",
    "- ç¼–ç å™¨å°†æºåºåˆ—ç¼–ç è½¬æ¢\r\n",
    "- è§£ç å™¨æŸ¥çœ‹æºåºåˆ—å’Œåˆå§‹'ç§å­'æ ‡è®°(å›¾ä¸­æ˜¯ '[start]'),å¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªçœŸå®æ ‡è®°.\r\n",
    "- å°†çœŸå®æ ‡è®°é€å…¥è§£ç å™¨æ¨åŠ¨ä¸‹ä¸€æ¬¡é¢„æµ‹,ç›´åˆ°ç”Ÿæˆä¸€ä¸ªåœæ­¢æ ‡è®°(å›¾ä¸­æ˜¯ '[end]').\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ä¸€ä¸ªæœºå™¨ç¿»è¯‘ä¾‹å­\r\n",
    "\r\n",
    "æˆ‘ä»¬å°†åœ¨ä¸€ä¸ªæœºå™¨ç¿»è¯‘çš„é—®é¢˜ä¸Šæ¼”ç¤ºåºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹.æœºå™¨ç¿»è¯‘æ­£æ˜¯ Transformer å¼€å‘çš„ç›®çš„,ä»ä¸€ä¸ªé€’å½’çš„åºåˆ—æ¨¡å‹å¼€å§‹,æˆ‘ä»¬æœ€ç»ˆä¼šç”¨åˆ°å®Œæ•´çš„ Transformer æ¨¡å‹.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# !wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\r\n",
    "# !unzip -q spa-eng.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è‹±è¯­åˆ°è¥¿ç­ç‰™è¯­çš„æ ·æœ¬æ•°æ®.\r\n",
    "\r\n",
    "- æ–‡æœ¬çš„æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªä¾‹å­\r\n",
    "- è‹±è¯­ åˆ¶è¡¨ç¬¦ è¥¿ç­ç‰™è¯­\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "text_file = \"spa-eng/spa.txt\"\r\n",
    "with open(text_file, encoding='UTF-8') as f:\r\n",
    "    lines = f.read().split(\"\\n\")[:-1]\r\n",
    "text_pairs = []\r\n",
    "for line in lines:\r\n",
    "    english, spanish = line.split(\"\\t\")\r\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\r\n",
    "    text_pairs.append((english, spanish))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import random\r\n",
    "print(random.choice(text_pairs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(\"It's wrong to deceive people, but worse to deceive yourself.\", '[start] EstÃ¡ mal engaÃ±ar a la gente, pero es aÃºn peor engaÃ±arte a ti mismo. [end]')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸€ä¸ªæ ·æœ¬ç¤ºä¾‹\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import random\r\n",
    "\r\n",
    "random.shuffle(text_pairs)  #éšæœºæ’åº\r\n",
    "num_val_samples = int(0.15 * len(text_pairs))  #15%çš„éªŒè¯æ•°æ®\r\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\r\n",
    "train_pairs = text_pairs[:num_train_samples]  #70%çš„è®­ç»ƒæ•°æ®\r\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples +\r\n",
    "                       num_val_samples]  #15%çš„éªŒè¯æ•°æ®\r\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]  #15%çš„æµ‹è¯•æ•°æ®\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "å°†æ•´ä¸ªæ ·æœ¬åˆ†å‰²: 70:15:15 è®­ç»ƒ:éªŒè¯:æµ‹è¯•\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "import tensorflow as tf\r\n",
    "import string\r\n",
    "import re\r\n",
    "\r\n",
    "strip_chars = string.punctuation + \"Â¿\"  # strip_chars = string.punctuation(æ‰€æœ‰æ ‡ç‚¹ç¬¦å·) + Â¿\r\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")  #ä¸æ›¿æ¢ [\r\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")  #ä¸æ›¿æ¢ ]\r\n",
    "\r\n",
    "\r\n",
    "def custom_standardization(input_string):  #å…¨éƒ¨å°å†™,åˆ é™¤é™¤ '[' ']' ä»¥å¤–çš„å…¨éƒ¨æ ‡ç‚¹\r\n",
    "    lowercase = tf.strings.lower(input_string)  #å°å†™\r\n",
    "    #                 æŒ‰ç…§æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢          æ ¼å¼åŒ–å­—ç¬¦ä¸² re.escape è‡ªåŠ¨æ·»åŠ è½¬ä¹‰\r\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\",\r\n",
    "                                    \"\")\r\n",
    "\r\n",
    "\r\n",
    "vocab_size = 15000  #è®¾ç½®æœ€å¤§è¯æ±‡é‡\r\n",
    "sequence_length = 20  #è®¾ç½®æœ€å¤§å¥å­é•¿åº¦\r\n",
    "\r\n",
    "source_vectorization = TextVectorization(  #è‹±è¯­\r\n",
    "    max_tokens=vocab_size,  #æœ€å¤§çš„è¯æ±‡é‡\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length,\r\n",
    ")\r\n",
    "target_vectorization = TextVectorization(  #è¥¿ç­ç‰™è¯­\r\n",
    "    max_tokens=vocab_size,  #æœ€å¤§çš„è¯æ±‡é‡\r\n",
    "    output_mode=\"int\",\r\n",
    "    output_sequence_length=sequence_length +\r\n",
    "    1,  #ç”Ÿæˆå…·æœ‰é¢å¤–æ ‡è®°çš„è¥¿ç­ç‰™è¯­å¥å­,æˆ‘ä»¬éœ€è¦åœ¨è®­ç»ƒæ—¶å°†å¥å­åè½¬ä¸€æ­¥(è¿›è¡Œé¢„æµ‹æ—¶ [start] å’Œ [end] çš„ç¼˜æ•…)\r\n",
    "    standardize=custom_standardization,\r\n",
    ")\r\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\r\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\r\n",
    "source_vectorization.adapt(train_english_texts)  #å­¦ä¹ \r\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "å› ä¸ºè¯­è¨€å·®åˆ«,è¥¿ç­ç‰™è¯­å’Œè‹±è¯­éœ€è¦äº’ç›¸ç‹¬ç«‹å¤„ç†.è¥¿ç­ç‰™è¯­å¤„ç†è¦ä¼ å…¥è‡ªå®šä¹‰çš„å­—ç¬¦ä¸²æ ¼å¼åŒ–å‡½æ•°.\r\n",
    "\r\n",
    "- ä¿ç•™ '[' å’Œ ']',é»˜è®¤æƒ…å†µä¸‹ '[' å’Œ ']' éƒ½ä¼šè¢«åˆ é™¤,ä½†æ˜¯ä¸ºäº†åŒºåˆ† `start` å’Œ `[start]` æˆ‘ä»¬éœ€è¦ä¿ç•™ '[' å’Œ ']'.(è¿™é‡Œæ˜¯ä»…è¥¿ç­ç‰™è¯­ä¸­æœ‰)\r\n",
    "- ä¸åŒè¯­è¨€çš„æ ‡ç‚¹ç¬¦å·æœ‰å·®åˆ«,å¦‚æœè‹±è¯­ä¸­æˆ‘ä»¬é€‰æ‹©å»æ‰æ ‡ç‚¹ç¬¦å·,é‚£ä¹ˆå¯¹åº”çš„è¥¿ç­ç‰™è¯­ä¸­çš„ `Â¿` ä¹Ÿéœ€è¦åˆ é™¤.\r\n",
    "\r\n",
    "**è­¦å‘Š**: å¯¹äºåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹,ä¸€èˆ¬åº”è¯¥å°†æ ‡ç‚¹ç¬¦å·æ˜ å°„ä¸ºç‹¬ç«‹çš„æ ‡è®°,è€Œä¸æ˜¯ç›´æ¥åˆ é™¤.å¦‚æœç›´æ¥åˆ é™¤,é‚£è¾“å‡ºè¯‘æ–‡ä¸ä¼šæœ‰æ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·.åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ä»…ä»…æ˜¯ä¸ºäº†ç®€åŒ–ä»£ç è€Œåˆ é™¤äº†æ ‡ç‚¹ç¬¦å·.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "batch_size = 64  #æ‰¹æ¬¡å¤§å°\r\n",
    "\r\n",
    "\r\n",
    "def format_dataset(eng, spa):\r\n",
    "    eng = source_vectorization(eng)  #è‹±æ–‡ è½¬æ¢ä¸ºå‘é‡\r\n",
    "    spa = target_vectorization(spa)  #è¥¿ç­ç‰™æ–‡ è½¬æ¢ä¸ºå‘é‡\r\n",
    "    return (\r\n",
    "        {\r\n",
    "            \"english\": eng,\r\n",
    "            \"spanish\": spa[:, :-1],  #è¾“å…¥è¥¿ç­ç‰™è¯­å¥å­ ä¸åŒ…æ‹¬æœ€å¥½ä¸€ä¸ªå­—ç¬¦,ä¿è¯è¾“å…¥ä¸ç›®æ ‡ç›¸åŒçš„é•¿åº¦\r\n",
    "        },\r\n",
    "        spa[:, 1:])  #ç›®æ ‡è¥¿ç­ç‰™è¯­å¥å­ æå‰ä¸€æ­¥, è¾“å…¥/è¾“å‡ºä¾ç„¶æ˜¯ç›¸åŒçš„é•¿åº¦.\r\n",
    "\r\n",
    "\r\n",
    "def make_dataset(pairs):  #æ„é€ æ•°æ®é›†\r\n",
    "    eng_texts, spa_texts = zip(*pairs)\r\n",
    "    eng_texts = list(eng_texts)\r\n",
    "    spa_texts = list(spa_texts)\r\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\r\n",
    "    dataset = dataset.batch(batch_size)\r\n",
    "    dataset = dataset.map(format_dataset)\r\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\r\n",
    "\r\n",
    "\r\n",
    "train_ds = make_dataset(train_pairs)\r\n",
    "val_ds = make_dataset(val_pairs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "æ•°æ®é›†æ˜¯ä¸€ä¸ªå…ƒç»„ (inputs, target),inputs æ˜¯å­—å…¸,ä¸¤ä¸ª key `encoder_inputs`(è‹±è¯­) å’Œ `decoder_inputs`(è¥¿ç­ç‰™è¯­)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "for inputs, targets in train_ds.take(1):\r\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\r\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\r\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è‡³æ­¤æ•°æ®å‡†å¤‡å®Œæ¯•.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RNN åºåˆ—åˆ°åºåˆ—æ¨¡å‹\r\n",
    "\r\n",
    "RNN åœ¨ 2015~2017 å¹´ä¸»å¯¼äº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„å¼€å‘,ç„¶åå…¶åœ°ä½è¢« Transformer å–ä»£.å½“æ—¶ RNN æ˜¯è®¸å¤šç°å®ä¸–ç•Œæœºå™¨ç¿»è¯‘ç³»ç»Ÿçš„åŸºç¡€,ä¾‹å¦‚ 2017 å¹´æ—¶ google ç¿»è¯‘å°±æ˜¯ç”± 7 ä¸ªå¤§å‹ ltsm å±‚å †å çš„ç½‘ç»œ.ä½†æ˜¯ä»Šå¤©è¿™æ ·çš„æ¨¡å‹ä¾æ—§å€¼å¾—å­¦ä¹ ,å®ƒæä¾›äº†ä¸€ä¸ªç†è§£åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ç®€å•å…¥å£.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import tensorflow.keras as keras\r\n",
    "import tensorflow.keras.layers as layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, ), dtype=\"int64\")\r\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\r\n",
    "x = layers.LSTM(32, return_sequences=True)(x)  #LSTM è¿”å›åºåˆ—\r\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  #è¾“å‡º\r\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "æœ€ç›´è§‚çš„ä½¿ç”¨ rnn çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹å°±æ˜¯æ¯ä¸ªè¿­ä»£å,ä¿æŒ rnn çš„è¾“å‡º.ä½†æ˜¯è¿™æ ·åšæœ‰å‡ ä¸ªé—®é¢˜\r\n",
    "\r\n",
    "- ç›®æ ‡åºåˆ—é•¿åº¦å¿…é¡»å’Œæºåºåˆ—é•¿åº¦ä¸€è‡´,è¿™åœ¨å®é™…æ¡ä»¶ä¸‹éå¸¸å°‘è§,ä½†æ˜¯æˆ‘ä»¬éšæ—¶å¯ä»¥å¯¹åºåˆ—è¿›è¡Œå¡«å……åˆ°ä¸¤è€…é•¿åº¦ä¸€è‡´,å› æ­¤è¿™ä¸æ˜¯å…³é”®é—®é¢˜.\r\n",
    "- ç”±äº rnn çš„ç‰¹ç‚¹,è¿™æ ·çš„æ¨¡å‹é¢„æµ‹ç›®æ ‡åºåˆ— `0~N` åªå…³æ³¨æºåºåˆ— `0~N` çš„æ ‡è®°,åˆ™åœ¨ç¿»è¯‘ä»»åŠ¡ä¸­æ— æ³•é€‚ç”¨.ä¸€ä¸ªä¾‹å­: 'he weather is nice today' ç¿»è¯‘ä¸ºæ³•è¯­ 'Il fait beau aujourdâ€™hui'.æ¨¡å‹éœ€è¦ä»…é€šè¿‡  `The` å°±èƒ½ç¿»è¯‘å‡º `Il` ,é€šè¿‡ `The weather`  ç¿»è¯‘å‡º `Il fait`,è¿™å‡ ä¹æ˜¯ä¸å¯èƒ½å®Œæˆçš„.\r\n",
    "\r\n",
    "å¦‚æœæ˜¯äººç±»è¯‘è€…,åº”è¯¥éƒ½ä¼šåœ¨å†™ä¸‹è¯‘æ–‡å‰é€šè¯»æ•´ä¸ªå¥å­,ç‰¹åˆ«æ˜¯å¤„ç†çš„è¯­è¨€åœ¨è¯­åºä¸Šéå¸¸ä¸åŒæ—¶(ä¾‹å¦‚: è‹±æ–‡å’Œæ—¥æ–‡).è€Œè¿™ä¹Ÿæ˜¯æ ‡å‡†åºåˆ—åˆ°åºåˆ—æ¨¡å‹éœ€è¦åšçš„äº‹æƒ….\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![rnn_seq2seq](rnn_seq2seq.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "å¦‚ä¸Šå›¾æ‰€ç¤º:\r\n",
    "\r\n",
    "- é¦–å…ˆè¦ä½¿ç”¨ç¼–ç å™¨(RNN),å°†è¾“å…¥æ•´ä¸ªæºåºåˆ—è½¬æ¢æˆå•ä¸€å¼ é‡(æˆ–ä¸€ç»„å¼ é‡).\r\n",
    "- ç„¶åå°†è¿™ä¸ªå¼ é‡(æˆ–è€…ä¸€ç»„å¼ é‡)ä¼ é€’ç»™å¦å¤–ä¸€ä¸ªè§£ç å™¨(RNN),ä½œä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€,å®ƒå°†æŸ¥çœ‹ç›®æ ‡åºåˆ—çš„ 0~N,å¹¶å°è¯•é¢„æµ‹ç›®æ ‡åºåˆ—åˆ° N+1.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "embed_dim = 256  #è¯å‘é‡ç»´åº¦\r\n",
    "latent_dim = 1024  #éšè—å±‚ç»´åº¦\r\n",
    "\r\n",
    "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\r\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\r\n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim),\r\n",
    "                                      merge_mode=\"sum\")(x)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è¿™é‡Œç¼–ç å™¨çš„å®ç°: ä½¿ç”¨ gru ä»£æ›¿äº† ltsm å› ä¸º gru æ›´åŠ ç®€å•,åªæœ‰ä¸€ä¸ªçŠ¶æ€å‘é‡.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\r\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)  #è¯åµŒå…¥\r\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\r\n",
    "x = decoder_gru(x, initial_state=encoded_source)  #gru\r\n",
    "x = layers.Dropout(0.5)(x)  #dropout å±‚\r\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(\r\n",
    "    x)  #å¯†é›†å±‚ æ¯ä¸ªè¾“å‡ºæ­¥éª¤å°è¯•è¥¿ç­ç‰™è¯­è¯çš„æ¦‚ç‡åˆ†å¸ƒ\r\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è§£ç å™¨å®ç°: ä¸€ä¸ªç®€å•çš„ gru å±‚,gru å±‚å°†ç¼–ç åçš„è‹±æ–‡åºåˆ—ä½œä¸ºåˆå§‹çŠ¶æ€,åœ¨ gru ä¹‹ååˆæ¥å…¥äº†ä¸€ä¸ªå¯†é›†å±‚,ä¸ºæ¯ä¸ªæ­¥éª¤äº§ç”Ÿä¸€ä¸ªè¥¿ç­ç‰™è¯­è¯æ±‡çš„æ¦‚ç‡åˆ†å¸ƒ.\r\n",
    "\r\n",
    "è®­ç»ƒè¿‡ç¨‹ä¸­,è§£ç å™¨å°†æ•´ä¸ªç›®æ ‡åºåˆ—ä½œä¸ºè¾“å…¥,ä½†æ˜¯ç”±äº rnn æ‰€ä»¥è§£ç å™¨æ˜¯åªçœ‹ 0~N é¢„æµ‹æ ‡è®° N,è¿™ä¸ªè¿‡ç¨‹å’Œå‰æ–‡å¤„ç†æ—¶åºæ•°æ®æ—¶æ„ä¹‰ç›¸åŒ,å³ rnn ä¸­æˆ‘ä»¬åªèƒ½ä½¿ç”¨ 'è¿‡å»' çš„æ•°æ®é¢„æµ‹ 'æœªæ¥',ç»å¯¹ä¸èƒ½æ‰“ç ´è¿™ä¸ªè¿‡ç¨‹,å¦åˆ™æˆ‘ä»¬çš„æ¨¡å‹æ— æ³•å®Œæˆç¿»è¯‘å·¥ä½œ.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "seq2seq_rnn.compile(optimizer=\"rmsprop\",\r\n",
    "                    loss=\"sparse_categorical_crossentropy\",\r\n",
    "                    metrics=[\"accuracy\"])\r\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 131s 94ms/step - loss: 1.6413 - accuracy: 0.4153 - val_loss: 1.3122 - val_accuracy: 0.5051\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 123s 95ms/step - loss: 1.3216 - accuracy: 0.5248 - val_loss: 1.1524 - val_accuracy: 0.5680\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 133s 102ms/step - loss: 1.1800 - accuracy: 0.5739 - val_loss: 1.0718 - val_accuracy: 0.5998\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 132s 101ms/step - loss: 1.0885 - accuracy: 0.6059 - val_loss: 1.0333 - val_accuracy: 0.6182\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 132s 101ms/step - loss: 1.0430 - accuracy: 0.6299 - val_loss: 1.0207 - val_accuracy: 0.6287\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 1.0130 - accuracy: 0.6481 - val_loss: 1.0158 - val_accuracy: 0.6344\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9936 - accuracy: 0.6618 - val_loss: 1.0156 - val_accuracy: 0.6394\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9800 - accuracy: 0.6717 - val_loss: 1.0182 - val_accuracy: 0.6401\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 130s 100ms/step - loss: 0.9699 - accuracy: 0.6798 - val_loss: 1.0192 - val_accuracy: 0.6418\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 128s 98ms/step - loss: 0.9619 - accuracy: 0.6857 - val_loss: 1.0241 - val_accuracy: 0.6424\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 131s 100ms/step - loss: 0.9565 - accuracy: 0.6903 - val_loss: 1.0264 - val_accuracy: 0.6429\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 120s 92ms/step - loss: 0.9521 - accuracy: 0.6937 - val_loss: 1.0286 - val_accuracy: 0.6435\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.9501 - accuracy: 0.6955 - val_loss: 1.0291 - val_accuracy: 0.6449\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 129s 99ms/step - loss: 0.9485 - accuracy: 0.6975 - val_loss: 1.0333 - val_accuracy: 0.6434\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 122s 94ms/step - loss: 0.9491 - accuracy: 0.6972 - val_loss: 1.0339 - val_accuracy: 0.6442\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f3b937a310>"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è¿™é‡Œçœ‹å‡†ç¡®ç‡æ˜¯ 64.4%: 64% çš„æ—¶é—´å†…æ­£ç¡®é¢„æµ‹äº†è¥¿ç­ç‰™è¯­çš„ä¸‹ä¸€ä¸ªå•è¯...\r\n",
    "\r\n",
    "ä½†æ˜¯è¯„ä»·æœºå™¨ç¿»è¯‘,ä½†æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ­£ç¡®ç‡è¿™ä¸ªæ ‡å‡†å¹¶ä¸é€‚åˆ.ç‰¹åˆ«æ˜¯è¿™ä¸ªæ ‡å‡†çš„ä¸€ä¸ªå‡è®¾å‰æ: é¢„æµ‹å‰ N+1 ä¸ªæ ‡è®°æ—¶,å·²ç»çŸ¥é“ 0~N çš„æ­£ç¡®ç›®æ ‡æ ‡è®°äº†.å®é™…ä¸Šåœ¨é¢„æµ‹è¿‡ç¨‹ä¸­,æˆ‘ä»¬æ˜¯ä»å¤´ç”Ÿæˆç›®æ ‡åºåˆ—,ä¸èƒ½ä¾èµ–ä»¥å‰ç”Ÿæˆæ ‡è®°æ˜¯ 100% å‡†ç¡®çš„.\r\n",
    "\r\n",
    "åœ¨ç°å®ä¸–ç•Œä¸­è€ƒå¯Ÿæœºå™¨ç¿»è¯‘,ä¸€èˆ¬ä¼šä½¿ç”¨ BLEU åˆ†æ•°,è¿™ä¸ªæ˜¯è€ƒå¯Ÿæ•´ä¸ªç”Ÿæˆåºåˆ—çš„æŒ‡æ ‡,ä¼¼ä¹ä¸äººç±»å¯¹ç¿»è¯‘è´¨é‡çš„æ„ŸçŸ¥æ­£ç›¸å…³.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "spa_vocab = target_vectorization.get_vocabulary()\r\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\r\n",
    "max_decoded_sentence_length = 20\r\n",
    "\r\n",
    "\r\n",
    "def decode_sequence(input_sentence):\r\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\r\n",
    "    decoded_sentence = \"[start]\"\r\n",
    "    for i in range(max_decoded_sentence_length):\r\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\r\n",
    "        next_token_predictions = seq2seq_rnn.predict(\r\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\r\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\r\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\r\n",
    "        decoded_sentence += \" \" + sampled_token\r\n",
    "        if sampled_token == \"[end]\":\r\n",
    "            break\r\n",
    "    return decoded_sentence\r\n",
    "\r\n",
    "\r\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\r\n",
    "for _ in range(20):  #éšæœºå–äº† 20ç»„\r\n",
    "    input_sentence = random.choice(test_eng_texts)\r\n",
    "    print(\"-\")\r\n",
    "    print(input_sentence)\r\n",
    "    print(decode_sequence(input_sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-\n",
      "The enemy dropped bombs on the factory.\n",
      "[start] el profesor se dijo en la guerra [end]\n",
      "-\n",
      "She had white shoes on.\n",
      "[start] ella tenÃ­a los zapatos [UNK] [end]\n",
      "-\n",
      "The letter does not say what time she will come up to Tokyo.\n",
      "[start] la carta no se va a decir que ella habÃ­a ido a tokio [end]\n",
      "-\n",
      "We are having a mild winter.\n",
      "[start] estamos en un aÃ±o [end]\n",
      "-\n",
      "What are your responsibilities?\n",
      "[start] cuÃ¡l son tus cosas [end]\n",
      "-\n",
      "Tom fed the goats.\n",
      "[start] tom de [UNK] a las [UNK] [end]\n",
      "-\n",
      "Tom has a pain in his big toe on his right foot.\n",
      "[start] tom tiene un gran en su [UNK] en su gran estoy de la un todo el trabajo [end]\n",
      "-\n",
      "New Zealand is pretty incredible.\n",
      "[start] [UNK] nueva es [UNK] [end]\n",
      "-\n",
      "I found a place to live.\n",
      "[start] encontrÃ© un lugar para vivir [end]\n",
      "-\n",
      "They all left.\n",
      "[start] todos se [UNK] [end]\n",
      "-\n",
      "Japan is not as large as Canada.\n",
      "[start] japÃ³n no es tan grande como un aquÃ­ [end]\n",
      "-\n",
      "Tom isn't sure.\n",
      "[start] tom no estÃ¡ seguro [end]\n",
      "-\n",
      "Tom is a thirteen-year-old boy.\n",
      "[start] tom es un hombre de por quÃ© edad [end]\n",
      "-\n",
      "Tom is more talented than I am.\n",
      "[start] tom es mÃ¡s que yo [end]\n",
      "-\n",
      "He kept on telling lies.\n",
      "[start] Ã‰l se [UNK] a hacer nada [end]\n",
      "-\n",
      "I've been preparing.\n",
      "[start] he estado [end]\n",
      "-\n",
      "I'm opposed to any type of war.\n",
      "[start] me [UNK] en una guerra de guerra [end]\n",
      "-\n",
      "This door will not open.\n",
      "[start] esta puerta no [end]\n",
      "-\n",
      "We have plenty of time.\n",
      "[start] tenemos un montÃ³n de tiempo [end]\n",
      "-\n",
      "The one and only dessert my son eats is chocolate cake.\n",
      "[start] el Ãºnico que mi hijo es el que mi hijo y yo es un poco de dinero [end]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è¿™é‡Œç›´æ¥å–äº† 20 ç»„ç»“æœ,çœ‹ä¸æ‡‚è¥¿ç­ç‰™æ–‡,æ‰€ä»¥æ— æ³•äººè‚‰è¯„ä»·.ä¹¦ä¸Šä½œè€…çš„è¯„ä»·æ˜¯ä½œä¸ºä¸€ä¸ªç©å…·è€Œè¨€,æ•ˆæœè¿˜å¯ä»¥,å°½ç®¡è¿˜æœ‰å¾ˆå¤šé”™è¯¯.\r\n",
    "\r\n",
    "æ³¨æ„: è¿™é‡Œçš„ä»£ç éå¸¸ç®€å•,ä½†æ˜¯æ•ˆç‡å´å¾ˆä½,æˆ‘ä»¬æ¯æ¬¡é‡‡æ ·ä¸€ä¸ªæ–°è¯å°±ä¼šé‡æ–°å¤„ç†æ•´ä¸ªæºåºåˆ—å’Œæ•´ä¸ªç”Ÿæˆçš„ç›®æ ‡åºåˆ—,å®é™…åº”ç”¨ä¸­ä¼šå°†ç¼–ç å™¨å’Œè§£ç å™¨æ‹†å¼€æˆä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹,è§£ç å™¨æ¯æ¬¡æ ‡è®°é‡‡æ ·è¿­ä»£æ—¶åªè¿è¡Œä¸€ä¸ªæ­¥éª¤,é‡ç”¨å…¶ä¸­çš„å†…éƒ¨çŠ¶æ€.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "å…³äºè¿™ä¸ªç©å…·æ¨¡å‹çš„æ”¹è¿›\r\n",
    "\r\n",
    "- ç¼–ç å™¨/è§£ç å™¨ å †å æ›´å¤šçš„ rnn å±‚.\r\n",
    "- ä½¿ç”¨ ltsm è€Œä¸æ˜¯ gru\r\n",
    "- ç­‰ç­‰\r\n",
    "\r\n",
    "ç„¶è€Œ rnn çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹æœ‰ä¸€äº›å¤©ç„¶çš„åŠ£åŠ¿\r\n",
    "\r\n",
    "- æºåºåˆ—å¿…é¡»å®Œå…¨ä¿å­˜åœ¨ç¼–ç å™¨çš„çŠ¶æ€å‘é‡ä¸­,è¿™å¯¹æ¨¡å‹èƒ½å¤Ÿç¿»è¯‘çš„å¥å­å¤§å°å’Œå¤æ‚æ€§æœ‰éå¸¸å¤§çš„é™åˆ¶.è¿™æœ‰ç‚¹åƒäººç±»ç¿»è¯‘å¥å­ä»…å‡­è®°å¿†çš„çŸ¥è¯†,è€Œä¸æ˜¯å¤šçœ‹å‡ éåŸå¥.\r\n",
    "- rnn åœ¨æ¯”è¾ƒéš¾ä»¥å¤„ç†é•¿åºåˆ—.rnn å¸¸å¸¸ä¼šé—å¿˜è¿‡å».å½“ä½ çš„åºåˆ—é•¿åº¦è¶…è¿‡ 100 æ—¶,ç¬¬ 100 ä¸ªæ ‡è®°è¾“å…¥æ¨¡å‹,ç„¶è€Œæ­¤æ—¶æ¨¡å‹å‡ ä¹æ²¡æœ‰åºåˆ—ä¸€å¼€å§‹è¾“å…¥çš„ä¿¡æ¯äº†.è¿™æ„å‘³ç€ rnn æ¨¡å‹æ— æ³•é•¿æ—¶é—´ä¿æŒä¸Šä¸‹æ–‡,è¿™ä¸ªç¼ºç‚¹ç¿»è¯‘é•¿ç¯‡æ–‡ç« æ˜¯è‡´å‘½çš„.\r\n",
    "\r\n",
    "ä¸Šé¢çš„é™åˆ¶æ˜¯ 2017 å¹´ä»¥åä¸šå†…ä» rnn å¤§é‡è½¬å‘ Transformer çš„åŸå› .\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer åºåˆ—åˆ°åºåˆ—æ¨¡å‹\r\n",
    "\r\n",
    "åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹æ˜¯ Transformer çœŸæ­£èƒ½å‘æŒ¥ä½œç”¨çš„åœ°æ–¹,Neural attention ä½¿å¾— Transformer æ¨¡å‹èƒ½æ¯” rnn æ›´æˆåŠŸå¤„ç†æ›´å¤æ‚/æ›´é•¿åºåˆ—.\r\n",
    "\r\n",
    "äººè‚‰å°†è‹±è¯­ç¿»è¯‘ä¸ºè¥¿ç­ç‰™è¯­æ—¶,æˆ‘ä»¬ä¸å¯èƒ½ä¸€ä¸ªè¯ä¸€ä¸ªè¯çš„è¯»è‹±æ–‡è®°ä½,å†ä¸€ä¸ªè¯ä¸€ä¸ªè¯çš„ç¿»è¯‘æˆè¥¿ç­ç‰™è¯­,è¿™æ ·ç¿»è¯‘æ–¹å¼å¯¹ä¸€å¥è¯5ä¸ªè¯,è¿™æ ·çš„æ–‡æœ¬è¿˜è¡Œ,å¯¹ä»˜ä¸€å¤§æ®µæ–‡å­—å°±æ— èƒ½ä¸ºåŠ›äº†.å®é™…ä¸Šäººè‚‰ç¿»è¯‘æ—¶(æ¯”å¦‚ç°åœ¨,æ‰“ä¸‹è¯‘æ–‡),æˆ‘ä»¬ä¼šåœ¨è¯‘æ–‡å’ŒåŸå¥ä¹‹é—´åå¤æ¨ªè·³,å½“å†™ä¸‹è¯‘æ–‡æ—¶ä¼šå‚è€ƒåŸå¥/åŸå¥ä¸Šä¸‹å¾ˆå¤šä¸åŒçš„è¯è¯­æ„æˆçš„è¯­å¢ƒ.\r\n",
    "\r\n",
    "ä¸Šé¢é‚£æ ·äººè‚‰ç¿»è¯‘çš„æ¨¡å¼,æ­£æ˜¯å¯ä»¥é€šè¿‡ neural attention å’Œ Transformer å®ç°çš„.ä¸Šä¸€èŠ‚æˆ‘ä»¬å·²ç»ç†Ÿæ‚‰äº† Transformer çš„ç¼–ç å™¨,ç¼–ç å™¨ä½¿ç”¨ elf-attention å®ç°å¯¹è¾“å…¥åºåˆ—ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¡¨ç¤º.åœ¨ Transformer å®ç°çš„åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹ä¸­,ç¼–ç å™¨è‡ªç„¶è¿˜æ˜¯ç¼–ç å™¨çš„è§’è‰²,è¯»å–æºåºåˆ—å¹¶è¾“å‡ºæºåºåˆ—çš„ç¼–ç è¡¨ç¤º.ä¸ rnn æ¨¡å‹ä¸åŒçš„æ˜¯,Transformer ç¼–ç å™¨çš„è¾“å‡ºä¾æ—§æ˜¯åºåˆ—(ä¸€ä¸²ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åµŒå…¥å‘é‡)\r\n",
    "\r\n",
    "Transformer åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ç¬¬äºŒä¸ªéƒ¨åˆ†æ˜¯ Transformer è§£ç å™¨,å¦‚åŒ rnn è§£ç å™¨,å®ƒä¼šè¯»å–ç›®æ ‡åºåˆ—çš„ 0~N è¯•å›¾é¢„æµ‹æ ‡è®°çš„ N+1.æœ€é‡è¦çš„æ˜¯åœ¨é¢„æµ‹æ—¶å€™,è§£ç å™¨ä¼šé€šè¿‡ neural attention è¯†åˆ«æºåºåˆ—ä¸­å“ªäº›æ ‡è®°ä¸ç›®å‰è¯•å›¾é¢„æµ‹æ ‡è®°æœ€å¯†åˆ‡(æˆ–è®¸è¿˜å’Œäººå·¥ç¿»è¯‘ä¸é‚£ä¹ˆä¸€æ ·).å›é¡¾ä¸€ä¸‹ æŸ¥è¯¢-é”®-å€¼æ¨¡å‹: Transformer è§£ç å™¨ä¸­,ç›®æ ‡åºåˆ—ä½œä¸ºäº† æŸ¥è¯¢ç”¨æ¥å…³æ³¨æºåºåˆ—çš„ä¸åŒéƒ¨åˆ†(æºåºåˆ—æ‰¿æ‹…äº† é”®\r\n",
    " å’Œ å€¼).\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![transformer](transformer.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸Šå›¾å®Œæ•´æ˜¾ç¤ºäº† Transformer åºåˆ—åˆ°åºåˆ—æ¨¡å‹,ä»”ç»†çœ‹ä¸€ä¸‹è§£ç å™¨å†…éƒ¨,ä¼¼ä¹ä¸ç¼–ç å™¨éå¸¸ç›¸ä¼¼,é™¤äº†æ’å…¥äº†é¢å¤–çš„ self-attention å±‚.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformer è§£ç å™¨\r\n",
    "\r\n",
    "ä¸ä¹‹å‰ä¸€æ ·,æˆ‘ä»¬è¦å®ç° Transformer è§£ç å™¨,\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class TransformerDecoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.dense_dim = dense_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads,\r\n",
    "                                                     key_dim=embed_dim)\r\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads,\r\n",
    "                                                     key_dim=embed_dim)\r\n",
    "        self.dense_proj = keras.Sequential([\r\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\r\n",
    "            layers.Dense(embed_dim),\r\n",
    "        ])\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()\r\n",
    "        self.layernorm_3 = layers.LayerNormalization()\r\n",
    "        self.supports_masking = True  #è¿™ä¸ªå±æ€§ç¡®ä¿äº†è¯¥å±‚å°†è¾“å…¥åºåˆ—çš„ mask ä¼ æ’­åˆ°è¾“å‡ºä¸Š.\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "        return config\r\n",
    "\r\n",
    "    def get_causal_attention_mask(self, inputs):\r\n",
    "        input_shape = tf.shape(inputs)\r\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "        j = tf.range(sequence_length)\r\n",
    "        mask = tf.cast(\r\n",
    "            i >= j,\r\n",
    "            dtype=\"int32\")  #äº§ç”Ÿå½¢çŠ¶ ï¼ˆsequence_length, sequence_lengthï¼‰çš„ 0-1 çŸ©é˜µ\r\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\r\n",
    "        mult = tf.concat(\r\n",
    "            [\r\n",
    "                tf.expand_dims(batch_size, -1),\r\n",
    "                tf.constant([1, 1], dtype=tf.int32)\r\n",
    "            ],\r\n",
    "            axis=0\r\n",
    "        )  #æ²¿ç€æ‰¹æ¬¡è½´å¤åˆ¶å®ƒï¼Œå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶çŸ©é˜µï¼ˆbatch_size, sequence_length, sequence_lengthï¼‰ã€‚\r\n",
    "        return tf.tile(mask, mult)\r\n",
    "\r\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)  #è·å– causal_mask\r\n",
    "        if mask is not None:  #å‡†å¤‡è¾“å…¥æ©ç \r\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\r\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)  #åˆå¹¶æ©ç \r\n",
    "        attention_output_1 = self.attention_1(\r\n",
    "            query=inputs, value=inputs, key=inputs,\r\n",
    "            attention_mask=causal_mask)  # å› æœæ©ç ä¼ é€’ç»™ self-attention\r\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "        attention_output_2 = self.attention_2(\r\n",
    "            query=attention_output_1,\r\n",
    "            value=encoder_outputs,\r\n",
    "            key=encoder_outputs,\r\n",
    "            attention_mask=padding_mask,  #å°†åˆå¹¶åçš„æ©ç ä¼ é€’ç»™ç¬¬äºŒæ³¨æ„åŠ›å±‚ï¼Œè¯¥å±‚å°†æºåºåˆ—ä¸ç›®æ ‡åºåˆ—è”ç³»èµ·æ¥ã€‚\r\n",
    "        )\r\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 +\r\n",
    "                                              attention_output_2)\r\n",
    "        proj_output = self.dense_proj(attention_output_2)\r\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "å€¼å¾—è¯´çš„æ˜¯ `self.supports_masking = True`,keras å¹¶æ²¡æœ‰é»˜è®¤å¯ç”¨ mask æ”¯æŒ,ä½ æ— æ³•å°† mask æ•°ç»„ä¼ é€’ä¼šæ²¡æœ‰å®ç° `compute_mask` æ²¡æœ‰æš´éœ² `supports_masking` å±æ€§çš„å±‚,è¿™ä¼šå¯¼è‡´ä¸€ä¸ªæŠ¥é”™.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```py\r\n",
    "def get_causal_attention_mask(self, inputs):\r\n",
    "    input_shape = tf.shape(inputs)\r\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "    i = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "    j = tf.range(sequence_length)\r\n",
    "    mask = tf.cast(\r\n",
    "        i >= j,\r\n",
    "        dtype=\"int32\")  #äº§ç”Ÿå½¢çŠ¶ ï¼ˆsequence_length, sequence_lengthï¼‰çš„ 0-1 çŸ©é˜µ\r\n",
    "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\r\n",
    "    mult = tf.concat(\r\n",
    "        [tf.expand_dims(batch_size, -1),\r\n",
    "         tf.constant([1, 1], dtype=tf.int32)],\r\n",
    "        axis=0\r\n",
    "    )  #æ²¿ç€æ‰¹æ¬¡è½´å¤åˆ¶å®ƒï¼Œå¾—åˆ°ä¸€ä¸ªå½¢çŠ¶çŸ©é˜µï¼ˆbatch_size, sequence_length, sequence_lengthï¼‰ã€‚\r\n",
    "    return tf.tile(mask, mult)\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ä¸ RNN ä¸åŒ Transformer è§£ç å™¨è§£ç å™¨ä¸€æ¬¡æ€§çœ‹åˆ°å…¨éƒ¨è¾“å…¥.RNN è§£ç å™¨æ¯æ¬¡åªçœ‹ä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥,åªèƒ½è®¿é—®å½“å‰æ­¥éª¤çš„ 0~N é¢„æµ‹å½“å‰æ­¥éª¤è¾“å‡º N (ç›®æ ‡åºåˆ—çš„ N+1).Transformer çš„è§£ç å™¨æ˜¯ä¸åŒºåˆ†å…ˆåçš„,ä¸€æ¬¡æ€§çœ‹åˆ°æ•´ä¸ªç›®æ ‡åºåˆ—,å¦‚æœä¸åŠ é™åˆ¶å…è®¸å®ƒä½¿ç”¨æ•´ä¸ªè¾“å…¥,è§£ç å™¨æœ€ç»ˆä¼šç›´æ¥å°†è¾“å…¥çš„ N + 1 å¤åˆ¶åˆ°è¾“å‡º N ä¸­,è¿™æ ·ä¼šä½¿å¾—æ¨¡å‹å‡†ç¡®åº¦è¾¾åˆ° 100%.ä½†æ˜¯å…è®¸é¢„æµ‹æ—¶,è¿™æ ·çš„æ¨¡å‹æ˜¯å®Œå…¨æ— ç”¨çš„,é¢„æµ‹æ¨ç†æ—¶,æ ¹æœ¬æ²¡æœ‰ N+1 çš„è¾“å…¥.\r\n",
    "\r\n",
    "è§£å†³è¿™ä¸ªé—®é¢˜ä¸€ä¸ªç®€å•çš„æ–¹æ³•: å±è”½æ¨¡å‹å¯¹æ­¤æ¬¡è¿­ä»£åç»­æ•°æ®çš„è®¿é—®,å°†è¾“å…¥é¡ºåºçœ‹ä½œæ˜¯æ—¶åºå,å±è”½ä»»ä½•å¯¹æœªæ¥ä¿¡æ¯çš„è®¿é—®.\r\n",
    "\r\n",
    "è¿™ä¸ªè§£å†³æ–¹æ¡ˆå¯¹åº” `get_causal_attention_mask` æ–¹æ³•.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "```py\r\n",
    "def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "    causal_mask = self.get_causal_attention_mask(inputs)  #è·å– causal_mask\r\n",
    "    if mask is not None:  #å‡†å¤‡è¾“å…¥æ©ç \r\n",
    "        padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\r\n",
    "        padding_mask = tf.minimum(padding_mask, causal_mask)  #åˆå¹¶æ©ç \r\n",
    "    attention_output_1 = self.attention_1(\r\n",
    "        query=inputs, value=inputs, key=inputs,\r\n",
    "        attention_mask=causal_mask)  # å› æœæ©ç ä¼ é€’ç»™ self-attention\r\n",
    "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "    attention_output_2 = self.attention_2(\r\n",
    "        query=attention_output_1,\r\n",
    "        value=encoder_outputs,\r\n",
    "        key=encoder_outputs,\r\n",
    "        attention_mask=padding_mask,  #å°†åˆå¹¶åçš„æ©ç ä¼ é€’ç»™ç¬¬äºŒæ³¨æ„åŠ›å±‚ï¼Œè¯¥å±‚å°†æºåºåˆ—ä¸ç›®æ ‡åºåˆ—è”ç³»èµ·æ¥ã€‚\r\n",
    "    )\r\n",
    "    attention_output_2 = self.layernorm_2(attention_output_1 +\r\n",
    "                                          attention_output_2)\r\n",
    "    proj_output = self.dense_proj(attention_output_2)\r\n",
    "    return self.layernorm_3(attention_output_2 + proj_output)\r\n",
    "```\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ç»„åˆ\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "\r\n",
    "class TransformerEncoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim  #è¾“å…¥çš„ç»´åº¦\r\n",
    "        self.dense_dim = dense_dim  #å¯†é›†å±‚å¤§å°\r\n",
    "        self.num_heads = num_heads  #head çš„æ•°é‡\r\n",
    "        self.attention = layers.MultiHeadAttention(\r\n",
    "            num_heads=num_heads, key_dim=embed_dim)  #MultiHeadAttention å±‚\r\n",
    "        self.dense_proj = keras.Sequential([\r\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\r\n",
    "            layers.Dense(embed_dim),\r\n",
    "        ])  #å¯†é›†å±‚\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()  #å½’ä¸€åŒ–å±‚\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()  #å½’ä¸€åŒ–å±‚\r\n",
    "\r\n",
    "    def call(self, inputs, mask=None):  #å®ç° call æ–¹æ³•\r\n",
    "        if mask is not None:  # å°ç»†èŠ‚: è¯åµŒå…¥å±‚ç”Ÿæˆçš„ mask æ˜¯äºŒç»´çš„,ä½†æ˜¯ attention å±‚çš„æœŸæœ›æ˜¯ 3/4 ç»´,è¿™é‡Œè¦æ‰©å¤§ mask ç»´åº¦\r\n",
    "            mask = mask[:, tf.newaxis, :]\r\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\r\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\r\n",
    "        proj_output = self.dense_proj(proj_input)\r\n",
    "        return self.layernorm_2(proj_input + proj_output)\r\n",
    "\r\n",
    "    def get_config(self):  #åºåˆ—åŒ–,å¯ä»¥é€šè¿‡è¿™ä¸ªæ–¹æ³•ä¿å­˜æ¨¡å‹\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class PositionalEmbedding(layers.Layer):\r\n",
    "    def __init__(self, sequence_length, input_dim, output_dim,\r\n",
    "                 **kwargs):  #ä½ç½®åµŒå…¥ç¼ºç‚¹æ˜¯å¿…é¡»äº‹å…ˆçŸ¥é“åºåˆ—é•¿åº¦\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim,\r\n",
    "                                                 output_dim=output_dim)  #åµŒå…¥å±‚\r\n",
    "        self.position_embeddings = layers.Embedding(\r\n",
    "            input_dim=sequence_length, output_dim=output_dim)  #æ ‡è®°ä½ç½®åµŒå…¥å±‚\r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        length = tf.shape(inputs)[-1]\r\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\r\n",
    "        embedded_tokens = self.token_embeddings(inputs)\r\n",
    "        embedded_positions = self.position_embeddings(positions)\r\n",
    "        return embedded_tokens + embedded_positions  #ä¸¤ä¸ªåµŒå…¥å±‚ç›¸åŠ \r\n",
    "\r\n",
    "    def compute_mask(\r\n",
    "            self,\r\n",
    "            inputs,\r\n",
    "            mask=None):  #ä¸ç›´æ¥ä½¿ç”¨ Embedding ä¸€æ ·,è¿™ä¸ªå±‚åº”è¯¥èƒ½å¤Ÿç”Ÿæˆ mask,è¿™æ ·å°±èƒ½å¿½ç•¥æ‰å¡«å……çš„æ•°æ®,å‡å°‘è¿ç®—é‡.\r\n",
    "        return tf.math.not_equal(inputs, 0)\r\n",
    "\r\n",
    "    def get_config(self):  #ä¿å­˜æ¨¡å‹æ—¶è°ƒç”¨\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"output_dim\": self.output_dim,\r\n",
    "            \"sequence_length\": self.sequence_length,\r\n",
    "            \"input_dim\": self.input_dim,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "embed_dim = 256\r\n",
    "dense_dim = 2048\r\n",
    "num_heads = 8\r\n",
    "\r\n",
    "encoder_inputs = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\r\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)  #ç¼–ç å™¨\r\n",
    "\r\n",
    "decoder_inputs = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\r\n",
    "x = TransformerDecoder(embed_dim, dense_dim,\r\n",
    "                       num_heads)(x, encoder_outputs)  #å¯¹ç›®æ ‡åºåˆ—è¿›è¡Œç¼–ç ,å°†å…¶ä¸æºç»“åˆ\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  #ä¸ºè¾“å‡ºä½ç½®é¢„æµ‹\r\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "transformer.compile(optimizer=\"rmsprop\",\r\n",
    "                    loss=\"sparse_categorical_crossentropy\",\r\n",
    "                    metrics=[\"accuracy\"])\r\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 183s 137ms/step - loss: 1.6965 - accuracy: 0.4098 - val_loss: 1.3724 - val_accuracy: 0.4975\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 165s 127ms/step - loss: 1.3630 - accuracy: 0.5233 - val_loss: 1.1919 - val_accuracy: 0.5589\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 145s 112ms/step - loss: 1.2038 - accuracy: 0.5746 - val_loss: 1.1039 - val_accuracy: 0.5928\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 147s 113ms/step - loss: 1.1094 - accuracy: 0.6091 - val_loss: 1.0418 - val_accuracy: 0.6235\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 164s 126ms/step - loss: 1.0541 - accuracy: 0.6323 - val_loss: 1.0291 - val_accuracy: 0.6290\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 165s 127ms/step - loss: 1.0239 - accuracy: 0.6489 - val_loss: 1.0184 - val_accuracy: 0.6348\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 162s 124ms/step - loss: 0.9988 - accuracy: 0.6630 - val_loss: 1.0128 - val_accuracy: 0.6411\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 151s 116ms/step - loss: 0.9789 - accuracy: 0.6740 - val_loss: 1.0006 - val_accuracy: 0.6481\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 169s 130ms/step - loss: 0.9604 - accuracy: 0.6837 - val_loss: 1.0008 - val_accuracy: 0.6503\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 162s 125ms/step - loss: 0.9451 - accuracy: 0.6920 - val_loss: 1.0041 - val_accuracy: 0.6511\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 144s 110ms/step - loss: 0.9288 - accuracy: 0.6997 - val_loss: 1.0005 - val_accuracy: 0.6549\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 140s 108ms/step - loss: 0.9167 - accuracy: 0.7060 - val_loss: 1.0083 - val_accuracy: 0.6564\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 141s 108ms/step - loss: 0.9039 - accuracy: 0.7123 - val_loss: 0.9963 - val_accuracy: 0.6585\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 162s 125ms/step - loss: 0.8904 - accuracy: 0.7181 - val_loss: 1.0068 - val_accuracy: 0.6595\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 158s 121ms/step - loss: 0.8781 - accuracy: 0.7226 - val_loss: 1.0184 - val_accuracy: 0.6546\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 155s 119ms/step - loss: 0.8687 - accuracy: 0.7273 - val_loss: 1.0150 - val_accuracy: 0.6596\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.8577 - accuracy: 0.7319 - val_loss: 1.0183 - val_accuracy: 0.6595\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 105s 81ms/step - loss: 0.8472 - accuracy: 0.7362 - val_loss: 1.0214 - val_accuracy: 0.6607\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.8356 - accuracy: 0.7401 - val_loss: 1.0271 - val_accuracy: 0.6602\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 111s 85ms/step - loss: 0.8287 - accuracy: 0.7431 - val_loss: 1.0303 - val_accuracy: 0.6617\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.8195 - accuracy: 0.7466 - val_loss: 1.0303 - val_accuracy: 0.6638\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 111s 86ms/step - loss: 0.8103 - accuracy: 0.7503 - val_loss: 1.0302 - val_accuracy: 0.6600\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 113s 87ms/step - loss: 0.8002 - accuracy: 0.7530 - val_loss: 1.0343 - val_accuracy: 0.6636\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 113s 87ms/step - loss: 0.7924 - accuracy: 0.7566 - val_loss: 1.0476 - val_accuracy: 0.6610\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.7869 - accuracy: 0.7592 - val_loss: 1.0503 - val_accuracy: 0.6614\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.7793 - accuracy: 0.7617 - val_loss: 1.0576 - val_accuracy: 0.6614\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 112s 86ms/step - loss: 0.7718 - accuracy: 0.7639 - val_loss: 1.0732 - val_accuracy: 0.6601\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 115s 88ms/step - loss: 0.7639 - accuracy: 0.7671 - val_loss: 1.0640 - val_accuracy: 0.6613\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 115s 89ms/step - loss: 0.7565 - accuracy: 0.7691 - val_loss: 1.0769 - val_accuracy: 0.6611\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 109s 83ms/step - loss: 0.7530 - accuracy: 0.7714 - val_loss: 1.0852 - val_accuracy: 0.6608\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1822f72c640>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "spa_vocab = target_vectorization.get_vocabulary()\r\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\r\n",
    "max_decoded_sentence_length = 20\r\n",
    "\r\n",
    "\r\n",
    "def decode_sequence(input_sentence):\r\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\r\n",
    "    decoded_sentence = \"[start]\"\r\n",
    "    for i in range(max_decoded_sentence_length):\r\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence\r\n",
    "                                                          ])[:, :-1]\r\n",
    "        predictions = transformer(\r\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\r\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\r\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\r\n",
    "        decoded_sentence += \" \" + sampled_token\r\n",
    "        if sampled_token == \"[end]\":\r\n",
    "            break\r\n",
    "    return decoded_sentence\r\n",
    "\r\n",
    "\r\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\r\n",
    "for _ in range(20):\r\n",
    "    input_sentence = random.choice(test_eng_texts)\r\n",
    "    print(\"-\")\r\n",
    "    print(input_sentence)\r\n",
    "    print(decode_sequence(input_sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-\n",
      "Is that so terrible?\n",
      "[start] es eso tan terrible [end]\n",
      "-\n",
      "Tom and Mary are about the same height.\n",
      "[start] tom y mary estÃ¡n de la mismo [UNK] [end]\n",
      "-\n",
      "Put out the light.\n",
      "[start] [UNK] la luz [end]\n",
      "-\n",
      "I usually take a shower before breakfast.\n",
      "[start] yo yo yo jamÃ¡s yo no [UNK] [end]\n",
      "-\n",
      "The shop is closed today.\n",
      "[start] la tienda estÃ¡ abierta hoy [end]\n",
      "-\n",
      "I don't like thick soup.\n",
      "[start] no me gusta la sopa [UNK] [end]\n",
      "-\n",
      "How long have you been in Japan?\n",
      "[start] cuÃ¡nto tiempo estuviste en japÃ³n [end]\n",
      "-\n",
      "Tom sang to Mary.\n",
      "[start] tom le compraste a mary [end]\n",
      "-\n",
      "Tom and Mary don't know what's happening.\n",
      "[start] tom y mary no se [UNK] lo que estÃ¡ hablando [end]\n",
      "-\n",
      "She listened to music for hours.\n",
      "[start] ella perdiÃ³ la mÃºsica por horas [end]\n",
      "-\n",
      "Tom and Mary drank beer together.\n",
      "[start] tom y mary se [UNK] la cerveza juntos [end]\n",
      "-\n",
      "This bed is too hard to sleep in.\n",
      "[start] esta cama es demasiado difÃ­cil para dormir en Ã©l [end]\n",
      "-\n",
      "He plays the violin very well.\n",
      "[start] sabe bien la radio demasiado bien [end]\n",
      "-\n",
      "Is there anything you can't do?\n",
      "[start] hay algÃºn que no pueda hacer [end]\n",
      "-\n",
      "Please forgive me.\n",
      "[start] por favor [UNK] [end]\n",
      "-\n",
      "Leave this to me.\n",
      "[start] [UNK] esto por mÃ­ [end]\n",
      "-\n",
      "She was brought up by her aunt.\n",
      "[start] ella fue la [UNK] a su tÃ­a [end]\n",
      "-\n",
      "Show pity on me.\n",
      "[start] [UNK] [end]\n",
      "-\n",
      "It smells delicious.\n",
      "[start] huele nuevos [end]\n",
      "-\n",
      "Are we allowed to take pictures here?\n",
      "[start] nos tenemos que tomar fotos aquÃ­ [end]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è¿™é‡Œå°±ä¸è¯„è®ºæ•ˆæœäº†å§,æ¯•ç«Ÿæˆ‘åˆçœ‹ä¸æ‡‚è¥¿ç­ç‰™è¯­.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "è¿™ä¸€èŠ‚çœŸçš„æ˜¯ğŸ˜µ,å¸Œæœ› 12 ç« å¥½ä¸€ç‚¹.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf')"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}