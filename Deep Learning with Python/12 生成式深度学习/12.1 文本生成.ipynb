{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 12.1 文本生成\r\n",
    "\r\n",
    "这一节中讨论的是利用 rnn 生成序列数据,以文本生成为例,但是相同的技术可以推广到任何种类的序列数据上.音乐/作画等等.\r\n",
    "\r\n",
    "序列数据不仅限于艺术内容生成,目前已经被成功应用在了语音合成与聊天机器人.google 在 2016 年的智能回复功能中就能实现电子邮件/短信息的快速回复,其背后的技术是同源的.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 简史\r\n",
    "\r\n",
    "2014 年以前很少见到 LTSM 的字样,即使在机器学习领域.使用 rnn 生成序列数据基本上要在 2016 年以后才出现在主流领域.但这些技术本身有相当长的历史,ltsm 在 1997 年为了逐个生成文本而开发.\r\n",
    "\r\n",
    "2002 年在瑞士 Schmidhuber 实验室工作的 Douglas Eck 首次将 ltsm 应用在了音乐生成上,并取得了令人满意的结果.Eck 现在是 Google 大脑的研究人员,2016 年创建了名为 Magenta 的小组,致力于使用深度学习技术创造更加迷人的音乐.有时候,好的想法需要 15 年才能变为实践.\r\n",
    "\r\n",
    "2000 ~ 2010年初,Alex Graves 使用 rnn 在序列生成方面做了开创性工作,特别是 2013 年使用循环混合密度网络用于生成类似人类的手写笔迹,被人看作是这一领域的转折点.在那个时刻,笔迹生成这一个特定的银行业,能做梦的机器这一概念适时引起了我的星期,并在我开发 keras 是提供了重要的灵感.Graves 在 2013 年上传预印本到 arXiv 时在 latex 文件留下了一段类似注释的评论: 序列数据生成是计算机所能做的最接近做梦的事情.几年后我们认为这一切的发展都理所当然,但在当时看到了 Graves 的演示,很难不为其中蕴藏的可能性而震撼.在 2015~2017 年,rnn 被成功应用在了文本和对话生成,音乐合成,语音合成等领域.\r\n",
    "\r\n",
    "然后在 2017~2018 年 Transformer 架构开始替代 rnn,不仅仅是在有监督的 nlp 任务上,在序列生成(特别是语言建模/单词级别文本生成)也开始替换 rnn.最著名的 Transformer 生成式应用可能是 gpt-3,这个模型包含了 1750 亿个参数的文本生成模型.由 openai 在一个令人震惊的数据集上训练,这个数据集囊括了大部分可用的数字书籍 维基百科 和整个互联网的很大一部分抓取内容.gpt-3 在 2020 年成了头条新闻,因为其有能力就任何话题生成一段看起来合理的文本,这种能力引发了短暂的炒作热潮.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "下面是我加的内容;\r\n",
    "\r\n",
    "gpt-3 的论文中有一句话,我们发现了一个 bug 但是因为训练成本,无法解决...有人保守估计 gpt-3 的训练费用在数百万~数千万美金之间,真的是模型每个字节按照美元算..\r\n",
    "\r\n",
    "在没有算法/模型的重大突破前,如何有一个通用性最好的模型呢?--将全部的人类文本资料全部扔到模型训练,并且过拟合,大力出奇迹,力大砖飞... gpt-3 就是目前最接近这样的通用模型.\r\n",
    "\r\n",
    "2021 年 github 推出了 copilot,辅助程序开发,怎么说呢,现有的深度学习终将替换掉一切重复劳动的工作.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 如何生成序列数据?\r\n",
    "\r\n",
    "深度学习产生序列数据的通用方法:\r\n",
    "\r\n",
    "- 训练一个预测模型(通常是 rnn 或者 Transformer)\r\n",
    "- 使用之前的标记作为输入,通过模型得到下一个/下几个标记.\r\n",
    "- 重复上面的步骤直到结束.\r\n",
    "\r\n",
    "例如给定输入 'the cat is on the',模型被用来预测输出下一个标记 'mat'.\r\n",
    "\r\n",
    "标记通常指的是单词或者字符,~~能够根据之前标记输出下一个标记的模型都被称作语言模型.~~(查了下概念,这句话翻译有歧义).\r\n",
    "\r\n",
    "一旦有了这样一个训练有素的语言模型后,我们就能开始取样了(生成新序列):\r\n",
    "\r\n",
    "- 提供初始文本(又被称为条件数据)\r\n",
    "- 要求模型输出下一个标记\r\n",
    "- 将输出的标记+之前的文本再重新输入模型\r\n",
    "\r\n",
    "以上过程可重复 N 多次,这个循环允许你生成任意长度的序列,最终得到的序列看起来就像人类书写的句子.\r\n",
    "\r\n",
    "![text_generation_process](text_generation_process.png)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 采样策略\r\n",
    "\r\n",
    "生成文本时,如何选择下一个符号非常重要,一种简单的方法是贪婪采样: 每次只选择可能性最大的下一个字符.这样做的后果是最终的序列成为了重复可预测字符串,而不是连贯的语言.\r\n",
    "\r\n",
    "一种更加有趣的方式是在采样中引入随机性,即从下一个字符的概率分布中进行采样--这样叫做随机采样.这种情况下,根据模型的结果,如果下一个字符是 e 的可能性是 0.3,那最终选择 e 的可能性就是 30%.贪婪采样也能看作是一种随机采样,只是某个字符概率是 1,其他都是 0.\r\n",
    "\r\n",
    "从模型的 softmax 输出中进行概率采样是一种很巧妙的办法,它甚至可用采样到不常见字符,从而使得生成更加有趣的句子.而且有时又会得到完全不再样本的新单词,表现一定的创造性.但是随机取样的问题是: 随机取样无法控制随机性的大小.\r\n",
    "\r\n",
    "又有另一个问题,为什么要具有随机性?\r\n",
    "\r\n",
    "- 考虑一个极端例子,纯随机采样.选取下一个字符每个字符概率均等.这样得到的序列最大的随机性,换句话说这样的序列有最大的熵.当然完全随机的序列不会有任何有趣的内容.\r\n",
    "- 另一个极端例子,贪婪采样.最终得到的序列不会有任何的随机性,同样不会有任何有趣的内容.对应的熵是最小的.\r\n",
    "- 更小的熵会让序列有可预测的结构(看起来更加真实),而更大的熵会得到根据创造性的序列.\r\n",
    "- 两个例子中间有着相当广阔的空间,我们需要的是这中间的某个点.因为人是最终序列的判定者,人是主观的,有趣的标准因人而异,因此我们无法提前获知最佳熵的位置.\r\n",
    "\r\n",
    "为了控制采样过程中的随机性,我们引入一个叫做 softmax 温度的参数.用来表示采样概率分布的熵,即所选择的下一个字符有多么出人意料或者多么不可预测.给定一个 temperature 值,将按照下面的方法对原始概率分布进行重新加权,计算得到一个新的概率分布.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "def reweight_distribution(original_distribution, temperature=0.5):\r\n",
    "    \"\"\"[summary]\r\n",
    "\r\n",
    "    Args:\r\n",
    "        original_distribution ([type]): [概率分布的一维 numpy 数组概率之和必须为 1]\r\n",
    "        temperature (float, optional): [一个因子,定量描述输出的分布熵]. Defaults to 0.5.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [type]: [description]\r\n",
    "    \"\"\"\r\n",
    "    distribution = np.log(original_distribution) / temperature\r\n",
    "    distribution = np.exp(distribution)\r\n",
    "    return distribution / np.sum(\r\n",
    "        distribution\r\n",
    "    )  #返回原始分布重新加权后结果,重新加权后 distribution 求和可能不再为 1,因此需要除以求和得到新的分布\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![temperature](temperature.png)\r\n",
    "\r\n",
    "更高的温度对应的是熵更大的采样分布,更低的温度对应的是更小的随机性.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 使用 keras 编写文本生成\r\n",
    "\r\n",
    "让我们使用 keras 完成文本生成.为了训练模型,首先需要大量的文本数据,可以是任何足够大的文本文件或者文本数据集(指环王,维基百科等等).\r\n",
    "\r\n",
    "这个例子中我们使用上一章的 imdb 数据集,对应的我们的模型将是专门生成针对这些电影评论的模型,而不是一般的英文语言模型.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 准备数据\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\r\n",
    "# !tar -xf aclImdb_v1.tar.gz"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "最终得到的是 aclImdb 文件夹,包含两个子文件夹,一个负面 一个正面.\r\n",
    "\r\n",
    "每个评论有一个文本文件。我们将调用 text_dataset_from_directory，标签模式=None,创建一个数据集,从这些文件中读取,并得到每个文件的文本内容.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "\r\n",
    "dataset = keras.preprocessing.text_dataset_from_directory(directory=\"aclImdb\",\r\n",
    "                                                          label_mode=None,\r\n",
    "                                                          batch_size=256)\r\n",
    "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \")\r\n",
    "                      )  #剔除 '<br />',<br /> 是 html 的标签,但是文本生成并不关心\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\r\n",
    "\r\n",
    "sequence_length = 100  #每个句子的长度\r\n",
    "vocab_size = 15000  #词典的大小\r\n",
    "# 最终只有 15000 个常见词\r\n",
    "\r\n",
    "text_vectorization = TextVectorization(\r\n",
    "    max_tokens=vocab_size,\r\n",
    "    output_mode=\"int\",  # 单词索引序列 int 类型\r\n",
    "    output_sequence_length=sequence_length,  #长度 100.但因为偏移 1,最终使用的是 99 长度 \r\n",
    ")\r\n",
    "text_vectorization.adapt(dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们使用 TextVectorization 层计算我们需要的词汇,每篇评论只使用 sequence_length 以内的词.TextVectorization 层进行文本矢量化时会将超过 sequence_length 的词截断.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def prepare_lm_dataset(text_batch):\r\n",
    "    vectorized_sequences = text_vectorization(text_batch)  #字符串转成 int 序列\r\n",
    "    x = vectorized_sequences[:, :-1]  #截断序列最后一个 创建输入\r\n",
    "    y = vectorized_sequences[:, 1:]  #序列偏移1 创建目标序列\r\n",
    "    return x, y\r\n",
    "\r\n",
    "\r\n",
    "lm_dataset = dataset.map(prepare_lm_dataset)\r\n",
    "lm_dataset = lm_dataset.prefetch(8)  #更好性能,确保每次内存都有 8 个批次预取.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 基于 Transformer 序列到序列模型\r\n",
    "\r\n",
    "我们在这一个小节将会训练一个模型,提供会模型一个词会返回句子中下一个词的概率分布.模型训练完毕,我们提供初始的文本,不断进行采样,直到得到我们需要的文本规模.\r\n",
    "\r\n",
    "与第 10 章温度预测的工作类似,我们会训练一个模型,将 N 个词作为输入预测出 N+1 个词,然而这个过程无法在文本生成直接套用.\r\n",
    "\r\n",
    "- 这样的模型只能在有 N 个词的时候进行预测,当 N 比较大时,我们被限制在需要提供比较大的初始输入.\r\n",
    "- 这样训练的大部分序列将是重叠的,以 N = 4 时为例,'A complete sentence must have, at minimum, three things: a subject, verb, and an object' 拆分\r\n",
    "  - \"A complete sentence must\"\r\n",
    "  - \"complete sentence must have\"\r\n",
    "  - \"sentence must have at\"\r\n",
    "  - 等等直到 \"verb and an object\".\r\n",
    "  \r\n",
    "  每一个这样的序列视为独立样本,模型将不得不做大量的冗余工作.对源序列的处理阶段将不得不重复编码大量相似但不相同的子序列.\r\n",
    "  - 有一种办法是,在取两个子序列时,一次跨过几个单词,但这样会减少训练的样本量,同时也只是部分解决问题,没有根本解决.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![seq2seq0001](seq2seq0001.png)\r\n",
    "\r\n",
    "另外一种彻底的解决办法:\r\n",
    "\r\n",
    "- N 个词的序列,一次送入模型,模型内部将输入序列偏移 1.\r\n",
    "- 采取一些屏蔽策略,确保每一个 i ($i\\in [0,N]$) 模型都只使用输入序列 $[0,i]$ 范围内的单词预测 i+1 个词.\r\n",
    "\r\n",
    "这样解决了上面的两个问题,同时模型哪怕只给 1 个初始单词,也能给出下一个词的概率分布.\r\n",
    "\r\n",
    "这样的解决方式也能在第 10 章的任务上使用,第 10 章温度预测问题上,我们要给定一个 120h 的温度序列,然后学习生成一个 120h 的温度序列,将偏移设置为未来 24h.这样不但解决了最初的问题,还解决了额外的 199 个相关问题.问题发生了变化:\r\n",
    "\r\n",
    "- 原问题: 给定 120h 的温度数据,预测接下来 24h 温度\r\n",
    "- 变换后问题: 给定少于 120h 的温度数据,预测接下来 24h 问题\r\n",
    "\r\n",
    "同样的训练后,结果可能会类似,但是效果会变差,因为这样的模型除了原问题还关心了额外的 199 个相似但不完全相同的问题,这样会轻微干扰原问题的解决.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "上一章我们已经了解了序列到序列的模型学习\r\n",
    "\r\n",
    "- 源序列送入编码器\r\n",
    "- 编码器将源序列的编码输出 + 目标序列 送入解码器.\r\n",
    "- 解码器试图预测偏移以后 目标序列的下一个标记.\r\n",
    "\r\n",
    "上面的流程在做文本生成时需要调整.\r\n",
    "\r\n",
    "文本生成没有源序列: 我们只是试图预测目标序列的下一个标记,这个过程我们只能使用解码器.由于因果填充,解码器只能看到 0~N 来预测 N+1 个单词.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import tensorflow.keras.layers as layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class PositionalEmbedding(layers.Layer):\r\n",
    "    def __init__(self, sequence_length, input_dim, output_dim,\r\n",
    "                 **kwargs):  #位置嵌入缺点是必须事先知道序列长度\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.token_embeddings = layers.Embedding(input_dim=input_dim,\r\n",
    "                                                 output_dim=output_dim)  #嵌入层\r\n",
    "        self.position_embeddings = layers.Embedding(\r\n",
    "            input_dim=sequence_length, output_dim=output_dim)  #标记位置嵌入层\r\n",
    "        self.sequence_length = sequence_length\r\n",
    "        self.input_dim = input_dim\r\n",
    "        self.output_dim = output_dim\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        length = tf.shape(inputs)[-1]\r\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\r\n",
    "        embedded_tokens = self.token_embeddings(inputs)\r\n",
    "        embedded_positions = self.position_embeddings(positions)\r\n",
    "        return embedded_tokens + embedded_positions  #两个嵌入层相加\r\n",
    "\r\n",
    "    def compute_mask(\r\n",
    "            self,\r\n",
    "            inputs,\r\n",
    "            mask=None):  #与直接使用 Embedding 一样,这个层应该能够生成 mask,这样就能忽略掉填充的数据,减少运算量.\r\n",
    "        return tf.math.not_equal(inputs, 0)\r\n",
    "\r\n",
    "    def get_config(self):  #保存模型时调用\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"output_dim\": self.output_dim,\r\n",
    "            \"sequence_length\": self.sequence_length,\r\n",
    "            \"input_dim\": self.input_dim,\r\n",
    "        })\r\n",
    "        return config"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class TransformerDecoder(layers.Layer):\r\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.embed_dim = embed_dim\r\n",
    "        self.dense_dim = dense_dim\r\n",
    "        self.num_heads = num_heads\r\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads,\r\n",
    "                                                     key_dim=embed_dim)\r\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads,\r\n",
    "                                                     key_dim=embed_dim)\r\n",
    "        self.dense_proj = keras.Sequential([\r\n",
    "            layers.Dense(dense_dim, activation=\"relu\"),\r\n",
    "            layers.Dense(embed_dim),\r\n",
    "        ])\r\n",
    "        self.layernorm_1 = layers.LayerNormalization()\r\n",
    "        self.layernorm_2 = layers.LayerNormalization()\r\n",
    "        self.layernorm_3 = layers.LayerNormalization()\r\n",
    "        self.supports_masking = True  #这个属性确保了该层将输入序列的 mask 传播到输出上.\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        config = super().get_config()\r\n",
    "        config.update({\r\n",
    "            \"embed_dim\": self.embed_dim,\r\n",
    "            \"num_heads\": self.num_heads,\r\n",
    "            \"dense_dim\": self.dense_dim,\r\n",
    "        })\r\n",
    "        return config\r\n",
    "\r\n",
    "    def get_causal_attention_mask(self, inputs):\r\n",
    "        input_shape = tf.shape(inputs)\r\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\r\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\r\n",
    "        j = tf.range(sequence_length)\r\n",
    "        mask = tf.cast(\r\n",
    "            i >= j,\r\n",
    "            dtype=\"int32\")  #产生形状 （sequence_length, sequence_length）的 0-1 矩阵\r\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\r\n",
    "        mult = tf.concat(\r\n",
    "            [\r\n",
    "                tf.expand_dims(batch_size, -1),\r\n",
    "                tf.constant([1, 1], dtype=tf.int32)\r\n",
    "            ],\r\n",
    "            axis=0\r\n",
    "        )  #沿着批次轴复制它，得到一个形状矩阵（batch_size, sequence_length, sequence_length）。\r\n",
    "        return tf.tile(mask, mult)\r\n",
    "\r\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\r\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)  #获取 causal_mask\r\n",
    "        if mask is not None:  #准备输入掩码\r\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\r\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)  #合并掩码\r\n",
    "        attention_output_1 = self.attention_1(\r\n",
    "            query=inputs, value=inputs, key=inputs,\r\n",
    "            attention_mask=causal_mask)  # 因果掩码传递给 self-attention\r\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\r\n",
    "        attention_output_2 = self.attention_2(\r\n",
    "            query=attention_output_1,\r\n",
    "            value=encoder_outputs,\r\n",
    "            key=encoder_outputs,\r\n",
    "            attention_mask=padding_mask,  #将合并后的掩码传递给第二注意力层，该层将源序列与目标序列联系起来。\r\n",
    "        )\r\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 +\r\n",
    "                                              attention_output_2)\r\n",
    "        proj_output = self.dense_proj(attention_output_2)\r\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里要重用 11 章的 PositionalEmbedding 和 TransformerDecoder.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "embed_dim = 256\r\n",
    "latent_dim = 2048\r\n",
    "num_heads = 2\r\n",
    "\r\n",
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\r\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\r\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\r\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(\r\n",
    "    x)  #对可能的词汇进行softmax,对每一个输出序列的步长进行计算.\r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 可变温度采样回调的文本生成\r\n",
    "\r\n",
    "我们在这一节会创建一个带回调函数的文本生成模型.在每个迭代后使用不同的 temperature 生成文本.这能让我们看到随着模型收敛,生成文本是如何演变的.以及不同的温度会如何影响文本生成.\r\n",
    "\r\n",
    "这一节我们使用的初始种子文本将始终是 'this movie'.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "tokens_index = dict(enumerate(\r\n",
    "    text_vectorization.get_vocabulary()))  # 单词索引映射回字符串,文本解码.\r\n",
    "\r\n",
    "\r\n",
    "def sample_next(predictions, temperature=1.0):  #随机采样 变温的\r\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\r\n",
    "    predictions = np.log(predictions) / temperature\r\n",
    "    exp_preds = np.exp(predictions)\r\n",
    "    predictions = exp_preds / np.sum(exp_preds)\r\n",
    "    probas = np.random.multinomial(1, predictions, 1)\r\n",
    "    return np.argmax(probas)\r\n",
    "\r\n",
    "\r\n",
    "class TextGenerator(keras.callbacks.Callback):\r\n",
    "    def __init__(self,\r\n",
    "                 prompt,\r\n",
    "                 generate_length,\r\n",
    "                 model_input_length,\r\n",
    "                 temperatures=(1., ),\r\n",
    "                 print_freq=1):\r\n",
    "        self.prompt = prompt\r\n",
    "        self.generate_length = generate_length\r\n",
    "        self.model_input_length = model_input_length\r\n",
    "        self.temperatures = temperatures\r\n",
    "        self.print_freq = print_freq\r\n",
    "\r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        if (epoch + 1) % self.print_freq != 0:\r\n",
    "            return\r\n",
    "        for temperature in self.temperatures:\r\n",
    "            print(\"== Generating with temperature\", temperature)\r\n",
    "            sentence = self.prompt  #开始句子\r\n",
    "            for i in range(self.generate_length):\r\n",
    "                tokenized_sentence = text_vectorization([sentence])  #当前序列送入模型\r\n",
    "                predictions = self.model(tokenized_sentence)  #获取预测\r\n",
    "                next_token = sample_next(predictions[0, i, :])  #检索预测结果\r\n",
    "                sampled_token = tokens_index[next_token]  #\r\n",
    "                sentence += \" \" + sampled_token  #新词加入序列\r\n",
    "            print(sentence)\r\n",
    "\r\n",
    "\r\n",
    "prompt = \"This movie\"\r\n",
    "text_gen_callback = TextGenerator(prompt,\r\n",
    "                                  generate_length=50,\r\n",
    "                                  model_input_length=sequence_length,\r\n",
    "                                  temperatures=(0.2, 0.5, 0.7, 1.,\r\n",
    "                                                1.5))  #使用不同的温度进行采样\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "391/391 [==============================] - 152s 376ms/step - loss: 5.3558\n",
      "== Generating with temperature 0.2\n",
      "This movie is has horrible at animation least sex but at it the is end just when not one something is i the will twilight enjoy zone and today didnt overall mean for sometimes beauty this is is at the all worst the performance worst not movie even where better there throughout\n",
      "== Generating with temperature 0.5\n",
      "This movie is has a such very an well attempt acted to comedy even or come thrill and its sees just that a shows film what after with another fullest [UNK] [UNK] builds to up understand in how a more breath then of when it its weve norwegian got yorker loose save\n",
      "== Generating with temperature 0.7\n",
      "This movie a was very a idea good of actor the especially cast liked conan it william excellent mccarthy acting this and in it her from party acting nor outside character them too did at a the kills students filmmakers jack just but imagine the this director film to a be guns\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is is going a to very give good me movie interested i better wonder if how hollywood i would happen come in on terms like and this the film film is is here both it i more have what read otherwise only the to muslim it films copied but\n",
      "== Generating with temperature 1.5\n",
      "This movie is is another among of joe the being harbour the best best movies pesky of rated the and original we comedies can this watch films in it the with most some moving two out reasons of why pacing i and need a to friend others able and to actually see\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - 146s 371ms/step - loss: 4.8160\n",
      "== Generating with temperature 0.2\n",
      "This movie is did the a best good of job the making cast of worked an and excellent shot job scenes of with mysterious the circumstances film the and audience the goth actors [UNK] are wenders very because [UNK] who of wear their with ridiculous a act couple in of this like\n",
      "== Generating with temperature 0.5\n",
      "This movie is was zero just redeeming awful bad the bad only [UNK] is the such only the weird filmmakers characters excuse are for then this you was have a an redneck edited and [UNK] bad of guy ta but flick this is was very amazing limited in at a warner film\n",
      "== Generating with temperature 0.7\n",
      "This movie was is a a [UNK] cold to dark get the mad idea scientist away was from to the intolerance walking i away decided with to kungfu open movie monsters the from credits different to now the after [UNK] youll being know 15 that minutes as we it have does to\n",
      "== Generating with temperature 1.0\n",
      "This movie film is never kind been of shown dreadful to a the film film and certainly the it plot is starts [UNK] to with a her new sons wife fit someone to separated be from driving the [UNK] villains everyone have is they [UNK] made at one [UNK] day it or\n",
      "== Generating with temperature 1.5\n",
      "This movie is stands haunting in this a movie musician well named who [UNK] may mostly enjoy be some in of the the apartment scenes for with a a limited rather vessel critical and [UNK] [UNK] of if these not guys nothing [UNK] can nor he the feature 1970s who  has\n",
      "Epoch 3/200\n",
      "391/391 [==============================] - 144s 368ms/step - loss: 4.6028\n",
      "== Generating with temperature 0.2\n",
      "This movie movie screams unacceptable it is tells painfully you slow alan and arkin plain are whom cute you and would sorority think chicks shes are actually cute less to loaded her with beloved good husbands music swimming that pool is dumb very english stupid should and way were to they be\n",
      "== Generating with temperature 0.5\n",
      "This movie movie is points very although well it [UNK] works watchable very argument well however it sidney delves readily into refers the to cake it i no gave heaven it even a a most glimpse important into lasting the attention screen of alongside its drinking multilayered boxes photo a childhood [UNK]\n",
      "== Generating with temperature 0.7\n",
      "This movie documentary stars examines pilot two who circumstances have plays to nikki move charles pull wilde herself brilliantly for in themselves a she movie has but no dont chemistry worry you about john this mitchell soap and opera olivia instead de a niro somewhat and betty the returns woman as after\n",
      "== Generating with temperature 1.0\n",
      "This movie is must without these longer reasons work the against acting gender team filmmaker [UNK] colin better [UNK] than make ever just receive an a [UNK] 910 system  i love [UNK] [UNK] as the first casting somewhat d by capt raymond rothrock but for another last time back from mrs\n",
      "== Generating with temperature 1.5\n",
      "This movie is leaves terribly her unbelievable first better [UNK] than movie that jet ive li ever scenes heard although it there is were no more redeeming excitement qualities almost that no i exception havent always actually had seen no before point reading is them no worth rocky watching ii it portray\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - 147s 374ms/step - loss: 4.4770\n",
      "== Generating with temperature 0.2\n",
      "This movie is was a mind [UNK] that drug is dealers not incest at and all having not sex just in enough fact ones thats theyre it too too cut little with stuff this that movie says was happen great but going what on it end was lets worse change with the\n",
      "== Generating with temperature 0.5\n",
      "This movie [UNK] was the one worst else and i that have is ever painfully [UNK] bad off the the tons title of shots [UNK] the acting violence is was a pretty lot decent of in the too film bad nowhere for monkey people is isnt probably interesting really by good the\n",
      "== Generating with temperature 0.7\n",
      "This movie flick isnt is its a about brilliant a acting hapless she man is who not team even anyone the like sex the thought only the american directors mean couple all notices women the who blatant are purchasing trying by to making do this their this only is work actually not\n",
      "== Generating with temperature 1.0\n",
      "This movie is was very great entertaining the and movie original is outline eighties for the the music audience i thought think they i would was watch wrong as i part felt of that [UNK] the is movie a version mustsee of this anna was fan as at good a but couple\n",
      "== Generating with temperature 1.5\n",
      "This movie is is cute pretty the and worst especially movie given we typical were low a budget couple flick plots but going this on movie the i acting had in potential it having seemed to to be show drawn at with heart each and other others its i quite think watched\n",
      "Epoch 5/200\n",
      "391/391 [==============================] - 146s 373ms/step - loss: 4.3881\n",
      "== Generating with temperature 0.2\n",
      "This movie movie by was [UNK] an [UNK] airplane let form down the astro line boy up being his in punishment destroy park those [UNK] movies machine whats like second that party is massacre more intrigued on gas us some market lame drugs shock they loud blacks and music [UNK] at need\n",
      "== Generating with temperature 0.5\n",
      "This movie was was truly the one first that director david of waited the until actors i play got the to soundtrack be to colours me editing it and also editor give could me have a the break dialogue the and editing the through reason the one acting rooftop was [UNK] poor\n",
      "== Generating with temperature 0.7\n",
      "This movie family came might under pleasantly george surprised romero to ratio catch temple up theme its of easy a addressing massive criticize aggressive having lack deadly of [UNK] [UNK] or this communication has skills become called a deserves murder but according rightfully to his the eyes [UNK] seemed this a 1951\n",
      "== Generating with temperature 1.0\n",
      "This movie is had just an short average attention family set guy through powell his but real in life fact and it his was first never foster present dog in while his his mother comedic isnt training struggling continuity and is people back [UNK] in is blue the toward sparks the have\n",
      "== Generating with temperature 1.5\n",
      "This movie is is supposed not to just be about [UNK] wrestling more acts than like a a monster geek you not need think not intelligent to gun take lines it because and he fails forgot to ryan do followed his by act other as than good this lord this of bad\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - 145s 370ms/step - loss: 4.3203\n",
      "== Generating with temperature 0.2\n",
      "This movie was was so like bad all its of neither the and right the around seventies in or [UNK] the that script truly since was there the was only one good actor thing in about this this film film is is it the about only the reason best you picture must\n",
      "== Generating with temperature 0.5\n",
      "This movie movie is held just true one to of some the awful moments awful where casting you very get wanted a i [UNK] was production a number bit of was my just main looking complaint back about i it thought all it the was book dumb and stupid banal newest engines\n",
      "== Generating with temperature 0.7\n",
      "This movie is is excellently a directed beautiful movie movie made is in [UNK] supporting is cast a great great character [UNK] its are whos the often best rubbish the again supporting and cast he are plays great a as very well human the and other the villains acting are is not\n",
      "== Generating with temperature 1.0\n",
      "This movie film blows is the [UNK] outdated its standards not but particularly it in certainly the one 80s kinds produced of and ray considering [UNK] the [UNK] adorable large flare and in short the it [UNK] is on a the very [UNK] short eyebrows film [UNK] was and apparently very not\n",
      "== Generating with temperature 1.5\n",
      "This movie is was dreadful incredibly by awfully the written usual actors [UNK] jo and zellweger were are probably about the 6 leading leads characters but are if strange an people obnoxious are teenagers struggling that to are go caught zombies on there the was only some one would i expect [UNK]\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - 153s 390ms/step - loss: 4.2658\n",
      "== Generating with temperature 0.2\n",
      "This movie is is one by of far his worst scores movies in date imdb movie everybody imdb speaks comments about on [UNK] this internet thinking [UNK] it heap to of trash on movies broadcast with everything [UNK] its thought greatest seems ever to since entertain its me leading to to a\n",
      "== Generating with temperature 0.5\n",
      "This movie might makes not you win set awards the on famous [UNK] [UNK] special and 1930 gerard by [UNK] the smith mirror has [UNK] hard [UNK] to humor come if across you all can adore read this his one snappy you [UNK] will toward love eager it and isnt enjoy and\n",
      "== Generating with temperature 0.7\n",
      "This movie movie has works one wonders audience how who appropriate is this writers movie director and of screenwriters this the is same a of shame pulp major fiction no but name courage attached in to the this trash movie dennis  hopper up the dude from 20000 60s ill always be\n",
      "== Generating with temperature 1.0\n",
      "This movie movie does is a russel laughable job great the acting special pretty effects cool compete effects for but european its movie its lovers worth western judgement  from think lynchs best with se7en and has brought us a different story for all missions in the 40s well not one way\n",
      "== Generating with temperature 1.5\n",
      "This movie documentary reminded has me quite of a the lot ride it on concerns the opposite monsters as of between [UNK] an [UNK] old countryside boy new named orleans jill innocent [UNK] there of is his terrible boy self [UNK] emerged [UNK] from berlin mortal into kombat prostitution sex is and\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - 143s 365ms/step - loss: 4.2207\n",
      "== Generating with temperature 0.2\n",
      "This movie [UNK] is is plain probably awful the i only thought watched it it and the those whole were time scenes to where other it people all would films i made tell my you life not that but feeling every bad moment in is the so second long half at a\n",
      "== Generating with temperature 0.5\n",
      "This movie underrated is is an a excellent lot movie of as the it 1 is is that very well underrated made i me even laugh though at if the you end dont to want feel to if just the  best thing universal effects you will see this movie that you\n",
      "== Generating with temperature 0.7\n",
      "This movie documentary sucked is so basically that on the our boys most and tire genre for on mistakes it so [UNK] really up explained that the we [UNK] know our the own civilization cross this paths film that is the based lack on of shocking narrative one dull of twist the\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is was awesome one i of thought the this japanese movie made was in good widescreen part in ii my the opinion columbia it color was started completely to out watch [UNK] if instead politics the these story are was you so dont she see made the [UNK] sound\n",
      "== Generating with temperature 1.5\n",
      "This movie is was so different good and i i was must really say draw at my the least end of according the to first imdb of review the today dvd would came have in back and in for the my past library an 2 [UNK] 100 to if see so it\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - 142s 363ms/step - loss: 4.1823\n",
      "== Generating with temperature 0.2\n",
      "This movie is might only really if matter you what want do to i see have is and david how [UNK] to to africa take in pictures common of context them one it place is a no fine mention but among also other very parts popular of former meyer life films walker\n",
      "== Generating with temperature 0.5\n",
      "This movie film is is repulsive about and the compelling movie story experience is its mostly in about regard ringo issues health of and duty fighters to a highlight 20th childhood century in modern x day but but that seeing doesnt above mean measure you in get it a tonight technical this\n",
      "== Generating with temperature 0.7\n",
      "This movie is was boring for and that unrealistic [UNK] theyre bollywood both movies cast taken predictable in plots the and male graphics costumes were were stunning art the are acting all is high really quality journey but mountain it and was the not impact one realistic of was the with main\n",
      "== Generating with temperature 1.0\n",
      "This movie would has be some rated clips searching make i for mean the spirited downer performances was are pretty nicely bad shared for by legendary the fans finale of and the a clips peek of at the things gas that stations addressing might the just audience not laughing knowing through by\n",
      "== Generating with temperature 1.5\n",
      "This movie is brings one back unique nothing films happens not to only impress make me you comparable need or of to loretta know who she is was the one wealthy of successful the women same are age located and in their the acting them [UNK] and would [UNK] turn to into\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - 143s 363ms/step - loss: 4.1497\n",
      "== Generating with temperature 0.2\n",
      "This movie production was was under quick yes the there acting was was pretty a bad realistic script show and near like the dog trailers top i in was the terrible low movie budget business but dc what [UNK] got did me not chuckle finish the the main basic problem premise was\n",
      "== Generating with temperature 0.5\n",
      "This movie film is is by a any movie major that major is flaws somewhere anyone along who your treats average will folks produce who this get film [UNK] while doesnt the really rest matter of will the theyre very good [UNK]  dynamic and grounded in life which includes the assassin\n",
      "== Generating with temperature 0.7\n",
      "This movie was seemed very like it a was bit a like masterpiece i for was this [UNK] movie simply i because was you about i to was dont the really one my person soul i that actually was had especially really in well the and storyline have played by [UNK] jim\n",
      "== Generating with temperature 1.0\n",
      "This movie is deserves about to 2 be ninjas the exploration history of this the movie kind i and enjoyed the it first i they thought usually it take would the be story a in nice the way film themselves made arent you very will easily always you match know the that\n",
      "== Generating with temperature 1.5\n",
      "This movie film is were well the made idea becomes of repetitive an and easy painfully parody boring that action episodes genre all with is obscure the this film brilliant doesnt because even its around [UNK] a plus fan about zombie the hunters wizard i of grew film up and watching took\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - 142s 361ms/step - loss: 4.1213\n",
      "== Generating with temperature 0.2\n",
      "This movie movie was blows better note than to the start hollywood of [UNK] the is better about than him this because might the they original only story saw line this on movie cable characters news mother stories murdered that two may criminals have who up fought before against dog nature [UNK]\n",
      "== Generating with temperature 0.5\n",
      "This movie country was begins [UNK] to with move several jfk different humphrey young bogart men in in to 1930s [UNK] india he to discovers see things his happen dream i woman had who high told hopes him of he going works out there discussions light with didnt their seem fancy to\n",
      "== Generating with temperature 0.7\n",
      "This movie was provided shameless its which entertainment scope fans for would fun recommend presentation catching instead food mostly network ferris watched [UNK] the demonstrates high the quality lives of of a various bunch view of folks celebrity who students go at ahead home and and discover seriously its ones a who\n",
      "== Generating with temperature 1.0\n",
      "This movie film has starts been with visiting a having terrorist only on to the be american a we massive prepare terrorist to following move the to president travel future to gather find vietnam gold carrying home gas to and confront destroy people this most state of of mind how if they\n",
      "== Generating with temperature 1.5\n",
      "This movie is was one made of today early crime australia films i passed understand above the the [UNK] films [UNK] and opens what a self targeted off audience the i teller enjoyed thought this the getting film ready together for because something it wrong was when presented it ending turned correctly\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - 139s 354ms/step - loss: 4.0959\n",
      "== Generating with temperature 0.2\n",
      "This movie is is stylized the and best not but that its was true not without that context great the acting settings in were this good is and the [UNK] movie was is the made r for entertainment it [UNK] was  also very well done without them the music the sound\n",
      "== Generating with temperature 0.5\n",
      "This movie film was might so work well by it everyone being that [UNK] im together supposed my to self de anybody niro with  a positive review but through this movie do not work day is alabama and therefore this group of jerks are thrown into the human race against the\n",
      "== Generating with temperature 0.7\n",
      "This movie was is a one really that awful was script terrible was sets and and human just dialogue made this me movie laugh a out few and days dont one get big this joke movie it is is the funny story you is could bad have dont character [UNK] willis why\n",
      "== Generating with temperature 1.0\n",
      "This movie film was is a excellent predictable piece and of the drivein genre theaters i together have to seen own before local i british was police being detective more in of that a era group that of is young a ladies big charmed party really with really a tom vintage who\n",
      "== Generating with temperature 1.5\n",
      "This movie film was starts awful off then and whenever starts the it film goes was bad done and that doesnt follows make a up relationship piece from and a things revolution get in better the life part and the his main fight problem is with that and as how later he\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - 136s 346ms/step - loss: 4.0732\n",
      "== Generating with temperature 0.2\n",
      "This movie plot and could thriller have be been acted a so bit bad ever its since just most a of movie drinking is with boring no and shock the that soundtrack sealed will the go brits forward can to torture falling a asleep bunch close of up young [UNK] criminals solely\n",
      "== Generating with temperature 0.5\n",
      "This movie film is is profound television it works tries for hard many mannerisms people and say style its or entertaining at but least ultimately there forgettable are for some those bright who wigs think regarding extremely leaving stupid behind [UNK] them the actually people chasing can the heat flag too taking\n",
      "== Generating with temperature 0.7\n",
      "This movie is was one an of abc the classic years exploitation of film murder history by coup disney de has palmas the silent opposite tarzan of of pretty the pictures 21st due century to addition all the of movies hollywoods the kingdom 20th of century course this not movies necessarily would\n",
      "== Generating with temperature 1.0\n",
      "This movie are shows tremendous black talents fun throughout japanese and action on films a are famous taken to from great others britain early it in will a not second until version 1940 the ends late of remaining the action original and and the with only the superior [UNK] [UNK] that portrayal\n",
      "== Generating with temperature 1.5\n",
      "This movie concerns is a a [UNK] good [UNK] sex woman action and always its does very she necessary can to seduce do the what man you its think no of action it to is be not a a clue movie the to movie have one sex for sex ya however yes\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - 144s 368ms/step - loss: 4.0530\n",
      "== Generating with temperature 0.2\n",
      "This movie is is packed shown with on actors [UNK] enjoying and second the it animation is is set beautiful on to the [UNK] beginning story and centers the around characters little with girl dark and is unbelievable not the an story abusive kind [UNK] of on [UNK] strong cant hinted compare\n",
      "== Generating with temperature 0.5\n",
      "This movie was could bad be i a tried lot watching to the find showtime in or the on case cable it television had and including a myself brief laughing glimpses at of what final is scene its in hard the to entire watch cast and except crew for plus the some\n",
      "== Generating with temperature 0.7\n",
      "This movie is is pretty every average bit joe in plays me john off american on psycho roy it [UNK] depicts it how with human each abilities of the us two [UNK] groups for among mainstream the american ones audience i roles wouldnt today know though two i men still are find\n",
      "== Generating with temperature 1.0\n",
      "This movie is is well the done middle ive and read a all few the of other the comments series and [UNK] since are i i have can read say books more a that lot version of left this but one it route does i a had bit it of while all\n",
      "== Generating with temperature 1.5\n",
      "This movie duncan is had really a overdone nice rap little period or free dark but the they evolution tried and to the make [UNK] loser for trying a his movie own [UNK] set a was cgi it [UNK] was  quite deserved if it would have made a half stars could\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - 150s 382ms/step - loss: 4.0349\n",
      "== Generating with temperature 0.2\n",
      "This movie movie has may such be a an gem explosion i to have try never and once do again it i is never a given movie moment a a very chance message writing of any diverse film related ive to also difficulties be that [UNK] america the is polish in and\n",
      "== Generating with temperature 0.5\n",
      "This movie is in great the light message of 2007 a oscars perfect opening if to you come finish to it believe you me are youll in ever for want you to to believe paraphrase a the woody movie allen you is need telling this fine movie costuming is are [UNK] the\n",
      "== Generating with temperature 0.7\n",
      "This movie is has a myers great for performance his but physical the debut only he better throws than in any this dramatic movie project he he has gives and one you of got the to powerful see laughter if especially he for is the even soul stranger hearts  vs himself\n",
      "== Generating with temperature 1.0\n",
      "This movie movie is seemed the miserable reality [UNK] of manners even which though everything its is like obviously watching a an film extremely where mediocre our actors understanding thought are [UNK] images as of students good do art absurd school pointless students drama drinks philosophy heavily it or would isnt not\n",
      "== Generating with temperature 1.5\n",
      "This movie film is is a about short real film family history who and have friends quite violent a other family kung called fu drug entertainment dealer this is is one not of the the best first film the to three [UNK] brothers thats carrot the and man is gets ok to\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - 140s 356ms/step - loss: 4.0185\n",
      "== Generating with temperature 0.2\n",
      "This movie little is dull also but one its of plain the male worst actors john i [UNK] have ever to seen talk him in no a other good way bad except student in was his any acting [UNK] and hurt this the movie guy was from so the bad female he\n",
      "== Generating with temperature 0.5\n",
      "This movie is has [UNK] a too very many easy many levels [UNK] i which have seems come to looking me for out a having lot made of about money ten from his china american in man the he head and back [UNK] on in nypd his blue [UNK] screen and as\n",
      "== Generating with temperature 0.7\n",
      "This movie is is highly [UNK] demonstrated and by divine actor self christopher ensemble [UNK] there looks is like a an [UNK] alternate following universe the in first the and picture [UNK] [UNK] music a in [UNK] a giving stage [UNK] resemblance [UNK] to perfectly the happy novel ending story of arc\n",
      "== Generating with temperature 1.0\n",
      "This movie [UNK] was killer [UNK] [UNK] its did a the rival best team of for jules a and spy [UNK] a almost showcase like for producer him its martin like lawrence zorro and [UNK] jim but martin nothing landau more [UNK] than than bruce the [UNK] credits [UNK] appeared didnt during\n",
      "== Generating with temperature 1.5\n",
      "This movie is is far very more well than than [UNK] what i has have the grown worst up part and of that them movie the industry only throws a us few to [UNK] be finds shown a as ninja being and his is other a only story good to one watch\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - 153s 390ms/step - loss: 4.0032\n",
      "== Generating with temperature 0.2\n",
      "This movie movie is gave great it acting as when usual simon julia cable roberts network starts has seeing a them preview where screen a [UNK] predictable girl plot sees in agency something enters touches the more track of than an that [UNK] seems will to celebrate pitch a anything slice endearing\n",
      "== Generating with temperature 0.5\n",
      "This movie is spawned horrible even movies worse i humor mean jokes its but a this lot movie [UNK] rates tells as the output [UNK] of weak fulci material the from teeth beginning to to the be final subjected chapter to to this this hollywood movie classic makers will wanted also to\n",
      "== Generating with temperature 0.7\n",
      "This movie is blows considered from by the numbers top and of run a appearances masked of unique horror horror [UNK] this this film movie pulls has off all all the the crazy celebrity type cameos of the themselves past talks just and beth into oh herself numerous but [UNK] everyone people\n",
      "== Generating with temperature 1.0\n",
      "This movie is was as the [UNK] movie thinking based that on have plot this or some fall semblance but of it a badly story written to pacing a though revolution not i in must really comprehend think and its the reality worst of kind the should planet be that reduced we\n",
      "== Generating with temperature 1.5\n",
      "This movie movie without in touching [UNK] i of must the say 10 in lines my [UNK] eyes desire out to my express body  truly spoken by the names of national geographic uncovered and no emotion the sad fate so sexually frustrated that no commercial reason are the main reason for\n",
      "Epoch 18/200\n",
      "127/391 [========>.....................] - ETA: 1:47 - loss: 3.9925"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16220/1942794232.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_gen_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "运行时间实在是太长了,不得不中断,这里取的是原文的结果.因为标点符号被剔除了,因此结果不含标点.\r\n",
    "\r\n",
    "With temperature=0.2:\r\n",
    "\r\n",
    "- \"this movie is a [UNK] of the original movie and the first half hour of the movie is pretty good but it is a very good movie it is a good movie for the time period\"\r\n",
    "- \"this movie is a [UNK] of the movie it is a movie that is so bad that it is a [UNK] movie it is a movie that is so bad that it makes you laugh and cry at the same time it is not a movie i dont think ive ever seen\"\r\n",
    "\r\n",
    "With temperature=0.5:\r\n",
    "\r\n",
    "- \"this movie is a [UNK] of the best genre movies of all time and it is not a good movie it is the only good thing about this movie i have seen it for the first time and i still remember it being a [UNK] movie i saw a lot of years\"\r\n",
    "- \"this movie is a waste of time and money i have to say that this movie was a complete waste of time i was surprised to see that the movie was made up of a good movie and the movie was not very good but it was a waste of time and\"\r\n",
    "\r\n",
    "With temperature=0.7:\r\n",
    "\r\n",
    "- \"this movie is fun to watch and it is really funny to watch all the characters are extremely hilarious also the cat is a bit like a [UNK] [UNK] and a hat [UNK] the rules of the movie can be told in another scene saves it from being in the back of\"\r\n",
    "- \"this movie is about [UNK] and a couple of young people up on a small boat in the middle of nowhere one might find themselves being exposed to a [UNK] dentist they are killed by [UNK] i was a huge fan of the book and i havent seen the original so it\"\r\n",
    "\r\n",
    "With temperature=1.0:\r\n",
    "\r\n",
    "- \"this movie was entertaining i felt the plot line was loud and touching but on a whole watch a stark contrast to the artistic of the original we watched the original version of england however whereas arc was a bit of a little too ordinary the [UNK] were the present parent [UNK]\"\r\n",
    "- \"this movie was a masterpiece away from the storyline but this movie was simply exciting and frustrating it really entertains friends like this the actors in this movie try to go straight from the sub thats image and they make it a really good tv show\"\r\n",
    "\r\n",
    "With temperature=1.5:\r\n",
    "\r\n",
    "- \"this movie was possibly the worst film about that 80 women its as weird insightful actors like barker movies but in great buddies yes no decorated shield even [UNK] land dinosaur ralph ian was must make a play happened falls after miscast [UNK] bach not really not wrestlemania seriously sam didnt exist\"\r\n",
    "- \"this movie could be so unbelievably lucas himself bringing our country wildly funny things has is for the garish serious and strong performances colin writing more detailed dominated but before and that images gears burning the plate patriotism we you expected dyan bosses devotion to must do your own duty and another\"\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "temperature 越低,文本越重复无聊.0.2° 时还有很多重复文本.随着 temperature 升高,文本的创造性越来越好,开始变的有趣,当时最终 temperature = 1.5 时,生成序列开始变得不成章法,无法阅读.按照结果来看,一个平衡点是 0.7°."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "要注意的是,通过更多的数据,更长的时间,更大的模型,我们是可以获得更加通畅/有趣/真实的生成文本的.gpt-3 就是一个例子.但是不能指望这样能够产生真正有意义的文本,除了随机的抽取文字和人类对文本的解释,这样的过程实际上是从一个统计模型中抽取数据,语言模型只有形式没有真正的内容.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "自然语言在人类世界中可以是很多东西: 一种沟通的媒介,一种感染世界的行动方式,一种社会的润滑剂,一种存储/表达你自己想法的方式.语言的这些实际用途是语言起源的意义.一个深度学习的 '语言模型',尽管名字带了语言俩字,也仅仅是因为样本来自人类语言.这样的模型无法交流(没有交流的必要,也没有交流的对象),不能对现实世界采取什么行动(没有意图),不能称为一个社会意义上的人,没有任何要表达的思想.语言是心灵的操作系统,要创造真正有意义的语言,需要一个真正意义上的心灵作为输出方.\r\n",
    "\r\n",
    "语言模型所作的捕捉人类活动所产生文本(书籍,电影评论,推特)的模式.这些文本存在统计意义上的结构是人类实现语言的一个副产物.有一个思想实验: 假如人与人之间的沟通语言也像计算机之间通信一样,没有那么多的冗余,再做到更好的压缩.语言原本的目的还是一样实现,但是这样的文本不再有任何有意义统计意义上的结构,自然也无法进行深度学习.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "题外话:\r\n",
    "\r\n",
    "在目前的观点中,似乎都将语言和心灵/智能捆绑在了一起,但是假设一个类似的模型,训练过全部的人类互联网文字,这样的模型非常可能通过在线的图灵测试,那这样的模型算是有智能吗?\r\n",
    "\r\n",
    "类似的话题是中文房间,哲学僵尸等等.现在我们对智能的定义还在初级阶段,随着对大脑和人工智能发展,或许在有生之年能看到一丝曙光.\r\n",
    "\r\n",
    "---\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一点收尾\r\n",
    "\r\n",
    "我们可以训练一个序列到序列的模型,从给定以前的标记,预测下一个标记.\r\n",
    "\r\n",
    "在使用文本数据情况下,这样的模型称为语言模型,单个标记可以是字符或单词.\r\n",
    "\r\n",
    "对下一个标记的采样要在规律性和随机性之间取得平衡.\r\n",
    "\r\n",
    "处理随机性问题一个方法是引入 softmax 温度,始终使用不同的温度尝试,找到一个平衡点.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf': venv)"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}