{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "10.4 RNN 的高级用法\r\n",
    "\r\n",
    "到目前为止,我们已经学过的内容\r\n",
    "\r\n",
    "- 什么是 RNN 及它如何工作\r\n",
    "- 什么是 ltsm 及 ltsm 比简单 rnn 更好\r\n",
    "- 如何使用 keras 的 rnn 层处理时序数据.\r\n",
    "\r\n",
    "接下来我们会学习 keras rnn 的高级特性,这可以帮助我们的模型达到更好的效果.\r\n",
    "\r\n",
    "- 循环 dropout: 在 rnn 层应用 dropout.\r\n",
    "- 堆叠循环层: 提高模型的容量和表现力,代价是更高的计算消耗.\r\n",
    "- 双向 RNN: 将相同的信息正向/反向送入模型,提高精读缓解遗忘问题.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 循环 dropout 缓解过拟合\r\n",
    "\r\n",
    "回顾 10.2 节的 ltsm 模型,尽管小模型可以击败基线,但是非常快的过拟合了,原因可能是模型容量过小.\r\n",
    "\r\n",
    "在更前面的内容中对抗过拟合有一个非常有效的办法 -> dropout,但是 dropout 在 rnn 并不是一件容易的事情..\r\n",
    "\r\n",
    "如果直接应用 dropout,那对 rnn 模型可能是一件坏事而不是好事,droupout 总是会破坏学习过程.直到 2015 年,Yarin Gal 在博士论文确定了 rnn 使用 droupout 的正确方式: 对每个时间步使用相同的 dropout 掩码(dropout mask,相同模式的舍弃单元),而不是让 dropout 掩码随着时间变化而随机变化.更重要的是为了对 GRU ltsm 等循环层做正则化表示,应该将不随时间变化的 dropout 掩码应用在层的内部循环激活上(成为 循环 dropout 掩码).对每个时间步长使用相同的 dropout 掩码,可以让我按钮沿着时间正确传播学习误差,而随着时间变化的 dropout 掩码会破坏这个过程.\r\n",
    "\r\n",
    "Yarin Gal 使用 keras 进行的研究,帮助将这种机制直接内置到了 keras 的 rnn 层中.keras 的 rnn 层都有两个与 dropout 相关的参数:\r\n",
    "\r\n",
    "- `dropout`: 浮点数,指定该层每个输入单元 dropout 的比例.\r\n",
    "- `recurrent_dropout`: 指定循环单元的 dropout 的比例.\r\n",
    "\r\n",
    "我们先第一个 ltsm 的例子中添加 dropout,看看这样对过拟合的影响.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from tensorflow import keras\r\n",
    "import tensorflow.keras.layers as layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sequence_length = 120\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "\r\n",
    "fname = os.path.join(\"jena_climate_2009_2016.csv\")\r\n",
    "\r\n",
    "with open(fname) as f:\r\n",
    "    data = f.read()\r\n",
    "\r\n",
    "lines = data.split(\"\\n\")\r\n",
    "header = lines[0].split(\",\")\r\n",
    "lines = lines[1:]\r\n",
    "\r\n",
    "temperature = np.zeros((len(lines), ))  #全0 数组\r\n",
    "raw_data = np.zeros((len(lines), len(header) - 1))  #全0 数组\r\n",
    "for i, line in enumerate(lines):\r\n",
    "    values = [float(x) for x in line.split(\",\")[1:]]  #每行的第一个值是时间戳，所以从第二个开始\r\n",
    "    temperature[i] = values[1]  #第一个值是温度\r\n",
    "    raw_data[i, :] = values[:]  #每行的数据\r\n",
    "\r\n",
    "num_train_samples = int(0.5 * len(raw_data))\r\n",
    "num_val_samples = int(0.25 * len(raw_data))\r\n",
    "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\r\n",
    "\r\n",
    "\r\n",
    "mean = raw_data[:num_train_samples].mean(axis=0)\r\n",
    "raw_data -= mean\r\n",
    "std = raw_data[:num_train_samples].std(axis=0)\r\n",
    "raw_data /= std"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sampling_rate = 6  # 6\r\n",
    "sequence_length = 120\r\n",
    "delay = sampling_rate * (sequence_length + 24 - 1)  #24小时后\r\n",
    "batch_size = 256\r\n",
    "\r\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "    raw_data[:-delay],\r\n",
    "    targets=temperature[delay:],\r\n",
    "    sampling_rate=sampling_rate,\r\n",
    "    sequence_length=sequence_length,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    start_index=0,\r\n",
    "    end_index=num_train_samples)\r\n",
    "\r\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "    raw_data[:-delay],\r\n",
    "    targets=temperature[delay:],\r\n",
    "    sampling_rate=sampling_rate,\r\n",
    "    sequence_length=sequence_length,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    start_index=num_train_samples,\r\n",
    "    end_index=num_train_samples + num_val_samples)\r\n",
    "\r\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "    raw_data[:-delay],\r\n",
    "    targets=temperature[delay:],\r\n",
    "    sampling_rate=sampling_rate,\r\n",
    "    sequence_length=sequence_length,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    start_index=num_train_samples + num_val_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "准备 10.2 的数据\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\r\n",
    "x = layers.LSTM(32, recurrent_dropout=0.25,\r\n",
    "                unroll=True)(inputs)  #按照后文提示设置了 unroll=True,训练时间也是完全不能接受\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "outputs = layers.Dense(1)(x)\r\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "\r\n",
    "这里会有一个警告:  Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\r\n",
    "\r\n",
    "常见\r\n",
    "> <https://keras.io/api/layers/recurrent_layers/lstm/>\r\n",
    "> <https://stackoverflow.com/questions/62044838/using-cudnn-kernel-for-lstm>\r\n",
    "\r\n",
    "似乎是因为 `recurrent_dropout` 不为 0,所有不能使用 cuDNN 内核.结果是训练非常缓慢.\r\n",
    "\r\n",
    "---\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们在原模型上应用了 gropout 层,设置了 `recurrent_dropout=0.25`.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"jena_lstm_dropout.keras\",\r\n",
    "                                    save_best_only=True)  #保存最好的模型\r\n",
    "]\r\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "history = model.fit(\r\n",
    "    train_dataset,\r\n",
    "    epochs=50,  # 5 倍的训练\r\n",
    "    validation_data=val_dataset,\r\n",
    "    callbacks=callbacks)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "819/819 [==============================] - 90s 75ms/step - loss: 29.0844 - mae: 3.9676 - val_loss: 9.5179 - val_mae: 2.3942\n",
      "Epoch 2/50\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 15.0497 - mae: 3.0137 - val_loss: 9.1772 - val_mae: 2.3529\n",
      "Epoch 3/50\n",
      "819/819 [==============================] - 53s 65ms/step - loss: 14.2617 - mae: 2.9347 - val_loss: 8.9910 - val_mae: 2.3270\n",
      "Epoch 4/50\n",
      "819/819 [==============================] - 55s 67ms/step - loss: 13.7550 - mae: 2.8793 - val_loss: 9.0376 - val_mae: 2.3267\n",
      "Epoch 5/50\n",
      "819/819 [==============================] - 54s 66ms/step - loss: 13.2614 - mae: 2.8311 - val_loss: 8.9118 - val_mae: 2.3105\n",
      "Epoch 6/50\n",
      "819/819 [==============================] - 54s 66ms/step - loss: 12.9278 - mae: 2.7922 - val_loss: 8.9167 - val_mae: 2.3117\n",
      "Epoch 7/50\n",
      "819/819 [==============================] - 54s 65ms/step - loss: 12.6572 - mae: 2.7651 - val_loss: 8.8193 - val_mae: 2.2974\n",
      "Epoch 8/50\n",
      "819/819 [==============================] - 51s 63ms/step - loss: 12.3747 - mae: 2.7313 - val_loss: 8.8636 - val_mae: 2.3066\n",
      "Epoch 9/50\n",
      "819/819 [==============================] - 52s 63ms/step - loss: 12.2140 - mae: 2.7138 - val_loss: 8.8435 - val_mae: 2.3027\n",
      "Epoch 10/50\n",
      "819/819 [==============================] - 52s 63ms/step - loss: 12.0554 - mae: 2.6961 - val_loss: 8.8083 - val_mae: 2.2939\n",
      "Epoch 11/50\n",
      "819/819 [==============================] - 52s 64ms/step - loss: 11.9649 - mae: 2.6840 - val_loss: 8.8122 - val_mae: 2.3024\n",
      "Epoch 12/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 11.7667 - mae: 2.6645 - val_loss: 8.8949 - val_mae: 2.3056\n",
      "Epoch 13/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 11.7039 - mae: 2.6575 - val_loss: 9.0005 - val_mae: 2.3197\n",
      "Epoch 14/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.6131 - mae: 2.6484 - val_loss: 9.0770 - val_mae: 2.3302\n",
      "Epoch 15/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 11.5438 - mae: 2.6389 - val_loss: 9.1167 - val_mae: 2.3359\n",
      "Epoch 16/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.4837 - mae: 2.6328 - val_loss: 8.9041 - val_mae: 2.3092\n",
      "Epoch 17/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.3861 - mae: 2.6246 - val_loss: 9.0523 - val_mae: 2.3243\n",
      "Epoch 18/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.3941 - mae: 2.6226 - val_loss: 9.2091 - val_mae: 2.3437\n",
      "Epoch 19/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.3024 - mae: 2.6117 - val_loss: 9.1930 - val_mae: 2.3384\n",
      "Epoch 20/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.2252 - mae: 2.6064 - val_loss: 9.3318 - val_mae: 2.3559\n",
      "Epoch 21/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 11.1586 - mae: 2.5973 - val_loss: 9.1891 - val_mae: 2.3365\n",
      "Epoch 22/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 11.1570 - mae: 2.5957 - val_loss: 9.1889 - val_mae: 2.3332\n",
      "Epoch 23/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 11.0769 - mae: 2.5857 - val_loss: 9.1787 - val_mae: 2.3327\n",
      "Epoch 24/50\n",
      "819/819 [==============================] - 49s 59ms/step - loss: 11.0410 - mae: 2.5842 - val_loss: 9.3265 - val_mae: 2.3475\n",
      "Epoch 25/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 10.9944 - mae: 2.5782 - val_loss: 9.3277 - val_mae: 2.3500\n",
      "Epoch 26/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.9329 - mae: 2.5709 - val_loss: 9.4012 - val_mae: 2.3597\n",
      "Epoch 27/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.9329 - mae: 2.5716 - val_loss: 9.2586 - val_mae: 2.3486\n",
      "Epoch 28/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 10.8806 - mae: 2.5646 - val_loss: 9.5377 - val_mae: 2.3769\n",
      "Epoch 29/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.8624 - mae: 2.5644 - val_loss: 9.3696 - val_mae: 2.3529\n",
      "Epoch 30/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.8003 - mae: 2.5566 - val_loss: 9.4160 - val_mae: 2.3620\n",
      "Epoch 31/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.7737 - mae: 2.5515 - val_loss: 9.4858 - val_mae: 2.3662\n",
      "Epoch 32/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.7205 - mae: 2.5484 - val_loss: 9.5282 - val_mae: 2.3745\n",
      "Epoch 33/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.7186 - mae: 2.5468 - val_loss: 9.3159 - val_mae: 2.3467\n",
      "Epoch 34/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.6713 - mae: 2.5424 - val_loss: 9.3495 - val_mae: 2.3487\n",
      "Epoch 35/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 10.6700 - mae: 2.5422 - val_loss: 9.3625 - val_mae: 2.3587\n",
      "Epoch 36/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 10.6666 - mae: 2.5391 - val_loss: 9.6080 - val_mae: 2.3875\n",
      "Epoch 37/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 10.5641 - mae: 2.5289 - val_loss: 9.3137 - val_mae: 2.3476\n",
      "Epoch 38/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 10.5600 - mae: 2.5276 - val_loss: 9.7104 - val_mae: 2.3952\n",
      "Epoch 39/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.5393 - mae: 2.5260 - val_loss: 9.4218 - val_mae: 2.3580\n",
      "Epoch 40/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 10.5579 - mae: 2.5265 - val_loss: 9.4607 - val_mae: 2.3675\n",
      "Epoch 41/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 10.5398 - mae: 2.5239 - val_loss: 9.4700 - val_mae: 2.3627\n",
      "Epoch 42/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 10.4827 - mae: 2.5197 - val_loss: 9.7301 - val_mae: 2.3932\n",
      "Epoch 43/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.4369 - mae: 2.5118 - val_loss: 9.3882 - val_mae: 2.3599\n",
      "Epoch 44/50\n",
      "819/819 [==============================] - 50s 60ms/step - loss: 10.3912 - mae: 2.5099 - val_loss: 9.5766 - val_mae: 2.3757\n",
      "Epoch 45/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.3965 - mae: 2.5078 - val_loss: 9.7790 - val_mae: 2.3995\n",
      "Epoch 46/50\n",
      "819/819 [==============================] - 49s 60ms/step - loss: 10.3592 - mae: 2.5042 - val_loss: 9.4656 - val_mae: 2.3628\n",
      "Epoch 47/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.3544 - mae: 2.5028 - val_loss: 9.5156 - val_mae: 2.3687\n",
      "Epoch 48/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.3067 - mae: 2.4973 - val_loss: 9.3892 - val_mae: 2.3565\n",
      "Epoch 49/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.3432 - mae: 2.4964 - val_loss: 9.5866 - val_mae: 2.3799\n",
      "Epoch 50/50\n",
      "819/819 [==============================] - 50s 61ms/step - loss: 10.3206 - mae: 2.4999 - val_loss: 9.5506 - val_mae: 2.3726\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为更不容易过拟合,设置训练轮次是 5 倍之前, 50 次.(慢的要死)\r\n",
    "\r\n",
    "在 cpu 上跑反而更快的多...\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "loss = history.history[\"mae\"]\r\n",
    "val_loss = history.history[\"val_mae\"]\r\n",
    "epochs = range(1, len(loss) + 1)\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\r\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\r\n",
    "plt.title(\"Training and validation MAE\")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 378.465625 263.63625\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-23T17:02:09.691380</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 263.63625 \r\nL 378.465625 263.63625 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 239.758125 \r\nL 371.265625 239.758125 \r\nL 371.265625 22.318125 \r\nL 36.465625 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m83665f1f68\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.472304\" xlink:href=\"#m83665f1f68\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(42.291054 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.587332\" xlink:href=\"#m83665f1f68\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(101.224832 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"169.70236\" xlink:href=\"#m83665f1f68\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(163.33986 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.817388\" xlink:href=\"#m83665f1f68\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(225.454888 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.932415\" xlink:href=\"#m83665f1f68\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(287.569915 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"356.047443\" xlink:href=\"#m83665f1f68\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(349.684943 254.356563)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m3a3c790fbc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"235.060937\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 2.25 -->\r\n      <g transform=\"translate(7.2 238.860156)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"205.534664\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 2.50 -->\r\n      <g transform=\"translate(7.2 209.333883)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"176.008391\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2.75 -->\r\n      <g transform=\"translate(7.2 179.80761)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"146.482119\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 3.00 -->\r\n      <g transform=\"translate(7.2 150.281337)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"116.955846\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 3.25 -->\r\n      <g transform=\"translate(7.2 120.755065)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"87.429573\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 3.50 -->\r\n      <g transform=\"translate(7.2 91.228792)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"57.9033\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 3.75 -->\r\n      <g transform=\"translate(7.2 61.702519)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m3a3c790fbc\" y=\"28.377028\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 4.00 -->\r\n      <g transform=\"translate(7.2 32.176246)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"meb3a226f5e\" style=\"stroke:#0000ff;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pbf2f38049c)\">\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"51.683807\" xlink:href=\"#meb3a226f5e\" y=\"32.201761\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"57.89531\" xlink:href=\"#meb3a226f5e\" y=\"144.861853\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"64.106812\" xlink:href=\"#meb3a226f5e\" y=\"154.193957\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"70.318315\" xlink:href=\"#meb3a226f5e\" y=\"160.741444\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"76.529818\" xlink:href=\"#meb3a226f5e\" y=\"166.433477\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"82.741321\" xlink:href=\"#meb3a226f5e\" y=\"171.029893\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"88.952824\" xlink:href=\"#meb3a226f5e\" y=\"174.230805\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"95.164326\" xlink:href=\"#meb3a226f5e\" y=\"178.216633\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"101.375829\" xlink:href=\"#meb3a226f5e\" y=\"180.289574\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"107.587332\" xlink:href=\"#meb3a226f5e\" y=\"182.372707\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"113.798835\" xlink:href=\"#meb3a226f5e\" y=\"183.804086\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"120.010337\" xlink:href=\"#meb3a226f5e\" y=\"186.101336\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"126.22184\" xlink:href=\"#meb3a226f5e\" y=\"186.930828\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"132.433343\" xlink:href=\"#meb3a226f5e\" y=\"188.012591\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"138.644846\" xlink:href=\"#meb3a226f5e\" y=\"189.125188\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"144.856349\" xlink:href=\"#meb3a226f5e\" y=\"189.846607\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"151.067851\" xlink:href=\"#meb3a226f5e\" y=\"190.816638\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"157.279354\" xlink:href=\"#meb3a226f5e\" y=\"191.060574\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"163.490857\" xlink:href=\"#meb3a226f5e\" y=\"192.345613\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"169.70236\" xlink:href=\"#meb3a226f5e\" y=\"192.970055\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"175.913862\" xlink:href=\"#meb3a226f5e\" y=\"194.039541\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"182.125365\" xlink:href=\"#meb3a226f5e\" y=\"194.232511\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"188.336868\" xlink:href=\"#meb3a226f5e\" y=\"195.418713\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"194.548371\" xlink:href=\"#meb3a226f5e\" y=\"195.595689\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"200.759874\" xlink:href=\"#meb3a226f5e\" y=\"196.300749\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"206.971376\" xlink:href=\"#meb3a226f5e\" y=\"197.164763\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"213.182879\" xlink:href=\"#meb3a226f5e\" y=\"197.076767\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"219.394382\" xlink:href=\"#meb3a226f5e\" y=\"197.902795\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"225.605885\" xlink:href=\"#meb3a226f5e\" y=\"197.929236\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"231.817388\" xlink:href=\"#meb3a226f5e\" y=\"198.853706\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"238.02889\" xlink:href=\"#meb3a226f5e\" y=\"199.447709\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"244.240393\" xlink:href=\"#meb3a226f5e\" y=\"199.817992\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"250.451896\" xlink:href=\"#meb3a226f5e\" y=\"200.011835\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"256.663399\" xlink:href=\"#meb3a226f5e\" y=\"200.530148\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"262.874901\" xlink:href=\"#meb3a226f5e\" y=\"200.550985\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"269.086404\" xlink:href=\"#meb3a226f5e\" y=\"200.912962\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"275.297907\" xlink:href=\"#meb3a226f5e\" y=\"202.116735\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"281.50941\" xlink:href=\"#meb3a226f5e\" y=\"202.278477\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"287.720913\" xlink:href=\"#meb3a226f5e\" y=\"202.464042\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"293.932415\" xlink:href=\"#meb3a226f5e\" y=\"202.401164\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"300.143918\" xlink:href=\"#meb3a226f5e\" y=\"202.707978\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"306.355421\" xlink:href=\"#meb3a226f5e\" y=\"203.210072\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"312.566924\" xlink:href=\"#meb3a226f5e\" y=\"204.142144\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"318.778426\" xlink:href=\"#meb3a226f5e\" y=\"204.363413\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"324.989929\" xlink:href=\"#meb3a226f5e\" y=\"204.610673\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"331.201432\" xlink:href=\"#meb3a226f5e\" y=\"205.04054\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"337.412935\" xlink:href=\"#meb3a226f5e\" y=\"205.207238\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"343.624438\" xlink:href=\"#meb3a226f5e\" y=\"205.855192\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"349.83594\" xlink:href=\"#meb3a226f5e\" y=\"205.955914\"/>\r\n     <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"356.047443\" xlink:href=\"#meb3a226f5e\" y=\"205.541844\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pbf2f38049c)\" d=\"M 51.683807 218.025076 \r\nL 57.89531 222.91358 \r\nL 64.106812 225.961422 \r\nL 70.318315 226.004082 \r\nL 76.529818 227.913196 \r\nL 82.741321 227.773643 \r\nL 88.952824 229.461235 \r\nL 95.164326 228.380598 \r\nL 101.375829 228.830965 \r\nL 107.587332 229.874489 \r\nL 113.798835 228.874582 \r\nL 120.010337 228.495767 \r\nL 126.22184 226.831912 \r\nL 132.433343 225.594264 \r\nL 138.644846 224.919784 \r\nL 144.856349 228.071109 \r\nL 151.067851 226.290735 \r\nL 157.279354 223.994751 \r\nL 163.490857 224.616968 \r\nL 169.70236 222.557741 \r\nL 175.913862 224.848656 \r\nL 182.125365 225.23071 \r\nL 188.336868 225.294489 \r\nL 194.548371 223.540274 \r\nL 200.759874 223.25089 \r\nL 206.971376 222.102926 \r\nL 213.182879 223.415701 \r\nL 219.394382 220.067999 \r\nL 225.605885 222.912312 \r\nL 231.817388 221.829395 \r\nL 238.02889 221.338171 \r\nL 244.240393 220.361917 \r\nL 250.451896 223.640659 \r\nL 256.663399 223.405113 \r\nL 262.874901 222.228625 \r\nL 269.086404 218.816469 \r\nL 275.297907 223.528757 \r\nL 281.50941 217.917257 \r\nL 287.720913 222.303499 \r\nL 293.932415 221.18068 \r\nL 300.143918 221.747707 \r\nL 306.355421 218.143736 \r\nL 312.566924 222.081103 \r\nL 318.778426 220.209918 \r\nL 324.989929 217.407814 \r\nL 331.201432 221.739597 \r\nL 337.412935 221.046731 \r\nL 343.624438 222.480277 \r\nL 349.83594 219.715202 \r\nL 356.047443 220.58696 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 239.758125 \r\nL 36.465625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 239.758125 \r\nL 371.265625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 239.758125 \r\nL 371.265625 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 22.318125 \r\nL 371.265625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_15\">\r\n    <!-- Training and validation MAE -->\r\n    <g transform=\"translate(120.5725 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M -19 4666 \r\nL 3928 4666 \r\nL 3928 4134 \r\nL 2272 4134 \r\nL 2272 0 \r\nL 1638 0 \r\nL 1638 4134 \r\nL -19 4134 \r\nL -19 4666 \r\nz\r\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2631 2963 \r\nQ 2534 3019 2420 3045 \r\nQ 2306 3072 2169 3072 \r\nQ 1681 3072 1420 2755 \r\nQ 1159 2438 1159 1844 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1341 3275 1631 3429 \r\nQ 1922 3584 2338 3584 \r\nQ 2397 3584 2469 3576 \r\nQ 2541 3569 2628 3553 \r\nL 2631 2963 \r\nz\r\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2194 1759 \r\nQ 1497 1759 1228 1600 \r\nQ 959 1441 959 1056 \r\nQ 959 750 1161 570 \r\nQ 1363 391 1709 391 \r\nQ 2188 391 2477 730 \r\nQ 2766 1069 2766 1631 \r\nL 2766 1759 \r\nL 2194 1759 \r\nz\r\nM 3341 1997 \r\nL 3341 0 \r\nL 2766 0 \r\nL 2766 531 \r\nQ 2569 213 2275 61 \r\nQ 1981 -91 1556 -91 \r\nQ 1019 -91 701 211 \r\nQ 384 513 384 1019 \r\nQ 384 1609 779 1909 \r\nQ 1175 2209 1959 2209 \r\nL 2766 2209 \r\nL 2766 2266 \r\nQ 2766 2663 2505 2880 \r\nQ 2244 3097 1772 3097 \r\nQ 1472 3097 1187 3025 \r\nQ 903 2953 641 2809 \r\nL 641 3341 \r\nQ 956 3463 1253 3523 \r\nQ 1550 3584 1831 3584 \r\nQ 2591 3584 2966 3190 \r\nQ 3341 2797 3341 1997 \r\nz\r\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 603 3500 \r\nL 1178 3500 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 3500 \r\nz\r\nM 603 4863 \r\nL 1178 4863 \r\nL 1178 4134 \r\nL 603 4134 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 3513 2113 \r\nL 3513 0 \r\nL 2938 0 \r\nL 2938 2094 \r\nQ 2938 2591 2744 2837 \r\nQ 2550 3084 2163 3084 \r\nQ 1697 3084 1428 2787 \r\nQ 1159 2491 1159 1978 \r\nL 1159 0 \r\nL 581 0 \r\nL 581 3500 \r\nL 1159 3500 \r\nL 1159 2956 \r\nQ 1366 3272 1645 3428 \r\nQ 1925 3584 2291 3584 \r\nQ 2894 3584 3203 3211 \r\nQ 3513 2838 3513 2113 \r\nz\r\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2906 1791 \r\nQ 2906 2416 2648 2759 \r\nQ 2391 3103 1925 3103 \r\nQ 1463 3103 1205 2759 \r\nQ 947 2416 947 1791 \r\nQ 947 1169 1205 825 \r\nQ 1463 481 1925 481 \r\nQ 2391 481 2648 825 \r\nQ 2906 1169 2906 1791 \r\nz\r\nM 3481 434 \r\nQ 3481 -459 3084 -895 \r\nQ 2688 -1331 1869 -1331 \r\nQ 1566 -1331 1297 -1286 \r\nQ 1028 -1241 775 -1147 \r\nL 775 -588 \r\nQ 1028 -725 1275 -790 \r\nQ 1522 -856 1778 -856 \r\nQ 2344 -856 2625 -561 \r\nQ 2906 -266 2906 331 \r\nL 2906 616 \r\nQ 2728 306 2450 153 \r\nQ 2172 0 1784 0 \r\nQ 1141 0 747 490 \r\nQ 353 981 353 1791 \r\nQ 353 2603 747 3093 \r\nQ 1141 3584 1784 3584 \r\nQ 2172 3584 2450 3431 \r\nQ 2728 3278 2906 2969 \r\nL 2906 3500 \r\nL 3481 3500 \r\nL 3481 434 \r\nz\r\n\" id=\"DejaVuSans-67\" transform=\"scale(0.015625)\"/>\r\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2906 2969 \r\nL 2906 4863 \r\nL 3481 4863 \r\nL 3481 0 \r\nL 2906 0 \r\nL 2906 525 \r\nQ 2725 213 2448 61 \r\nQ 2172 -91 1784 -91 \r\nQ 1150 -91 751 415 \r\nQ 353 922 353 1747 \r\nQ 353 2572 751 3078 \r\nQ 1150 3584 1784 3584 \r\nQ 2172 3584 2448 3432 \r\nQ 2725 3281 2906 2969 \r\nz\r\nM 947 1747 \r\nQ 947 1113 1208 752 \r\nQ 1469 391 1925 391 \r\nQ 2381 391 2643 752 \r\nQ 2906 1113 2906 1747 \r\nQ 2906 2381 2643 2742 \r\nQ 2381 3103 1925 3103 \r\nQ 1469 3103 1208 2742 \r\nQ 947 2381 947 1747 \r\nz\r\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 191 3500 \r\nL 800 3500 \r\nL 1894 563 \r\nL 2988 3500 \r\nL 3597 3500 \r\nL 2284 0 \r\nL 1503 0 \r\nL 191 3500 \r\nz\r\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 603 4863 \r\nL 1178 4863 \r\nL 1178 0 \r\nL 603 0 \r\nL 603 4863 \r\nz\r\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1172 4494 \r\nL 1172 3500 \r\nL 2356 3500 \r\nL 2356 3053 \r\nL 1172 3053 \r\nL 1172 1153 \r\nQ 1172 725 1289 603 \r\nQ 1406 481 1766 481 \r\nL 2356 481 \r\nL 2356 0 \r\nL 1766 0 \r\nQ 1100 0 847 248 \r\nQ 594 497 594 1153 \r\nL 594 3053 \r\nL 172 3053 \r\nL 172 3500 \r\nL 594 3500 \r\nL 594 4494 \r\nL 1172 4494 \r\nz\r\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 1959 3097 \r\nQ 1497 3097 1228 2736 \r\nQ 959 2375 959 1747 \r\nQ 959 1119 1226 758 \r\nQ 1494 397 1959 397 \r\nQ 2419 397 2687 759 \r\nQ 2956 1122 2956 1747 \r\nQ 2956 2369 2687 2733 \r\nQ 2419 3097 1959 3097 \r\nz\r\nM 1959 3584 \r\nQ 2709 3584 3137 3096 \r\nQ 3566 2609 3566 1747 \r\nQ 3566 888 3137 398 \r\nQ 2709 -91 1959 -91 \r\nQ 1206 -91 779 398 \r\nQ 353 888 353 1747 \r\nQ 353 2609 779 3096 \r\nQ 1206 3584 1959 3584 \r\nz\r\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 628 4666 \r\nL 1569 4666 \r\nL 2759 1491 \r\nL 3956 4666 \r\nL 4897 4666 \r\nL 4897 0 \r\nL 4281 0 \r\nL 4281 4097 \r\nL 3078 897 \r\nL 2444 897 \r\nL 1241 4097 \r\nL 1241 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-4d\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 2188 4044 \r\nL 1331 1722 \r\nL 3047 1722 \r\nL 2188 4044 \r\nz\r\nM 1831 4666 \r\nL 2547 4666 \r\nL 4325 0 \r\nL 3669 0 \r\nL 3244 1197 \r\nL 1141 1197 \r\nL 716 0 \r\nL 50 0 \r\nL 1831 4666 \r\nz\r\n\" id=\"DejaVuSans-41\" transform=\"scale(0.015625)\"/>\r\n      <path d=\"M 628 4666 \r\nL 3578 4666 \r\nL 3578 4134 \r\nL 1259 4134 \r\nL 1259 2753 \r\nL 3481 2753 \r\nL 3481 2222 \r\nL 1259 2222 \r\nL 1259 531 \r\nL 3634 531 \r\nL 3634 0 \r\nL 628 0 \r\nL 628 4666 \r\nz\r\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-54\"/>\r\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-72\"/>\r\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"487.59375\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     <use x=\"550.972656\" xlink:href=\"#DejaVuSans-64\"/>\r\n     <use x=\"614.449219\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"646.236328\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"705.416016\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"766.695312\" xlink:href=\"#DejaVuSans-6c\"/>\r\n     <use x=\"794.478516\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"822.261719\" xlink:href=\"#DejaVuSans-64\"/>\r\n     <use x=\"885.738281\" xlink:href=\"#DejaVuSans-61\"/>\r\n     <use x=\"947.017578\" xlink:href=\"#DejaVuSans-74\"/>\r\n     <use x=\"986.226562\" xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"1014.009766\" xlink:href=\"#DejaVuSans-6f\"/>\r\n     <use x=\"1075.191406\" xlink:href=\"#DejaVuSans-6e\"/>\r\n     <use x=\"1138.570312\" xlink:href=\"#DejaVuSans-20\"/>\r\n     <use x=\"1170.357422\" xlink:href=\"#DejaVuSans-4d\"/>\r\n     <use x=\"1256.636719\" xlink:href=\"#DejaVuSans-41\"/>\r\n     <use x=\"1325.044922\" xlink:href=\"#DejaVuSans-45\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 257.91875 59.674375 \r\nL 364.265625 59.674375 \r\nQ 366.265625 59.674375 366.265625 57.674375 \r\nL 366.265625 29.318125 \r\nQ 366.265625 27.318125 364.265625 27.318125 \r\nL 257.91875 27.318125 \r\nQ 255.91875 27.318125 255.91875 29.318125 \r\nL 255.91875 57.674375 \r\nQ 255.91875 59.674375 257.91875 59.674375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"line2d_18\">\r\n     <g>\r\n      <use style=\"fill:#0000ff;stroke:#0000ff;\" x=\"269.91875\" xlink:href=\"#meb3a226f5e\" y=\"35.416562\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Training MAE -->\r\n     <g transform=\"translate(287.91875 38.916562)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-54\"/>\r\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-72\"/>\r\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"148.726562\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"176.509766\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"239.888672\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"267.671875\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"331.050781\" xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"394.527344\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"426.314453\" xlink:href=\"#DejaVuSans-4d\"/>\r\n      <use x=\"512.59375\" xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"581.001953\" xlink:href=\"#DejaVuSans-45\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_19\">\r\n     <path d=\"M 259.91875 50.094687 \r\nL 279.91875 50.094687 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\"/>\r\n    <g id=\"text_17\">\r\n     <!-- Validation MAE -->\r\n     <g transform=\"translate(287.91875 53.594687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 1831 0 \r\nL 50 4666 \r\nL 709 4666 \r\nL 2188 738 \r\nL 3669 4666 \r\nL 4325 4666 \r\nL 2547 0 \r\nL 1831 0 \r\nz\r\n\" id=\"DejaVuSans-56\" transform=\"scale(0.015625)\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-56\"/>\r\n      <use x=\"60.658203\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"121.9375\" xlink:href=\"#DejaVuSans-6c\"/>\r\n      <use x=\"149.720703\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"177.503906\" xlink:href=\"#DejaVuSans-64\"/>\r\n      <use x=\"240.980469\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"302.259766\" xlink:href=\"#DejaVuSans-74\"/>\r\n      <use x=\"341.46875\" xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"369.251953\" xlink:href=\"#DejaVuSans-6f\"/>\r\n      <use x=\"430.433594\" xlink:href=\"#DejaVuSans-6e\"/>\r\n      <use x=\"493.8125\" xlink:href=\"#DejaVuSans-20\"/>\r\n      <use x=\"525.599609\" xlink:href=\"#DejaVuSans-4d\"/>\r\n      <use x=\"611.878906\" xlink:href=\"#DejaVuSans-41\"/>\r\n      <use x=\"680.287109\" xlink:href=\"#DejaVuSans-45\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pbf2f38049c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = keras.models.load_model(\"jena_lstm.keras\")\r\n",
    "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "405/405 [==============================] - 7s 18ms/step - loss: 10.6795 - mae: 2.5664\n",
      "Test MAE: 2.57\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小贴士\r\n",
    "\r\n",
    "keras ltsm/gru 训练时 gpu 上使用 cuDNN 内核的前提是保持默认参数,上文我们打破了,所以无法调用 cuDNN 内核.模型的训练时间也就回退到了普通 tensorflow 的思想,比通常慢 3~5 倍\r\n",
    "\r\n",
    "当我们不能使用 cuDNN 时,可以尝试解卷积,解除 for 循环,并简单将其转换为内联(??),看原文就是增加一个参数 `unroll=True`,不过这样会大大增加显存的消耗,而且不能向模型传递任何没有固定形状的数据.(就是不能有 None)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# inputs = keras.Input(shape=(sequence_length, num_features))\r\n",
    "# x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rnn 堆叠\r\n",
    "\r\n",
    "模型不再过拟合吗,但是似乎遇到了性能瓶颈.此时需要考虑增大网络容量了.\r\n",
    "\r\n",
    "回顾一下机器学习的基本流程: 增加网络容量通常是个好办法,直到过拟合变为主要障碍(假设已经采取了基本的降低过拟合的策略,例如 dropout),只要不是过拟合太严重,很可能是容量不足问题.\r\n",
    "\r\n",
    "增加网络容量的办法通常是增加每层的单元数,或者增加层数,rnn 堆叠是更加更强大 rnn 网络的经典方法,目前 google 的翻译算法就是 7 个大型 ltsm 的堆叠.\r\n",
    "\r\n",
    "为了将 rnn 层堆叠起来,所有中间层都需要输出网站的序列,而不是只输出最后一个时间步的输出.这是通过指定 `return_sequences=True` 实现的.\r\n",
    "\r\n",
    "下面的例子中我们会将 ltsm 范围 gru,gru 和 ltsm 非常相似,你可以将其视为 ltsm 的简化版本.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\r\n",
    "x = layers.GRU(32, recurrent_dropout=0.5, unroll=True,\r\n",
    "               return_sequences=True)(inputs)\r\n",
    "x = layers.GRU(32, recurrent_dropout=0.5, unroll=True)(x)\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "outputs = layers.Dense(1)(x)\r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "\r\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我们还是不能使用 cuRNN 内核,所以训练非常之慢...\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "我们堆叠了两个 gru 层,同时使用了 dropout.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\r\n",
    "                                    save_best_only=True)\r\n",
    "]\r\n",
    "history = model.fit(train_dataset,\r\n",
    "                    epochs=50,\r\n",
    "                    validation_data=val_dataset,\r\n",
    "                    callbacks=callbacks)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "819/819 [==============================] - 148s 178ms/step - loss: 24.6477 - mae: 3.6737 - val_loss: 10.0024 - val_mae: 2.4640\n",
      "Epoch 2/50\n",
      "819/819 [==============================] - 144s 176ms/step - loss: 14.0645 - mae: 2.9031 - val_loss: 9.0201 - val_mae: 2.3240\n",
      "Epoch 3/50\n",
      "819/819 [==============================] - 148s 181ms/step - loss: 13.3406 - mae: 2.8282 - val_loss: 9.2028 - val_mae: 2.3511\n",
      "Epoch 4/50\n",
      "819/819 [==============================] - 149s 182ms/step - loss: 12.7574 - mae: 2.7630 - val_loss: 8.6870 - val_mae: 2.2830\n",
      "Epoch 5/50\n",
      " 45/819 [>.............................] - ETA: 2:10 - loss: 12.5069 - mae: 2.7326"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1700/2616552557.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                     save_best_only=True)\n\u001b[0;32m      4\u001b[0m ]\n\u001b[1;32m----> 5\u001b[1;33m history = model.fit(train_dataset,\n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\stati\\Git\\Deep-Learning\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = history.history[\"mae\"]\r\n",
    "val_loss = history.history[\"val_mae\"]\r\n",
    "epochs = range(1, len(loss) + 1)\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\r\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\r\n",
    "plt.title(\"Training and validation MAE\")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\r\n",
    "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 双向 rnn\r\n",
    "\r\n",
    "本节介绍的最后一种方法叫做双向 rnn,双向 rnn 是一种常见的 rnn 变体,它在某些任务上表现比普通 rnn 更好,常用于自然语言处理.\r\n",
    "\r\n",
    "rnn 非常依赖与输入顺序/时间.rnn 按照顺序处理序列的时间步,而打乱时间步或者反转时间步将会完全改变 rnn 从序列中提取的模式.正是这个原因使得 rnn 在顺序敏感问题上表现很好.双向 rnn 利用了 rnn 对顺序的敏感性: 它包含两个普通 rnn,每个 rnn 分别沿着输入序列的正序和逆序进行处理,最后将结果合并.双向 rnn 有可能捕获单向 rnn 忽略的模式.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "值得注意的是截止到现在,本文例子中的 rnn 模型输入全部都是正向,至少我们没有刻意尝试逆向.逆向序列会更好吗?我们尝试一下第一个实验中使用的 ltsm 模型.(将输入实例程序替换 yield  samples[:, ::-1, :], targets)\r\n",
    "\r\n",
    "![backwards_lstm_model_metrics](backwards_lstm_model_metrics.png)\r\n",
    "\r\n",
    "逆序 ltsm 的结果,甚至低于基线..这表明至少在这个实验中,按照时间正向输入数据对解决问题非常重要: ltsm 在记忆刚刚过去的时间比记忆更遥远过去更加容易,对这个问题而言,较劲的天气数据比较早的数据预测上更有价值,因此输入正向的序列结果肯定比逆序的要好.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf')"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}