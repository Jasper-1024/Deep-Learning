{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "10.4 RNN 的高级用法\r\n",
    "\r\n",
    "到目前为止,我们已经学过的内容\r\n",
    "\r\n",
    "- 什么是 RNN 及它如何工作\r\n",
    "- 什么是 ltsm 及 ltsm 比简单 rnn 更好\r\n",
    "- 如何使用 keras 的 rnn 层处理时序数据.\r\n",
    "\r\n",
    "接下来我们会学习 keras rnn 的高级特性,这可以帮助我们的模型达到更好的效果.\r\n",
    "\r\n",
    "- 循环 dropout: 在 rnn 层应用 dropout.\r\n",
    "- 堆叠循环层: 提高模型的容量和表现力,代价是更高的计算消耗.\r\n",
    "- 双向 RNN: 将相同的信息正向/反向送入模型,提高精读缓解遗忘问题.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 循环 dropout 缓解过拟合\r\n",
    "\r\n",
    "回顾 10.2 节的 ltsm 模型,尽管小模型可以击败基线,但是非常快的过拟合了,原因可能是模型容量过小.\r\n",
    "\r\n",
    "在更前面的内容中对抗过拟合有一个非常有效的办法 -> dropout,但是 dropout 在 rnn 并不是一件容易的事情..\r\n",
    "\r\n",
    "如果直接应用 dropout,那对 rnn 模型可能是一件坏事而不是好事,droupout 总是会破坏学习过程.直到 2015 年,Yarin Gal 在博士论文确定了 rnn 使用 droupout 的正确方式: 对每个时间步使用相同的 dropout 掩码(dropout mask,相同模式的舍弃单元),而不是让 dropout 掩码随着时间变化而随机变化.更重要的是为了对 GRU ltsm 等循环层做正则化表示,应该将不随时间变化的 dropout 掩码应用在层的内部循环激活上(成为 循环 dropout 掩码).对每个时间步长使用相同的 dropout 掩码,可以让我按钮沿着时间正确传播学习误差,而随着时间变化的 dropout 掩码会破坏这个过程.\r\n",
    "\r\n",
    "Yarin Gal 使用 keras 进行的研究,帮助将这种机制直接内置到了 keras 的 rnn 层中.keras 的 rnn 层都有两个与 dropout 相关的参数:\r\n",
    "\r\n",
    "- `dropout`: 浮点数,指定该层每个输入单元 dropout 的比例.\r\n",
    "- `recurrent_dropout`: 指定循环单元的 dropout 的比例.\r\n",
    "\r\n",
    "我们先第一个 ltsm 的例子中添加 dropout,看看这样对过拟合的影响.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from tensorflow import keras\r\n",
    "import tensorflow.keras.layers as layers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sequence_length = 120\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "\r\n",
    "fname = os.path.join(\"jena_climate_2009_2016.csv\")\r\n",
    "\r\n",
    "with open(fname) as f:\r\n",
    "    data = f.read()\r\n",
    "\r\n",
    "lines = data.split(\"\\n\")\r\n",
    "header = lines[0].split(\",\")\r\n",
    "lines = lines[1:]\r\n",
    "\r\n",
    "temperature = np.zeros((len(lines), ))  #全0 数组\r\n",
    "raw_data = np.zeros((len(lines), len(header) - 1))  #全0 数组\r\n",
    "for i, line in enumerate(lines):\r\n",
    "    values = [float(x) for x in line.split(\",\")[1:]]  #每行的第一个值是时间戳，所以从第二个开始\r\n",
    "    temperature[i] = values[1]  #第一个值是温度\r\n",
    "    raw_data[i, :] = values[:]  #每行的数据\r\n",
    "\r\n",
    "num_train_samples = int(0.5 * len(raw_data))\r\n",
    "num_val_samples = int(0.25 * len(raw_data))\r\n",
    "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\r\n",
    "\r\n",
    "\r\n",
    "mean = raw_data[:num_train_samples].mean(axis=0)\r\n",
    "raw_data -= mean\r\n",
    "std = raw_data[:num_train_samples].std(axis=0)\r\n",
    "raw_data /= std"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "sampling_rate = 6  # 6\r\n",
    "sequence_length = 120\r\n",
    "delay = sampling_rate * (sequence_length + 24 - 1)  #24小时后\r\n",
    "batch_size = 256\r\n",
    "\r\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "    raw_data[:-delay],\r\n",
    "    targets=temperature[delay:],\r\n",
    "    sampling_rate=sampling_rate,\r\n",
    "    sequence_length=sequence_length,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    start_index=0,\r\n",
    "    end_index=num_train_samples)\r\n",
    "\r\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "    raw_data[:-delay],\r\n",
    "    targets=temperature[delay:],\r\n",
    "    sampling_rate=sampling_rate,\r\n",
    "    sequence_length=sequence_length,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    start_index=num_train_samples,\r\n",
    "    end_index=num_train_samples + num_val_samples)\r\n",
    "\r\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "    raw_data[:-delay],\r\n",
    "    targets=temperature[delay:],\r\n",
    "    sampling_rate=sampling_rate,\r\n",
    "    sequence_length=sequence_length,\r\n",
    "    shuffle=True,\r\n",
    "    batch_size=batch_size,\r\n",
    "    start_index=num_train_samples + num_val_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "准备 10.2 的数据\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\r\n",
    "x = layers.LSTM(32, recurrent_dropout=0.25,\r\n",
    "                unroll=True)(inputs)  #按照后文提示设置了 unroll=True,训练时间也是完全不能接受\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "outputs = layers.Dense(1)(x)\r\n",
    "model = keras.Model(inputs, outputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\r\n",
    "\r\n",
    "这里会有一个警告:  Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\r\n",
    "\r\n",
    "常见\r\n",
    "> <https://keras.io/api/layers/recurrent_layers/lstm/>\r\n",
    "> <https://stackoverflow.com/questions/62044838/using-cudnn-kernel-for-lstm>\r\n",
    "\r\n",
    "似乎是因为 `recurrent_dropout` 不为 0,所有不能使用 cuDNN 内核.结果是训练非常缓慢.\r\n",
    "\r\n",
    "---\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们在原模型上应用了 gropout 层,设置了 `recurrent_dropout=0.25`.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"jena_lstm_dropout.keras\",\r\n",
    "                                    save_best_only=True)  #保存最好的模型\r\n",
    "]\r\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(\r\n",
    "    train_dataset,\r\n",
    "    epochs=50,  # 5 倍的训练\r\n",
    "    validation_data=val_dataset,\r\n",
    "    callbacks=callbacks)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为更不容易过拟合,设置训练轮次是 5 倍之前, 50 次.(慢的要死)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "loss = history.history[\"mae\"]\r\n",
    "val_loss = history.history[\"val_mae\"]\r\n",
    "epochs = range(1, len(loss) + 1)\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\r\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\r\n",
    "plt.title(\"Training and validation MAE\")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = history.history[\"mae\"]\r\n",
    "val_loss = history.history[\"val_mae\"]\r\n",
    "epochs = range(1, len(loss) + 1)\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\r\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\r\n",
    "plt.title(\"Training and validation MAE\")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.models.load_model(\"jena_lstm.keras\")\r\n",
    "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 小贴士\r\n",
    "\r\n",
    "keras ltsm/gru 训练时 gpu 上使用 cuDNN 内核的前提是保持默认参数,上文我们打破了,所以无法调用 cuDNN 内核.模型的训练时间也就回退到了普通 tensorflow 的思想,比通常慢 3~5 倍\r\n",
    "\r\n",
    "当我们不能使用 cuDNN 时,可以尝试解卷积,解除 for 循环,并简单将其转换为内联(??),看原文就是增加一个参数 `unroll=True`,不过这样会大大增加显存的消耗,而且不能向模型传递任何没有固定形状的数据.(就是不能有 None)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# inputs = keras.Input(shape=(sequence_length, num_features))\r\n",
    "# x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rnn 堆叠\r\n",
    "\r\n",
    "模型不再过拟合吗,但是似乎遇到了性能瓶颈.此时需要考虑增大网络容量了.\r\n",
    "\r\n",
    "回顾一下机器学习的基本流程: 增加网络容量通常是个好办法,直到过拟合变为主要障碍(假设已经采取了基本的降低过拟合的策略,例如 dropout),只要不是过拟合太严重,很可能是容量不足问题.\r\n",
    "\r\n",
    "增加网络容量的办法通常是增加每层的单元数,或者增加层数,rnn 堆叠是更加更强大 rnn 网络的经典方法,目前 google 的翻译算法就是 7 个大型 ltsm 的堆叠.\r\n",
    "\r\n",
    "为了将 rnn 层堆叠起来,所有中间层都需要输出网站的序列,而不是只输出最后一个时间步的输出.这是通过指定 `return_sequences=True` 实现的.\r\n",
    "\r\n",
    "下面的例子中我们会将 ltsm 范围 gru,gru 和 ltsm 非常相似,你可以将其视为 ltsm 的简化版本.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\r\n",
    "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\r\n",
    "x = layers.GRU(32, recurrent_dropout=0.5)(x)\r\n",
    "x = layers.Dropout(0.5)(x)\r\n",
    "outputs = layers.Dense(1)(x)\r\n",
    "model = keras.Model(inputs, outputs)\r\n",
    "\r\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "这里我们还是不能使用 cuRNN 内核,所以训练非常之慢...\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "我们堆叠了两个 gru 层,同时使用了 dropout.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "callbacks = [\r\n",
    "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\r\n",
    "                                    save_best_only=True)\r\n",
    "]\r\n",
    "history = model.fit(train_dataset,\r\n",
    "                    epochs=50,\r\n",
    "                    validation_data=val_dataset,\r\n",
    "                    callbacks=callbacks)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = history.history[\"mae\"]\r\n",
    "val_loss = history.history[\"val_mae\"]\r\n",
    "epochs = range(1, len(loss) + 1)\r\n",
    "plt.figure()\r\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\r\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\r\n",
    "plt.title(\"Training and validation MAE\")\r\n",
    "plt.legend()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\r\n",
    "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 双向 rnn\r\n",
    "\r\n",
    "本节介绍的最后一种方法叫做双向 rnn,双向 rnn 是一种常见的 rnn 变体,它在某些任务上表现比普通 rnn 更好,常用于自然语言处理.\r\n",
    "\r\n",
    "rnn 非常依赖与输入顺序/时间.rnn 按照顺序处理序列的时间步,而打乱时间步或者反转时间步将会完全改变 rnn 从序列中提取的模式.正是这个原因使得 rnn 在顺序敏感问题上表现很好.双向 rnn 利用了 rnn 对顺序的敏感性: 它包含两个普通 rnn,每个 rnn 分别沿着输入序列的正序和逆序进行处理,最后将结果合并.双向 rnn 有可能捕获单向 rnn 忽略的模式.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "值得注意的是截止到现在,本文例子中的 rnn 模型输入全部都是正向,至少我们没有刻意尝试逆向.逆向序列会更好吗?我们尝试一下第一个实验中使用的 ltsm 模型.(将输入实例程序替换 yield  samples[:, ::-1, :], targets)\r\n",
    "\r\n",
    "![backwards_lstm_model_metrics](backwards_lstm_model_metrics.png)\r\n",
    "\r\n",
    "逆序 ltsm 的结果,甚至低于基线..这表明至少在这个实验中,按照时间正向输入数据对解决问题非常重要: ltsm 在记忆刚刚过去的时间比记忆更遥远过去更加容易,对这个问题而言,较劲的天气数据比较早的数据预测上更有价值,因此输入正向的序列结果肯定比逆序的要好.\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tf')"
  },
  "interpreter": {
   "hash": "aac46f597da82ce9618f0e6f094e6d401f1ab16d9be89acf77ce1dd63d67a333"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}