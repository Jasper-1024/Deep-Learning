{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 张量的运算\r\n",
    "\r\n",
    "如果有矩阵的基础,就比较好理解了.\r\n",
    "\r\n",
    "第一个 mnist 的示例中,第一个全连接层 `keras.layers.Dense(512, activation='relu')` 实际上等同于 `output = relu(dot(W, input) + b)`\r\n",
    "\r\n",
    "- 首先是 input 和 W 的矩阵乘法,得到一个向量 c.\r\n",
    "- 之后是 c 与 b 的加法运算.\r\n",
    "- 最后是 relu 运算.\r\n",
    "\r\n",
    "正如所有计算机程序最后编译后都是简单的二进制运算,神经网络本质上也是张量之间的基本运算.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "张量运算\r\n",
    "\r\n",
    "- 加法\r\n",
    "- 乘法\r\n",
    "- 变形\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "加法减法,逐元素运算,没什么好说的.符号 +-\r\n",
    "\r\n",
    "值得提一下的是当张量 shape 不匹配时,较小的张量可以通过广播扩张以匹配较大张量.\r\n",
    "\r\n",
    "广播实际上是补充缺少的轴,不断重复以填充缺少内容.(图源: <https://www.cnblogs.com/Assist/p/11158028.html>)\r\n",
    "\r\n",
    "![张量广播](./张量广播.png)\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "张量乘法,实际上与矩阵乘法运算相同.\r\n",
    "\r\n",
    "Numpy 中使用 dot(A,B) 表示.数学上是 .\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "张量变形/变换,元素不变,变化形状.变换坐标系.\r\n",
    "\r\n",
    "最常见的是转置.对于二维矩阵就是行列互换. `np.transpose(x)`\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}